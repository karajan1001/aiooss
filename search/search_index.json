{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Aiooss2 API Reference","title":"Welcome to Aiooss2"},{"location":"#welcome-to-aiooss2","text":"API Reference","title":"Welcome to Aiooss2"},{"location":"reference/aiooss2/","text":"Imports from this file follows the contents in `oss2`` AioBucket Bases: _AioBase Used for Bucket and Object operations, creating\u3001deleting Bucket, uploading\u3001downloading Object, etc\u3002 use case (bucket in HangZhou area):: import oss2 import aiooss2 import asyncio auth = oss2.Auth('your-access-key-id', 'your-access-key-secret') bucket = aiooss2.Bucket(auth, 'http://oss-cn-hangzhou.aliyuncs.com', 'your-bucket') def upload(): data = b\"\u0001\" * 1024 resp = await bucket.put_object('readme.txt', 'content of the object') return resp loop = asyncio.get_event_loop() loop.run_until_complete(upload()) Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 class AioBucket ( _AioBase ): \"\"\"Used for Bucket and Object operations, creating\u3001deleting Bucket, uploading\u3001downloading Object, etc\u3002 use case (bucket in HangZhou area):: >>> import oss2 >>> import aiooss2 >>> import asyncio >>> auth = oss2.Auth('your-access-key-id', 'your-access-key-secret') >>> bucket = aiooss2.Bucket(auth, 'http://oss-cn-hangzhou.aliyuncs.com', >>> 'your-bucket') >>> def upload(): >>> data = b\"\\x01\" * 1024 >>> resp = await bucket.put_object('readme.txt', >>> 'content of the object') >>> return resp >>> loop = asyncio.get_event_loop() >>> loop.run_until_complete(upload()) <oss2.models.PutObjectResult object at 0x029B9930> \"\"\" auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ] def __init__ ( self , auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ], endpoint : str , bucket_name : str , is_cname : bool = False , ** kwargs , ): \"\"\" Args: bucket_name (str): the bucket name to operate \"\"\" self . bucket_name = bucket_name . strip () if is_valid_bucket_name ( self . bucket_name ) is not True : raise ClientError ( f \"The bucket_name ' { self . bucket_name } ' is invalid, please check it.\" ) super () . __init__ ( auth , endpoint , is_cname , ** kwargs , ) async def _do_object ( self , method : str , key : Union [ bytes , str ], ** kwargs ) -> \"AioResponse\" : return await self . _do ( method , self . bucket_name , key , ** kwargs ) async def _do_bucket ( self , method : str , ** kwargs ) -> \"AioResponse\" : return await self . _do ( method , self . bucket_name , \"\" , ** kwargs ) async def put_object ( self , key : str , data , headers : Optional [ Mapping ] = None , progress_callback : Optional [ Callable ] = None , ) -> \"PutObjectResult\" : \"\"\"upload some contents to an object (use case) :: >>> await bucket.put_object('readme.txt', 'content of readme.txt') >>> with open(u'local_file.txt', 'rb') as f: >>> await bucket.put_object('remote_file.txt', f) Args: key (str): object name to upload data (Union[str, bytes, IO, Iterable]): contents to upload headers (Optional[Mapping], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. Returns: PutObjectResult: \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), key ) data = make_adapter ( data , progress_callback = progress_callback , enable_crc = self . enable_crc , ) resp : \"AioResponse\" = await self . _do_object ( \"PUT\" , key , data = data , headers = headers ) logger . debug ( \"Put object done\" ) result = PutObjectResult ( resp ) if self . enable_crc and result . crc is not None : check_crc ( \"put object\" , data . crc , result . crc , result . request_id ) return result async def get_object ( # pylint: disable=too-many-arguments self , key : str , byte_range : Optional [ Sequence [ Optional [ int ]]] = None , headers : Optional [ dict ] = None , progress_callback : Optional [ Callable ] = None , process = None , params : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> AioGetObjectResult : \"\"\"download the contents of an object (use case) :: >>> async with await bucket.get_object(\"helloword\") as result >>> print(await result.read()) 'hello world' Args: key (str): object name to download. byte_range (Optional[Sequence[Optional[int]]], optional): Range to download. headers (Optional[dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. process (_type_, optional): oss file process method. params (Optional[Union[Dict, CaseInsensitiveDict]], optional): Returns: AioGetObjectResult: \"\"\" headers_dict : CaseInsensitiveDict = CaseInsensitiveDict ( headers ) range_string = _make_range_string ( byte_range ) if range_string : headers_dict [ \"range\" ] = range_string params = {} if params is None else params if process : params [ Bucket . PROCESS ] = process resp = await self . _do_object ( \"GET\" , key , headers = headers_dict , params = params ) logger . debug ( \"Get object done\" ) return AioGetObjectResult ( resp , progress_callback , self . enable_crc ) async def delete_object ( self , key : str , params : Union [ Dict , CaseInsensitiveDict ] = None , headers : Optional [ Dict ] = None , ) -> \"RequestResult\" : \"\"\"delete an object (use case) :: >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object1' 'object2' >>> await bucket.delete_object(\"object1\") >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object2' Args: key (str): _description_ headers (Optional[Dict], optional): HTTP headers to specify. params (Union[Dict, CaseInsensitiveDict], optional): Returns: RequestResult: \"\"\" resp = await self . _do_object ( \"DELETE\" , key , params = params , headers = headers ) logger . debug ( \"Delete object done\" ) return RequestResult ( resp ) async def list_objects ( # pylint: disable=too-many-arguments self , prefix : str = \"\" , delimiter : str = \"\" , marker : str = \"\" , max_keys : int = 100 , headers : Optional [ Dict ] = None , ) -> \"ListObjectsResult\" : \"\"\"list objects in a bucket (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Args: prefix (str, optional): only list objects start with this prefix. delimiter (str, optional): delimiter as a folder separator. marker (str, optional): use in paginate. max_keys (int, optional): numbers of objects for one page. headers (Optional[Dict], optional): HTTP headers to specify. Returns: ListObjectsResult: \"\"\" headers = CaseInsensitiveDict ( headers ) resp = await self . _do_object ( \"GET\" , \"\" , params = { \"prefix\" : prefix , \"delimiter\" : delimiter , \"marker\" : marker , \"max-keys\" : str ( max_keys ), \"encoding-type\" : \"url\" , }, headers = headers , ) logger . debug ( \"List objects done\" ) return await self . _parse_result ( resp , parse_list_objects , ListObjectsResult ) async def get_object_meta ( self , key : str , params : Optional [ Union [ dict , CaseInsensitiveDict ]] = None , headers : Optional [ Dict ] = None , ) -> \"GetObjectMetaResult\" : \"\"\"get meta data from object (use case) :: >>> result = await bucket.get_object_meta(object_name) >>> print(result.content_length ) 256 Args: key (str): object key params (Optional[Union[dict, CaseInsensitiveDict]], optional): headers (Optional[Dict], optional): HTTP headers to specify. Returns: GetObjectMetaResult: \"\"\" headers = CaseInsensitiveDict ( headers ) if params is None : params = {} if Bucket . OBJECTMETA not in params : params [ Bucket . OBJECTMETA ] = \"\" resp = await self . _do_object ( \"GET\" , key , params = params , headers = headers ) logger . debug ( \"Get object metadata done\" ) return GetObjectMetaResult ( resp ) async def object_exists ( self , key : str , headers : Optional [ Dict ] = None ) -> bool : \"\"\"Return True if key exists, False otherwise. raise Exception for other exceptions. (use case) :: >>> result = await bucket.object_exists('object1') >>> print(result) True Args: key (str): key of the object headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. Returns: bool: \"\"\" try : await self . get_object_meta ( key , headers = headers ) except NoSuchKey : return False return True async def get_bucket_info ( self ) -> GetBucketInfoResult : \"\"\"Get bucket information, `Create time`, `Endpoint`, `Owner`, `ACL` (use case) :: >>> resp = await aiobucket.get_bucket_info() >>> print(resp.name) 'bucket_name' Returns: GetBucketInfoResult \"\"\" resp = await self . _do_bucket ( \"GET\" , params = { Bucket . BUCKET_INFO : \"\" }) logger . debug ( \"Get bucket info done\" ) return await self . _parse_result ( resp , parse_get_bucket_info , GetBucketInfoResult ) async def append_object ( self , key : str , position : int , data , headers : Optional [ Dict ] = None , progress_callback : Optional [ Callable ] = None , init_crc : Optional [ int ] = None , ) -> \"AppendObjectResult\" : \"\"\"Append value to an object (use case) :: >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' >>> resp = await bucket.append_object('object1', 11, \"!!!\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world!!!' Args: key (str): key of the object position (int): position to append data (_type_): data to append headers (Optional[Dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. init_crc (Optional[int], optional): init value of the crc Returns: AppendObjectResult: \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), key ) data = make_adapter ( data , progress_callback = progress_callback , enable_crc = self . enable_crc , ) resp = await self . _do_object ( \"POST\" , key , data = data , headers = headers , params = { \"append\" : \"\" , \"position\" : str ( position )}, ) logger . debug ( \"Append object done\" ) result = AppendObjectResult ( resp ) if self . enable_crc and result . crc is not None and init_crc is not None : check_crc ( \"append object\" , data . crc , result . crc , result . request_id ) return result async def put_object_from_file ( self , key : str , filename : str , headers : Optional [ Mapping ] = None , progress_callback : Optional [ Callable ] = None , ) -> \"PutObjectResult\" : \"\"\"Upload a local file to a oss key (use case) :: >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" >>> result = await aiobucket.put_object_from_file(\"object1\", \"file\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' Args: key (str): key of the oss filename (str): filename to upload headers (Optional[Mapping], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. Returns: _type_: _description_ \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), filename ) with open ( to_unicode ( filename ), \"rb\" ) as f_stream : return await self . put_object ( key , f_stream , headers = headers , progress_callback = progress_callback , ) async def get_object_to_file ( self , key : str , filename : str , byte_range : Optional [ Sequence [ int ]] = None , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , progress_callback : Optional [ Callable ] = None , process : Optional [ Callable ] = None , params : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> AioGetObjectResult : \"\"\"Download contents of object to file. (use case) :: >>> result = await aiobucket.get_object_to_file(\"object1\", \"file\") >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" Args: key (str): object name to download. filename (str): filename to save the data downloaded. byte_range (Optional[Sequence[Optional[int]]], optional): Range to download. headers (Optional[dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. process (_type_, optional): oss file process method. params (Optional[Union[Dict, CaseInsensitiveDict]], optional): Returns: AioGetObjectResult: \"\"\" with open ( to_unicode ( filename ), \"wb\" ) as f_w : result = await self . get_object ( key , byte_range = byte_range , headers = headers , progress_callback = progress_callback , process = process , params = params , ) if result . content_length is None : copyfileobj ( result , f_w ) else : await copyfileobj_and_verify ( result , f_w , result . content_length , request_id = result . request_id , ) if self . enable_crc and byte_range is None : if ( ( headers is None ) or ( \"Accept-Encoding\" not in headers ) or ( headers [ \"Accept-Encoding\" ] != \"gzip\" ) ): check_crc ( \"get\" , result . client_crc , result . server_crc , result . request_id , ) return result async def batch_delete_objects ( self , key_list : List [ str ], headers : Optional [ Dict ] = None ) -> BatchDeleteObjectsResult : \"\"\"Delete a batch of objects (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' 'object3' 'object4' 'object5' 'object6' >>> await aiobucket.batch_delete_objects([\"object1\", \"object2\", \"object3\"]) >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object4' 'object5' 'object6' Args: key_list (List[str]): list of objects to delete. headers (Optional[Dict], optional): HTTP headers to specify. Raises: ClientError: Returns: BatchDeleteObjectResult: \"\"\" if not key_list : raise ClientError ( \"key_list should not be empty\" ) data = to_batch_delete_objects_request ( key_list , False ) header_dict : CaseInsensitiveDict = CaseInsensitiveDict ( headers ) header_dict [ \"Content-MD5\" ] = content_md5 ( data ) resp = await self . _do_object ( \"POST\" , \"\" , data = data , params = { \"delete\" : \"\" , \"encoding-type\" : \"url\" }, headers = header_dict , ) return await self . _parse_result ( resp , parse_batch_delete_objects , BatchDeleteObjectsResult ) async def copy_object ( self , source_bucket_name : str , source_key : str , target_key : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Dict ] = None , ) -> PutObjectResult : \"\"\"copy object to another place (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' >>> await aiobucket.copy_object(bucket_name, \"object1\", \"object2\") >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Args: source_bucket_name (str): source object bucket source_key (str): source object key target_key (str): target object key headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Dict], optional): Returns: PutObjectResult: \"\"\" headers = CaseInsensitiveDict ( headers ) if params and Bucket . VERSIONID in params : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) + \"?versionId=\" + params [ Bucket . VERSIONID ] ) else : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) ) resp = await self . _do_object ( \"PUT\" , target_key , headers = headers ) return PutObjectResult ( resp ) async def head_object ( self , key : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Mapping ] = None , ) -> \"HeadObjectResult\" : \"\"\"Get object metadata >>> result = bucket.head_object('readme.txt') >>> print(result.content_type) text/plain Args: key (str): object key headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Mapping], optional): Returns: HeadObjectResult: Raises: `NotFound <oss2.exceptions.NotFound>` if object does not exist. \"\"\" logger . debug ( \"Start to head object, bucket: %s , key: %s , headers: %s \" , self . bucket_name , to_string ( key ), headers , ) resp = await self . _do_object ( \"HEAD\" , key , headers = headers , params = params ) logger . debug ( \"Head object done, req_id: %s , status_code: %s \" , resp . request_id , resp . status , ) return await self . _parse_result ( resp , parse_dummy_result , HeadObjectResult ) async def abort_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> RequestResult : \"\"\"abort multipart uploading process\"\"\" return await abort_multipart_upload ( self , * args , ** kwargs ) async def complete_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"Complete multipart uploading process create a new file.\"\"\" return await complete_multipart_upload ( self , * args , ** kwargs ) async def init_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> InitMultipartUploadResult : \"\"\"initialize multipart uploading The returning upload id, bucket name and object name together defines this uploading event.\"\"\" return await init_multipart_upload ( self , * args , ** kwargs ) async def list_multipart_uploads ( self : \"AioBucket\" , * args , ** kwargs ) -> ListMultipartUploadsResult : \"\"\"List multipart uploading process\"\"\" return await list_multipart_uploads ( self , * args , ** kwargs ) async def list_parts ( self : \"AioBucket\" , * args , ** kwargs ) -> ListPartsResult : \"\"\"list uploaded parts in a part uploading progress.\"\"\" return await list_parts ( self , * args , ** kwargs ) async def upload_part ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"upload single part.\"\"\" return await upload_part ( self , * args , ** kwargs ) async def upload_part_copy ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"copy part or whole of a source file to a slice of a target file.\"\"\" return await upload_part_copy ( self , * args , ** kwargs ) __init__ ( auth , endpoint , bucket_name , is_cname = False , kwargs ) Parameters: Name Type Description Default bucket_name str the bucket name to operate required Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 def __init__ ( self , auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ], endpoint : str , bucket_name : str , is_cname : bool = False , ** kwargs , ): \"\"\" Args: bucket_name (str): the bucket name to operate \"\"\" self . bucket_name = bucket_name . strip () if is_valid_bucket_name ( self . bucket_name ) is not True : raise ClientError ( f \"The bucket_name ' { self . bucket_name } ' is invalid, please check it.\" ) super () . __init__ ( auth , endpoint , is_cname , ** kwargs , ) abort_multipart_upload ( args , kwargs ) async abort multipart uploading process Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 799 800 801 802 803 async def abort_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> RequestResult : \"\"\"abort multipart uploading process\"\"\" return await abort_multipart_upload ( self , * args , ** kwargs ) append_object ( key , position , data , headers = None , progress_callback = None , init_crc = None ) async Append value to an object (use case) :: >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' >>> resp = await bucket.append_object('object1', 11, \"!!!\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world!!!' Parameters: Name Type Description Default key str key of the object required position int position to append required data _type_ data to append required headers Optional [ Dict ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None init_crc Optional [ int ] init value of the crc None Returns: Name Type Description AppendObjectResult AppendObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 async def append_object ( self , key : str , position : int , data , headers : Optional [ Dict ] = None , progress_callback : Optional [ Callable ] = None , init_crc : Optional [ int ] = None , ) -> \"AppendObjectResult\" : \"\"\"Append value to an object (use case) :: >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' >>> resp = await bucket.append_object('object1', 11, \"!!!\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world!!!' Args: key (str): key of the object position (int): position to append data (_type_): data to append headers (Optional[Dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. init_crc (Optional[int], optional): init value of the crc Returns: AppendObjectResult: \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), key ) data = make_adapter ( data , progress_callback = progress_callback , enable_crc = self . enable_crc , ) resp = await self . _do_object ( \"POST\" , key , data = data , headers = headers , params = { \"append\" : \"\" , \"position\" : str ( position )}, ) logger . debug ( \"Append object done\" ) result = AppendObjectResult ( resp ) if self . enable_crc and result . crc is not None and init_crc is not None : check_crc ( \"append object\" , data . crc , result . crc , result . request_id ) return result batch_delete_objects ( key_list , headers = None ) async Delete a batch of objects (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' 'object3' 'object4' 'object5' 'object6' >>> await aiobucket.batch_delete_objects([\"object1\", \"object2\", \"object3\"]) >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object4' 'object5' 'object6' Parameters: Name Type Description Default key_list List [ str ] list of objects to delete. required headers Optional [ Dict ] HTTP headers to specify. None Raises: Type Description ClientError Returns: Name Type Description BatchDeleteObjectResult BatchDeleteObjectsResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 async def batch_delete_objects ( self , key_list : List [ str ], headers : Optional [ Dict ] = None ) -> BatchDeleteObjectsResult : \"\"\"Delete a batch of objects (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' 'object3' 'object4' 'object5' 'object6' >>> await aiobucket.batch_delete_objects([\"object1\", \"object2\", \"object3\"]) >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object4' 'object5' 'object6' Args: key_list (List[str]): list of objects to delete. headers (Optional[Dict], optional): HTTP headers to specify. Raises: ClientError: Returns: BatchDeleteObjectResult: \"\"\" if not key_list : raise ClientError ( \"key_list should not be empty\" ) data = to_batch_delete_objects_request ( key_list , False ) header_dict : CaseInsensitiveDict = CaseInsensitiveDict ( headers ) header_dict [ \"Content-MD5\" ] = content_md5 ( data ) resp = await self . _do_object ( \"POST\" , \"\" , data = data , params = { \"delete\" : \"\" , \"encoding-type\" : \"url\" }, headers = header_dict , ) return await self . _parse_result ( resp , parse_batch_delete_objects , BatchDeleteObjectsResult ) complete_multipart_upload ( args , kwargs ) async Complete multipart uploading process create a new file. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 805 806 807 808 809 async def complete_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"Complete multipart uploading process create a new file.\"\"\" return await complete_multipart_upload ( self , * args , ** kwargs ) copy_object ( source_bucket_name , source_key , target_key , headers = None , params = None ) async copy object to another place (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' >>> await aiobucket.copy_object(bucket_name, \"object1\", \"object2\") >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Parameters: Name Type Description Default source_bucket_name str source object bucket required source_key str source object key required target_key str target object key required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None params Optional [ Dict ] None Returns: Name Type Description PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 async def copy_object ( self , source_bucket_name : str , source_key : str , target_key : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Dict ] = None , ) -> PutObjectResult : \"\"\"copy object to another place (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' >>> await aiobucket.copy_object(bucket_name, \"object1\", \"object2\") >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Args: source_bucket_name (str): source object bucket source_key (str): source object key target_key (str): target object key headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Dict], optional): Returns: PutObjectResult: \"\"\" headers = CaseInsensitiveDict ( headers ) if params and Bucket . VERSIONID in params : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) + \"?versionId=\" + params [ Bucket . VERSIONID ] ) else : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) ) resp = await self . _do_object ( \"PUT\" , target_key , headers = headers ) return PutObjectResult ( resp ) delete_object ( key , params = None , headers = None ) async delete an object (use case) :: >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object1' 'object2' >>> await bucket.delete_object(\"object1\") >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object2' Parameters: Name Type Description Default key str description required headers Optional [ Dict ] HTTP headers to specify. None params Union [ Dict , CaseInsensitiveDict ] None Returns: Name Type Description RequestResult RequestResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 async def delete_object ( self , key : str , params : Union [ Dict , CaseInsensitiveDict ] = None , headers : Optional [ Dict ] = None , ) -> \"RequestResult\" : \"\"\"delete an object (use case) :: >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object1' 'object2' >>> await bucket.delete_object(\"object1\") >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object2' Args: key (str): _description_ headers (Optional[Dict], optional): HTTP headers to specify. params (Union[Dict, CaseInsensitiveDict], optional): Returns: RequestResult: \"\"\" resp = await self . _do_object ( \"DELETE\" , key , params = params , headers = headers ) logger . debug ( \"Delete object done\" ) return RequestResult ( resp ) get_bucket_info () async Get bucket information, Create time , Endpoint , Owner , ACL (use case) :: >>> resp = await aiobucket.get_bucket_info() >>> print(resp.name) 'bucket_name' Returns: Type Description GetBucketInfoResult GetBucketInfoResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 async def get_bucket_info ( self ) -> GetBucketInfoResult : \"\"\"Get bucket information, `Create time`, `Endpoint`, `Owner`, `ACL` (use case) :: >>> resp = await aiobucket.get_bucket_info() >>> print(resp.name) 'bucket_name' Returns: GetBucketInfoResult \"\"\" resp = await self . _do_bucket ( \"GET\" , params = { Bucket . BUCKET_INFO : \"\" }) logger . debug ( \"Get bucket info done\" ) return await self . _parse_result ( resp , parse_get_bucket_info , GetBucketInfoResult ) get_object ( key , byte_range = None , headers = None , progress_callback = None , process = None , params = None ) async download the contents of an object (use case) :: >>> async with await bucket.get_object(\"helloword\") as result >>> print(await result.read()) 'hello world' Parameters: Name Type Description Default key str object name to download. required byte_range Optional [ Sequence [ Optional [ int ]]] Range to download. None headers Optional [ dict ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None process _type_ oss file process method. None params Optional [ Union [ Dict , CaseInsensitiveDict ]] None Returns: Name Type Description AioGetObjectResult AioGetObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 async def get_object ( # pylint: disable=too-many-arguments self , key : str , byte_range : Optional [ Sequence [ Optional [ int ]]] = None , headers : Optional [ dict ] = None , progress_callback : Optional [ Callable ] = None , process = None , params : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> AioGetObjectResult : \"\"\"download the contents of an object (use case) :: >>> async with await bucket.get_object(\"helloword\") as result >>> print(await result.read()) 'hello world' Args: key (str): object name to download. byte_range (Optional[Sequence[Optional[int]]], optional): Range to download. headers (Optional[dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. process (_type_, optional): oss file process method. params (Optional[Union[Dict, CaseInsensitiveDict]], optional): Returns: AioGetObjectResult: \"\"\" headers_dict : CaseInsensitiveDict = CaseInsensitiveDict ( headers ) range_string = _make_range_string ( byte_range ) if range_string : headers_dict [ \"range\" ] = range_string params = {} if params is None else params if process : params [ Bucket . PROCESS ] = process resp = await self . _do_object ( \"GET\" , key , headers = headers_dict , params = params ) logger . debug ( \"Get object done\" ) return AioGetObjectResult ( resp , progress_callback , self . enable_crc ) get_object_meta ( key , params = None , headers = None ) async get meta data from object (use case) :: >>> result = await bucket.get_object_meta(object_name) >>> print(result.content_length ) 256 Parameters: Name Type Description Default key str object key required params Optional [ Union [ dict , CaseInsensitiveDict ]] None headers Optional [ Dict ] HTTP headers to specify. None Returns: Name Type Description GetObjectMetaResult GetObjectMetaResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 async def get_object_meta ( self , key : str , params : Optional [ Union [ dict , CaseInsensitiveDict ]] = None , headers : Optional [ Dict ] = None , ) -> \"GetObjectMetaResult\" : \"\"\"get meta data from object (use case) :: >>> result = await bucket.get_object_meta(object_name) >>> print(result.content_length ) 256 Args: key (str): object key params (Optional[Union[dict, CaseInsensitiveDict]], optional): headers (Optional[Dict], optional): HTTP headers to specify. Returns: GetObjectMetaResult: \"\"\" headers = CaseInsensitiveDict ( headers ) if params is None : params = {} if Bucket . OBJECTMETA not in params : params [ Bucket . OBJECTMETA ] = \"\" resp = await self . _do_object ( \"GET\" , key , params = params , headers = headers ) logger . debug ( \"Get object metadata done\" ) return GetObjectMetaResult ( resp ) get_object_to_file ( key , filename , byte_range = None , headers = None , progress_callback = None , process = None , params = None ) async Download contents of object to file. (use case) :: >>> result = await aiobucket.get_object_to_file(\"object1\", \"file\") >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" Parameters: Name Type Description Default key str object name to download. required filename str filename to save the data downloaded. required byte_range Optional [ Sequence [ Optional [ int ]]] Range to download. None headers Optional [ dict ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None process _type_ oss file process method. None params Optional [ Union [ Dict , CaseInsensitiveDict ]] None Returns: Name Type Description AioGetObjectResult AioGetObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 async def get_object_to_file ( self , key : str , filename : str , byte_range : Optional [ Sequence [ int ]] = None , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , progress_callback : Optional [ Callable ] = None , process : Optional [ Callable ] = None , params : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> AioGetObjectResult : \"\"\"Download contents of object to file. (use case) :: >>> result = await aiobucket.get_object_to_file(\"object1\", \"file\") >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" Args: key (str): object name to download. filename (str): filename to save the data downloaded. byte_range (Optional[Sequence[Optional[int]]], optional): Range to download. headers (Optional[dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. process (_type_, optional): oss file process method. params (Optional[Union[Dict, CaseInsensitiveDict]], optional): Returns: AioGetObjectResult: \"\"\" with open ( to_unicode ( filename ), \"wb\" ) as f_w : result = await self . get_object ( key , byte_range = byte_range , headers = headers , progress_callback = progress_callback , process = process , params = params , ) if result . content_length is None : copyfileobj ( result , f_w ) else : await copyfileobj_and_verify ( result , f_w , result . content_length , request_id = result . request_id , ) if self . enable_crc and byte_range is None : if ( ( headers is None ) or ( \"Accept-Encoding\" not in headers ) or ( headers [ \"Accept-Encoding\" ] != \"gzip\" ) ): check_crc ( \"get\" , result . client_crc , result . server_crc , result . request_id , ) return result head_object ( key , headers = None , params = None ) async Get object metadata >>> result = bucket.head_object('readme.txt') >>> print(result.content_type) text/plain Parameters: Name Type Description Default key str object key required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None params Optional [ Mapping ] None Returns: Name Type Description HeadObjectResult HeadObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 async def head_object ( self , key : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Mapping ] = None , ) -> \"HeadObjectResult\" : \"\"\"Get object metadata >>> result = bucket.head_object('readme.txt') >>> print(result.content_type) text/plain Args: key (str): object key headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Mapping], optional): Returns: HeadObjectResult: Raises: `NotFound <oss2.exceptions.NotFound>` if object does not exist. \"\"\" logger . debug ( \"Start to head object, bucket: %s , key: %s , headers: %s \" , self . bucket_name , to_string ( key ), headers , ) resp = await self . _do_object ( \"HEAD\" , key , headers = headers , params = params ) logger . debug ( \"Head object done, req_id: %s , status_code: %s \" , resp . request_id , resp . status , ) return await self . _parse_result ( resp , parse_dummy_result , HeadObjectResult ) init_multipart_upload ( args , kwargs ) async initialize multipart uploading The returning upload id, bucket name and object name together defines this uploading event. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 811 812 813 814 815 816 817 async def init_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> InitMultipartUploadResult : \"\"\"initialize multipart uploading The returning upload id, bucket name and object name together defines this uploading event.\"\"\" return await init_multipart_upload ( self , * args , ** kwargs ) list_multipart_uploads ( args , kwargs ) async List multipart uploading process Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 819 820 821 822 823 async def list_multipart_uploads ( self : \"AioBucket\" , * args , ** kwargs ) -> ListMultipartUploadsResult : \"\"\"List multipart uploading process\"\"\" return await list_multipart_uploads ( self , * args , ** kwargs ) list_objects ( prefix = '' , delimiter = '' , marker = '' , max_keys = 100 , headers = None ) async list objects in a bucket (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Parameters: Name Type Description Default prefix str only list objects start with this prefix. '' delimiter str delimiter as a folder separator. '' marker str use in paginate. '' max_keys int numbers of objects for one page. 100 headers Optional [ Dict ] HTTP headers to specify. None Returns: Name Type Description ListObjectsResult ListObjectsResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 async def list_objects ( # pylint: disable=too-many-arguments self , prefix : str = \"\" , delimiter : str = \"\" , marker : str = \"\" , max_keys : int = 100 , headers : Optional [ Dict ] = None , ) -> \"ListObjectsResult\" : \"\"\"list objects in a bucket (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Args: prefix (str, optional): only list objects start with this prefix. delimiter (str, optional): delimiter as a folder separator. marker (str, optional): use in paginate. max_keys (int, optional): numbers of objects for one page. headers (Optional[Dict], optional): HTTP headers to specify. Returns: ListObjectsResult: \"\"\" headers = CaseInsensitiveDict ( headers ) resp = await self . _do_object ( \"GET\" , \"\" , params = { \"prefix\" : prefix , \"delimiter\" : delimiter , \"marker\" : marker , \"max-keys\" : str ( max_keys ), \"encoding-type\" : \"url\" , }, headers = headers , ) logger . debug ( \"List objects done\" ) return await self . _parse_result ( resp , parse_list_objects , ListObjectsResult ) list_parts ( args , kwargs ) async list uploaded parts in a part uploading progress. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 825 826 827 async def list_parts ( self : \"AioBucket\" , * args , ** kwargs ) -> ListPartsResult : \"\"\"list uploaded parts in a part uploading progress.\"\"\" return await list_parts ( self , * args , ** kwargs ) object_exists ( key , headers = None ) async Return True if key exists, False otherwise. raise Exception for other exceptions. (use case) :: >>> result = await bucket.object_exists('object1') >>> print(result) True Parameters: Name Type Description Default key str key of the object required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] None Returns: Name Type Description bool bool Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 async def object_exists ( self , key : str , headers : Optional [ Dict ] = None ) -> bool : \"\"\"Return True if key exists, False otherwise. raise Exception for other exceptions. (use case) :: >>> result = await bucket.object_exists('object1') >>> print(result) True Args: key (str): key of the object headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. Returns: bool: \"\"\" try : await self . get_object_meta ( key , headers = headers ) except NoSuchKey : return False return True put_object ( key , data , headers = None , progress_callback = None ) async upload some contents to an object (use case) :: >>> await bucket.put_object('readme.txt', 'content of readme.txt') >>> with open(u'local_file.txt', 'rb') as f: >>> await bucket.put_object('remote_file.txt', f) Parameters: Name Type Description Default key str object name to upload required data Union [ str , bytes , IO , Iterable ] contents to upload required headers Optional [ Mapping ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None Returns: Name Type Description PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 async def put_object ( self , key : str , data , headers : Optional [ Mapping ] = None , progress_callback : Optional [ Callable ] = None , ) -> \"PutObjectResult\" : \"\"\"upload some contents to an object (use case) :: >>> await bucket.put_object('readme.txt', 'content of readme.txt') >>> with open(u'local_file.txt', 'rb') as f: >>> await bucket.put_object('remote_file.txt', f) Args: key (str): object name to upload data (Union[str, bytes, IO, Iterable]): contents to upload headers (Optional[Mapping], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. Returns: PutObjectResult: \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), key ) data = make_adapter ( data , progress_callback = progress_callback , enable_crc = self . enable_crc , ) resp : \"AioResponse\" = await self . _do_object ( \"PUT\" , key , data = data , headers = headers ) logger . debug ( \"Put object done\" ) result = PutObjectResult ( resp ) if self . enable_crc and result . crc is not None : check_crc ( \"put object\" , data . crc , result . crc , result . request_id ) return result put_object_from_file ( key , filename , headers = None , progress_callback = None ) async Upload a local file to a oss key (use case) :: >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" >>> result = await aiobucket.put_object_from_file(\"object1\", \"file\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' Parameters: Name Type Description Default key str key of the oss required filename str filename to upload required headers Optional [ Mapping ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None Returns: Name Type Description _type_ PutObjectResult description Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 async def put_object_from_file ( self , key : str , filename : str , headers : Optional [ Mapping ] = None , progress_callback : Optional [ Callable ] = None , ) -> \"PutObjectResult\" : \"\"\"Upload a local file to a oss key (use case) :: >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" >>> result = await aiobucket.put_object_from_file(\"object1\", \"file\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' Args: key (str): key of the oss filename (str): filename to upload headers (Optional[Mapping], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. Returns: _type_: _description_ \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), filename ) with open ( to_unicode ( filename ), \"rb\" ) as f_stream : return await self . put_object ( key , f_stream , headers = headers , progress_callback = progress_callback , ) upload_part ( args , kwargs ) async upload single part. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 829 830 831 async def upload_part ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"upload single part.\"\"\" return await upload_part ( self , * args , ** kwargs ) upload_part_copy ( args , kwargs ) async copy part or whole of a source file to a slice of a target file. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 833 834 835 async def upload_part_copy ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"copy part or whole of a source file to a slice of a target file.\"\"\" return await upload_part_copy ( self , * args , ** kwargs ) AioBucketIterator Bases: _AioBaseIterator Iterate over buckets of an user Return SimplifiedBucketInfo <oss2.models.SimplifiedBucketInfo> every iteration Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 class AioBucketIterator ( _AioBaseIterator ): \"\"\"Iterate over buckets of an user Return `SimplifiedBucketInfo <oss2.models.SimplifiedBucketInfo>` every iteration \"\"\" def __init__ ( self , service : \"AioService\" , prefix : str = \"\" , marker : str = \"\" , max_keys : int = 100 , max_retries : Optional [ int ] = None , ): \"\"\" Args: service (AioService): Service class of a special user. prefix (str, optional): prefix to filter the buckets results. marker (str, optional): paginate separator. max_keys (int, optional): max return number per page. max_retries (Optional[int], optional): max retry count. \"\"\" super () . __init__ ( marker , max_retries ) self . service = service self . prefix = prefix self . max_keys = max_keys async def _fetch ( self ): result = await self . service . list_buckets ( prefix = self . prefix , marker = self . next_marker , max_keys = self . max_keys ) self . entries : List [ \"SimplifiedBucketInfo\" ] = result . buckets return result . is_truncated , result . next_marker __init__ ( service , prefix = '' , marker = '' , max_keys = 100 , max_retries = None ) Parameters: Name Type Description Default service AioService Service class of a special user. required prefix str prefix to filter the buckets results. '' marker str paginate separator. '' max_keys int max return number per page. 100 max_retries Optional [ int ] max retry count. None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def __init__ ( self , service : \"AioService\" , prefix : str = \"\" , marker : str = \"\" , max_keys : int = 100 , max_retries : Optional [ int ] = None , ): \"\"\" Args: service (AioService): Service class of a special user. prefix (str, optional): prefix to filter the buckets results. marker (str, optional): paginate separator. max_keys (int, optional): max return number per page. max_retries (Optional[int], optional): max retry count. \"\"\" super () . __init__ ( marker , max_retries ) self . service = service self . prefix = prefix self . max_keys = max_keys AioObjectIterator Bases: _AioBaseIterator Iterator to iterate objects from a bucket. Return SimplifiedObjectInfo <oss2.models.SimplifiedObjectInfo> object. if SimplifiedObjectInfo.is_prefix() is true, the object returned is a directory. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class AioObjectIterator ( _AioBaseIterator ): \"\"\"Iterator to iterate objects from a bucket. Return `SimplifiedObjectInfo <oss2.models.SimplifiedObjectInfo>` object. if `SimplifiedObjectInfo.is_prefix()` is true, the object returned is a directory. \"\"\" def __init__ ( self , bucket : \"AioBucket\" , prefix : str = \"\" , delimiter : str = \"\" , marker : str = \"\" , max_keys : int = 100 , max_retries : Optional [ int ] = None , headers : Optional [ Dict ] = None , ): \"\"\"_summary_ Args: bucket (AioBucket): bucket class to iterate prefix (str, optional): prefix to filter the object results delimiter (str, optional): delimiter in object name marker (str, optional): paginate separator max_keys (int, optional): key number returns from `list_objects` every time,to notice that iterator can return more objects than it max_retries (Optional[int], optional): retry number headers (Optional[Dict], optional): HTTP header \"\"\" super () . __init__ ( marker , max_retries ) self . bucket = bucket self . prefix = prefix self . delimiter = delimiter self . max_keys = max_keys self . headers = http . CaseInsensitiveDict ( headers ) async def _fetch ( self ): result : ListObjectsResult = await self . bucket . list_objects ( prefix = self . prefix , delimiter = self . delimiter , marker = self . next_marker , max_keys = self . max_keys , headers = self . headers , ) self . entries : List [ \"SimplifiedObjectInfo\" ] = result . object_list + [ SimplifiedObjectInfo ( prefix , None , None , None , None , None ) for prefix in result . prefix_list ] self . entries . sort ( key = lambda obj : obj . key ) return result . is_truncated , result . next_marker __init__ ( bucket , prefix = '' , delimiter = '' , marker = '' , max_keys = 100 , max_retries = None , headers = None ) summary Parameters: Name Type Description Default bucket AioBucket bucket class to iterate required prefix str prefix to filter the object results '' delimiter str delimiter in object name '' marker str paginate separator '' max_keys int key number returns from list_objects 100 max_retries Optional [ int ] retry number None headers Optional [ Dict ] HTTP header None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def __init__ ( self , bucket : \"AioBucket\" , prefix : str = \"\" , delimiter : str = \"\" , marker : str = \"\" , max_keys : int = 100 , max_retries : Optional [ int ] = None , headers : Optional [ Dict ] = None , ): \"\"\"_summary_ Args: bucket (AioBucket): bucket class to iterate prefix (str, optional): prefix to filter the object results delimiter (str, optional): delimiter in object name marker (str, optional): paginate separator max_keys (int, optional): key number returns from `list_objects` every time,to notice that iterator can return more objects than it max_retries (Optional[int], optional): retry number headers (Optional[Dict], optional): HTTP header \"\"\" super () . __init__ ( marker , max_retries ) self . bucket = bucket self . prefix = prefix self . delimiter = delimiter self . max_keys = max_keys self . headers = http . CaseInsensitiveDict ( headers ) AioService Bases: _AioBase Service class used for operations like list all bucket Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 class AioService ( _AioBase ): \"\"\"Service class used for operations like list all bucket\"\"\" def __init__ ( self , auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ], endpoint : str , session : Optional [ AioSession ] = None , connect_timeout : Optional [ int ] = None , app_name : str = \"\" , proxies = None , ): \"\"\"_summary_ Args: auth (Union[Auth, AnonymousAuth, StsAuth]): Auth class. endpoint (str): endpoint address or CNAME. session (Optional[AioSession], optional): reuse a custom session. connect_timeout (int): connection. app_name (str, optional): app name. proxies (_type_, optional): proxies settings. \"\"\" super () . __init__ ( auth , endpoint , False , session , connect_timeout , app_name = app_name , proxies = proxies , ) async def list_buckets ( self , prefix : str = \"\" , marker : str = \"\" , max_keys : int = 100 , params : Optional [ Dict ] = None , ) -> ListBucketsResult : \"\"\"List buckets with given prefix of an user (use case) :: >>> async for obj in AioBucketIterator(aioservice): >>> print(obj.name) 'bucket_name1' 'bucket_name2' ... Args: prefix (str, optional): prefix to filter the buckets results. marker (str, optional): paginate separator. max_keys (int, optional): max return number per page. params (Optional[Dict], optional): Some optional params. Returns: oss2.models.ListBucketsResult: \"\"\" list_param = {} list_param [ \"prefix\" ] = prefix list_param [ \"marker\" ] = marker list_param [ \"max-keys\" ] = str ( max_keys ) if params is not None : if \"tag-key\" in params : list_param [ \"tag-key\" ] = params [ \"tag-key\" ] if \"tag-value\" in params : list_param [ \"tag-value\" ] = params [ \"tag-value\" ] resp = await self . _do ( \"GET\" , \"\" , \"\" , params = list_param ) logger . debug ( \"List buckets done\" ) return await self . _parse_result ( resp , parse_list_buckets , ListBucketsResult ) __init__ ( auth , endpoint , session = None , connect_timeout = None , app_name = '' , proxies = None ) summary Parameters: Name Type Description Default auth Union [ Auth , AnonymousAuth , StsAuth ] Auth class. required endpoint str endpoint address or CNAME. required session Optional [ AioSession ] reuse a custom session. None connect_timeout int connection. None app_name str app name. '' proxies _type_ proxies settings. None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 def __init__ ( self , auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ], endpoint : str , session : Optional [ AioSession ] = None , connect_timeout : Optional [ int ] = None , app_name : str = \"\" , proxies = None , ): \"\"\"_summary_ Args: auth (Union[Auth, AnonymousAuth, StsAuth]): Auth class. endpoint (str): endpoint address or CNAME. session (Optional[AioSession], optional): reuse a custom session. connect_timeout (int): connection. app_name (str, optional): app name. proxies (_type_, optional): proxies settings. \"\"\" super () . __init__ ( auth , endpoint , False , session , connect_timeout , app_name = app_name , proxies = proxies , ) list_buckets ( prefix = '' , marker = '' , max_keys = 100 , params = None ) async List buckets with given prefix of an user (use case) :: >>> async for obj in AioBucketIterator(aioservice): >>> print(obj.name) 'bucket_name1' 'bucket_name2' ... Parameters: Name Type Description Default prefix str prefix to filter the buckets results. '' marker str paginate separator. '' max_keys int max return number per page. 100 params Optional [ Dict ] Some optional params. None Returns: Type Description ListBucketsResult oss2.models.ListBucketsResult: Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 async def list_buckets ( self , prefix : str = \"\" , marker : str = \"\" , max_keys : int = 100 , params : Optional [ Dict ] = None , ) -> ListBucketsResult : \"\"\"List buckets with given prefix of an user (use case) :: >>> async for obj in AioBucketIterator(aioservice): >>> print(obj.name) 'bucket_name1' 'bucket_name2' ... Args: prefix (str, optional): prefix to filter the buckets results. marker (str, optional): paginate separator. max_keys (int, optional): max return number per page. params (Optional[Dict], optional): Some optional params. Returns: oss2.models.ListBucketsResult: \"\"\" list_param = {} list_param [ \"prefix\" ] = prefix list_param [ \"marker\" ] = marker list_param [ \"max-keys\" ] = str ( max_keys ) if params is not None : if \"tag-key\" in params : list_param [ \"tag-key\" ] = params [ \"tag-key\" ] if \"tag-value\" in params : list_param [ \"tag-value\" ] = params [ \"tag-value\" ] resp = await self . _do ( \"GET\" , \"\" , \"\" , params = list_param ) logger . debug ( \"List buckets done\" ) return await self . _parse_result ( resp , parse_list_buckets , ListBucketsResult ) AioSession Async session wrapper Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 class AioSession : \"\"\"Async session wrapper\"\"\" def __init__ ( self , psize : Optional [ int ] = None ): \"\"\" Args: psize: limit amount of simultaneously opened connections \"\"\" self . psize = psize or defaults . connection_pool_size self . session : Optional [ ClientSession ] = None self . conn : Optional [ TCPConnector ] = None async def do_request ( self , req : \"Request\" , timeout : Optional [ int ] = None ) -> \"AioResponse\" : \"\"\"Do request Args: req: request info timeout: timeout in seconds Raises: RequestError: Returns: AioResponse: Async Response wrapper \"\"\" logger . debug ( \"Send request, method: %s , url: %s , params: %s , headers: %s , \" \"timeout: %d \" , req . method , req . url , req . params , req . headers , timeout , ) try : assert self . session resp = await self . session . request ( req . method , req . url , data = req . data , params = req . params , headers = req . headers , timeout = timeout , ) return AioResponse ( resp ) except ClientResponseError as err : raise RequestError ( err ) from err async def __aenter__ ( self ): self . conn = TCPConnector ( limit = self . psize , limit_per_host = self . psize ) self . session = ClientSession ( connector = self . conn ) return self async def __aexit__ ( self , * args ): await self . close () async def close ( self ): \"\"\"gracefully close the AioSession class\"\"\" await self . conn . close () await self . session . close () @property def closed ( self ): \"\"\"Is client session closed. A readonly property. \"\"\" if self . session is None : return True return self . session . closed closed property Is client session closed. A readonly property. __init__ ( psize = None ) Parameters: Name Type Description Default psize Optional [ int ] limit amount of simultaneously opened connections None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 112 113 114 115 116 117 118 119 120 def __init__ ( self , psize : Optional [ int ] = None ): \"\"\" Args: psize: limit amount of simultaneously opened connections \"\"\" self . psize = psize or defaults . connection_pool_size self . session : Optional [ ClientSession ] = None self . conn : Optional [ TCPConnector ] = None close () async gracefully close the AioSession class Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 170 171 172 173 async def close ( self ): \"\"\"gracefully close the AioSession class\"\"\" await self . conn . close () await self . session . close () do_request ( req , timeout = None ) async Do request Parameters: Name Type Description Default req Request request info required timeout Optional [ int ] timeout in seconds None Raises: Type Description RequestError Returns: Name Type Description AioResponse AioResponse Async Response wrapper Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 async def do_request ( self , req : \"Request\" , timeout : Optional [ int ] = None ) -> \"AioResponse\" : \"\"\"Do request Args: req: request info timeout: timeout in seconds Raises: RequestError: Returns: AioResponse: Async Response wrapper \"\"\" logger . debug ( \"Send request, method: %s , url: %s , params: %s , headers: %s , \" \"timeout: %d \" , req . method , req . url , req . params , req . headers , timeout , ) try : assert self . session resp = await self . session . request ( req . method , req . url , data = req . data , params = req . params , headers = req . headers , timeout = timeout , ) return AioResponse ( resp ) except ClientResponseError as err : raise RequestError ( err ) from err resumable_download ( bucket , key , filename , store = None , headers = None , part_size = None , progress_callback = None , num_threads = None , params = None , multiget_threshold = None ) async Resumable download object to local file, implementation method is to create a list temporary files whose name is formed by the original filename with some random surfix. If the downloading was interrupted by some reasons, only those remaied parts need to be downloaded. Using CryptoBucket will make the download fallback to the normal one. Avoid using multi-threading/processing as the temp downloaded file might be covered. Parameters: Name Type Description Default bucket Union [ AioBucket , CryptoBucket ] bucket object to download required key Union [ str , bytes ] object key to store the file required filename Union [ str , bytes ] filename to download required store Optional [ ResumableStore ] ResumableStore object to keep the downloading info in the previous operation. None headers Optional [ Mapping ] HTTP headers to send. Defaults to None. download_part only accept OSS_REQUEST_PAYER get_object and get_object_to_file only accept OSS_REQUEST_PAYER, OSS_TRAFFIC_LIMIT None multipget_threshold Optional [ int ] threshold to use multipart download instead of a normal one. required part_size Optional [ int ] partition size of the multipart. None progress_callback Optional [ Callable ] callback function for progress bar. None num_threads Optional [ int ] concurrency number during the uploadinging None params Optional [ Mapping ] None Return None: Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/resumable.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 async def resumable_download ( # pylint: disable=too-many-arguments bucket : Union [ \"AioBucket\" , \"CryptoBucket\" ], key : Union [ str , bytes ], filename : Union [ str , bytes ], store : Optional [ \"ResumableStore\" ] = None , headers : Optional [ Mapping ] = None , part_size : Optional [ int ] = None , progress_callback : Optional [ Callable ] = None , num_threads : Optional [ int ] = None , params : Optional [ Mapping ] = None , multiget_threshold : Optional [ int ] = None , ) -> None : \"\"\"Resumable download object to local file, implementation method is to create a list temporary files whose name is formed by the original filename with some random surfix. If the downloading was interrupted by some reasons, only those remaied parts need to be downloaded. # Using `CryptoBucket` will make the download fallback to the normal one. # Avoid using multi-threading/processing as the temp downloaded file might # be covered. Args: bucket (Union[AioBucket, CryptoBucket]): bucket object to download key (Union[str, bytes]): object key to store the file filename (Union[str, bytes]): filename to download store (Optional[\"ResumableStore\"]): ResumableStore object to keep the downloading info in the previous operation. headers (Optional[Mapping]): HTTP headers to send. Defaults to None. # download_part only accept OSS_REQUEST_PAYER # get_object and get_object_to_file only accept OSS_REQUEST_PAYER, OSS_TRAFFIC_LIMIT multipget_threshold (Optional[int]): threshold to use multipart download instead of a normal one. part_size (Optional[int]): partition size of the multipart. progress_callback (Optional[Callable]): callback function for progress bar. num_threads (Optional[int]): concurrency number during the uploadinging params (Optional[Mapping]): Return: None: \"\"\" key_str : str = to_string ( key ) filename_str : str = to_unicode ( filename ) logger . debug ( \"Start to resumable download, bucket: %s , key: %s , filename: %s , \" \"multiget_threshold: %s , part_size: %s , num_threads: %s \" , bucket . bucket_name , key_str , filename_str , multiget_threshold , part_size , num_threads , ) multiget_threshold = multiget_threshold or MULTIGET_THRESHOLD valid_headers = _populate_valid_headers ( headers , [ OSS_REQUEST_PAYER , OSS_TRAFFIC_LIMIT ] ) result = await bucket . head_object ( key_str , params = params , headers = valid_headers ) logger . debug ( \"The size of object to download is: %s \" , result . content_length ) if result . content_length >= multiget_threshold : downloader = ResumableDownloader ( bucket , key , filename , _ObjectInfo . make ( result ), part_size = part_size , progress_callback = progress_callback , num_threads = num_threads , store = store , params = params , headers = valid_headers , ) await downloader . download ( result . server_crc ) else : await bucket . get_object_to_file ( key_str , filename_str , progress_callback = progress_callback , params = params , headers = valid_headers , ) resumable_upload ( bucket , key , filename , store = None , headers = None , multipart_threshold = None , part_size = None , progress_callback = None , num_threads = None , params = None ) async Resumable upload local file , The implementation is splitting local files to multipart, storing uploading information in local files. If the uploading was interrupted by some reasons, only those remaied parts need to be uploaded. Using CryptoBucket will make the upload fallback to the normal one. Parameters: Name Type Description Default bucket Union [ AioBucket , CryptoBucket ] bucket object to upload required key Union [ str , bytes ] object key to store the file required filename Union [ str , bytes ] filename to upload required store Optional [ ResumableStore ] ResumableStore object to keep the uploading info in the previous operation. Defaults to None. None headers Optional [ Mapping ] HTTP headers to send. Defaults to None. put_object or init_multipart_upload can make use of the whole headers uplpad_part only accept OSS_REQUEST_PAYER, OSS_TRAFFIC_LIMIT complete_multipart_upload only accept OSS_REQUEST_PAYER, OSS_OBJECT_ACL None multipart_threshold Optional [ int ] threshold to use multipart upload instead of a normal one. Defaults to None. None part_size Optional [ int ] partition size of the multipart. Defaults to None. None progress_callback Optional [ Callable ] callback function for progress bar. Defaults to None. None num_threads Optional [ int ] concurrency number during the uploading Defaults to None. None params Optional [ Mapping ] Defaults to None. None Returns: Name Type Description PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/resumable.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 async def resumable_upload ( # pylint: disable=too-many-arguments bucket : Union [ \"AioBucket\" , \"CryptoBucket\" ], key : Union [ str , bytes ], filename : Union [ str , bytes ], store : Optional [ \"ResumableStore\" ] = None , headers : Optional [ Mapping ] = None , multipart_threshold : Optional [ int ] = None , part_size : Optional [ int ] = None , progress_callback : Optional [ Callable ] = None , num_threads : Optional [ int ] = None , params : Optional [ Mapping ] = None , ) -> \"PutObjectResult\" : \"\"\"Resumable upload local file , The implementation is splitting local files to multipart, storing uploading information in local files. If the uploading was interrupted by some reasons, only those remaied parts need to be uploaded. # Using `CryptoBucket` will make the upload fallback to the normal one. Args: bucket (Union[AioBucket, CryptoBucket]): bucket object to upload key (Union[str, bytes]): object key to store the file filename (Union[str, bytes]): filename to upload store (Optional[\"ResumableStore\"]): ResumableStore object to keep the uploading info in the previous operation. Defaults to None. headers (Optional[Mapping]): HTTP headers to send. Defaults to None. # put_object or init_multipart_upload can make use of the whole headers # uplpad_part only accept OSS_REQUEST_PAYER, OSS_TRAFFIC_LIMIT # complete_multipart_upload only accept OSS_REQUEST_PAYER, OSS_OBJECT_ACL multipart_threshold (Optional[int]): threshold to use multipart upload instead of a normal one. Defaults to None. part_size (Optional[int]): partition size of the multipart. Defaults to None. progress_callback (Optional[Callable]): callback function for progress bar. Defaults to None. num_threads (Optional[int]): concurrency number during the uploading Defaults to None. params (Optional[Mapping]): Defaults to None. Returns: PutObjectResult: \"\"\" key_str = to_string ( key ) filename_str = to_unicode ( filename ) size = os . path . getsize ( filename_str ) logger . debug ( \"Start to resumable upload, bucket: %s , key: %s , filename: %s , \" \"headers: %s , multipart_threshold: %s , part_size: %s , \" \"num_threads: %s , size of file to upload is %s \" , bucket . bucket_name , key_str , filename_str , headers , multipart_threshold , part_size , num_threads , size , ) multipart_threshold = multipart_threshold or MULTIPART_THRESHOLD num_threads = num_threads or MULTIPART_NUM_THREADS part_size = part_size or PART_SIZE if size >= multipart_threshold and not isinstance ( bucket , CryptoBucket ): store = store or ResumableStore () uploader = ResumableUploader ( bucket , key_str , filename_str , size , store , part_size = part_size , headers = headers , progress_callback = progress_callback , num_threads = num_threads , params = params , ) result = await uploader . upload () else : result = await bucket . put_object_from_file ( key_str , filename_str , headers = headers , progress_callback = progress_callback , ) return result","title":"Aiooss2"},{"location":"reference/aiooss2/#aiooss2.AioBucket","text":"Bases: _AioBase Used for Bucket and Object operations, creating\u3001deleting Bucket, uploading\u3001downloading Object, etc\u3002 use case (bucket in HangZhou area):: import oss2 import aiooss2 import asyncio auth = oss2.Auth('your-access-key-id', 'your-access-key-secret') bucket = aiooss2.Bucket(auth, 'http://oss-cn-hangzhou.aliyuncs.com', 'your-bucket') def upload(): data = b\"\u0001\" * 1024 resp = await bucket.put_object('readme.txt', 'content of the object') return resp loop = asyncio.get_event_loop() loop.run_until_complete(upload()) Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 class AioBucket ( _AioBase ): \"\"\"Used for Bucket and Object operations, creating\u3001deleting Bucket, uploading\u3001downloading Object, etc\u3002 use case (bucket in HangZhou area):: >>> import oss2 >>> import aiooss2 >>> import asyncio >>> auth = oss2.Auth('your-access-key-id', 'your-access-key-secret') >>> bucket = aiooss2.Bucket(auth, 'http://oss-cn-hangzhou.aliyuncs.com', >>> 'your-bucket') >>> def upload(): >>> data = b\"\\x01\" * 1024 >>> resp = await bucket.put_object('readme.txt', >>> 'content of the object') >>> return resp >>> loop = asyncio.get_event_loop() >>> loop.run_until_complete(upload()) <oss2.models.PutObjectResult object at 0x029B9930> \"\"\" auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ] def __init__ ( self , auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ], endpoint : str , bucket_name : str , is_cname : bool = False , ** kwargs , ): \"\"\" Args: bucket_name (str): the bucket name to operate \"\"\" self . bucket_name = bucket_name . strip () if is_valid_bucket_name ( self . bucket_name ) is not True : raise ClientError ( f \"The bucket_name ' { self . bucket_name } ' is invalid, please check it.\" ) super () . __init__ ( auth , endpoint , is_cname , ** kwargs , ) async def _do_object ( self , method : str , key : Union [ bytes , str ], ** kwargs ) -> \"AioResponse\" : return await self . _do ( method , self . bucket_name , key , ** kwargs ) async def _do_bucket ( self , method : str , ** kwargs ) -> \"AioResponse\" : return await self . _do ( method , self . bucket_name , \"\" , ** kwargs ) async def put_object ( self , key : str , data , headers : Optional [ Mapping ] = None , progress_callback : Optional [ Callable ] = None , ) -> \"PutObjectResult\" : \"\"\"upload some contents to an object (use case) :: >>> await bucket.put_object('readme.txt', 'content of readme.txt') >>> with open(u'local_file.txt', 'rb') as f: >>> await bucket.put_object('remote_file.txt', f) Args: key (str): object name to upload data (Union[str, bytes, IO, Iterable]): contents to upload headers (Optional[Mapping], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. Returns: PutObjectResult: \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), key ) data = make_adapter ( data , progress_callback = progress_callback , enable_crc = self . enable_crc , ) resp : \"AioResponse\" = await self . _do_object ( \"PUT\" , key , data = data , headers = headers ) logger . debug ( \"Put object done\" ) result = PutObjectResult ( resp ) if self . enable_crc and result . crc is not None : check_crc ( \"put object\" , data . crc , result . crc , result . request_id ) return result async def get_object ( # pylint: disable=too-many-arguments self , key : str , byte_range : Optional [ Sequence [ Optional [ int ]]] = None , headers : Optional [ dict ] = None , progress_callback : Optional [ Callable ] = None , process = None , params : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> AioGetObjectResult : \"\"\"download the contents of an object (use case) :: >>> async with await bucket.get_object(\"helloword\") as result >>> print(await result.read()) 'hello world' Args: key (str): object name to download. byte_range (Optional[Sequence[Optional[int]]], optional): Range to download. headers (Optional[dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. process (_type_, optional): oss file process method. params (Optional[Union[Dict, CaseInsensitiveDict]], optional): Returns: AioGetObjectResult: \"\"\" headers_dict : CaseInsensitiveDict = CaseInsensitiveDict ( headers ) range_string = _make_range_string ( byte_range ) if range_string : headers_dict [ \"range\" ] = range_string params = {} if params is None else params if process : params [ Bucket . PROCESS ] = process resp = await self . _do_object ( \"GET\" , key , headers = headers_dict , params = params ) logger . debug ( \"Get object done\" ) return AioGetObjectResult ( resp , progress_callback , self . enable_crc ) async def delete_object ( self , key : str , params : Union [ Dict , CaseInsensitiveDict ] = None , headers : Optional [ Dict ] = None , ) -> \"RequestResult\" : \"\"\"delete an object (use case) :: >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object1' 'object2' >>> await bucket.delete_object(\"object1\") >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object2' Args: key (str): _description_ headers (Optional[Dict], optional): HTTP headers to specify. params (Union[Dict, CaseInsensitiveDict], optional): Returns: RequestResult: \"\"\" resp = await self . _do_object ( \"DELETE\" , key , params = params , headers = headers ) logger . debug ( \"Delete object done\" ) return RequestResult ( resp ) async def list_objects ( # pylint: disable=too-many-arguments self , prefix : str = \"\" , delimiter : str = \"\" , marker : str = \"\" , max_keys : int = 100 , headers : Optional [ Dict ] = None , ) -> \"ListObjectsResult\" : \"\"\"list objects in a bucket (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Args: prefix (str, optional): only list objects start with this prefix. delimiter (str, optional): delimiter as a folder separator. marker (str, optional): use in paginate. max_keys (int, optional): numbers of objects for one page. headers (Optional[Dict], optional): HTTP headers to specify. Returns: ListObjectsResult: \"\"\" headers = CaseInsensitiveDict ( headers ) resp = await self . _do_object ( \"GET\" , \"\" , params = { \"prefix\" : prefix , \"delimiter\" : delimiter , \"marker\" : marker , \"max-keys\" : str ( max_keys ), \"encoding-type\" : \"url\" , }, headers = headers , ) logger . debug ( \"List objects done\" ) return await self . _parse_result ( resp , parse_list_objects , ListObjectsResult ) async def get_object_meta ( self , key : str , params : Optional [ Union [ dict , CaseInsensitiveDict ]] = None , headers : Optional [ Dict ] = None , ) -> \"GetObjectMetaResult\" : \"\"\"get meta data from object (use case) :: >>> result = await bucket.get_object_meta(object_name) >>> print(result.content_length ) 256 Args: key (str): object key params (Optional[Union[dict, CaseInsensitiveDict]], optional): headers (Optional[Dict], optional): HTTP headers to specify. Returns: GetObjectMetaResult: \"\"\" headers = CaseInsensitiveDict ( headers ) if params is None : params = {} if Bucket . OBJECTMETA not in params : params [ Bucket . OBJECTMETA ] = \"\" resp = await self . _do_object ( \"GET\" , key , params = params , headers = headers ) logger . debug ( \"Get object metadata done\" ) return GetObjectMetaResult ( resp ) async def object_exists ( self , key : str , headers : Optional [ Dict ] = None ) -> bool : \"\"\"Return True if key exists, False otherwise. raise Exception for other exceptions. (use case) :: >>> result = await bucket.object_exists('object1') >>> print(result) True Args: key (str): key of the object headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. Returns: bool: \"\"\" try : await self . get_object_meta ( key , headers = headers ) except NoSuchKey : return False return True async def get_bucket_info ( self ) -> GetBucketInfoResult : \"\"\"Get bucket information, `Create time`, `Endpoint`, `Owner`, `ACL` (use case) :: >>> resp = await aiobucket.get_bucket_info() >>> print(resp.name) 'bucket_name' Returns: GetBucketInfoResult \"\"\" resp = await self . _do_bucket ( \"GET\" , params = { Bucket . BUCKET_INFO : \"\" }) logger . debug ( \"Get bucket info done\" ) return await self . _parse_result ( resp , parse_get_bucket_info , GetBucketInfoResult ) async def append_object ( self , key : str , position : int , data , headers : Optional [ Dict ] = None , progress_callback : Optional [ Callable ] = None , init_crc : Optional [ int ] = None , ) -> \"AppendObjectResult\" : \"\"\"Append value to an object (use case) :: >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' >>> resp = await bucket.append_object('object1', 11, \"!!!\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world!!!' Args: key (str): key of the object position (int): position to append data (_type_): data to append headers (Optional[Dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. init_crc (Optional[int], optional): init value of the crc Returns: AppendObjectResult: \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), key ) data = make_adapter ( data , progress_callback = progress_callback , enable_crc = self . enable_crc , ) resp = await self . _do_object ( \"POST\" , key , data = data , headers = headers , params = { \"append\" : \"\" , \"position\" : str ( position )}, ) logger . debug ( \"Append object done\" ) result = AppendObjectResult ( resp ) if self . enable_crc and result . crc is not None and init_crc is not None : check_crc ( \"append object\" , data . crc , result . crc , result . request_id ) return result async def put_object_from_file ( self , key : str , filename : str , headers : Optional [ Mapping ] = None , progress_callback : Optional [ Callable ] = None , ) -> \"PutObjectResult\" : \"\"\"Upload a local file to a oss key (use case) :: >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" >>> result = await aiobucket.put_object_from_file(\"object1\", \"file\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' Args: key (str): key of the oss filename (str): filename to upload headers (Optional[Mapping], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. Returns: _type_: _description_ \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), filename ) with open ( to_unicode ( filename ), \"rb\" ) as f_stream : return await self . put_object ( key , f_stream , headers = headers , progress_callback = progress_callback , ) async def get_object_to_file ( self , key : str , filename : str , byte_range : Optional [ Sequence [ int ]] = None , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , progress_callback : Optional [ Callable ] = None , process : Optional [ Callable ] = None , params : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> AioGetObjectResult : \"\"\"Download contents of object to file. (use case) :: >>> result = await aiobucket.get_object_to_file(\"object1\", \"file\") >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" Args: key (str): object name to download. filename (str): filename to save the data downloaded. byte_range (Optional[Sequence[Optional[int]]], optional): Range to download. headers (Optional[dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. process (_type_, optional): oss file process method. params (Optional[Union[Dict, CaseInsensitiveDict]], optional): Returns: AioGetObjectResult: \"\"\" with open ( to_unicode ( filename ), \"wb\" ) as f_w : result = await self . get_object ( key , byte_range = byte_range , headers = headers , progress_callback = progress_callback , process = process , params = params , ) if result . content_length is None : copyfileobj ( result , f_w ) else : await copyfileobj_and_verify ( result , f_w , result . content_length , request_id = result . request_id , ) if self . enable_crc and byte_range is None : if ( ( headers is None ) or ( \"Accept-Encoding\" not in headers ) or ( headers [ \"Accept-Encoding\" ] != \"gzip\" ) ): check_crc ( \"get\" , result . client_crc , result . server_crc , result . request_id , ) return result async def batch_delete_objects ( self , key_list : List [ str ], headers : Optional [ Dict ] = None ) -> BatchDeleteObjectsResult : \"\"\"Delete a batch of objects (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' 'object3' 'object4' 'object5' 'object6' >>> await aiobucket.batch_delete_objects([\"object1\", \"object2\", \"object3\"]) >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object4' 'object5' 'object6' Args: key_list (List[str]): list of objects to delete. headers (Optional[Dict], optional): HTTP headers to specify. Raises: ClientError: Returns: BatchDeleteObjectResult: \"\"\" if not key_list : raise ClientError ( \"key_list should not be empty\" ) data = to_batch_delete_objects_request ( key_list , False ) header_dict : CaseInsensitiveDict = CaseInsensitiveDict ( headers ) header_dict [ \"Content-MD5\" ] = content_md5 ( data ) resp = await self . _do_object ( \"POST\" , \"\" , data = data , params = { \"delete\" : \"\" , \"encoding-type\" : \"url\" }, headers = header_dict , ) return await self . _parse_result ( resp , parse_batch_delete_objects , BatchDeleteObjectsResult ) async def copy_object ( self , source_bucket_name : str , source_key : str , target_key : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Dict ] = None , ) -> PutObjectResult : \"\"\"copy object to another place (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' >>> await aiobucket.copy_object(bucket_name, \"object1\", \"object2\") >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Args: source_bucket_name (str): source object bucket source_key (str): source object key target_key (str): target object key headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Dict], optional): Returns: PutObjectResult: \"\"\" headers = CaseInsensitiveDict ( headers ) if params and Bucket . VERSIONID in params : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) + \"?versionId=\" + params [ Bucket . VERSIONID ] ) else : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) ) resp = await self . _do_object ( \"PUT\" , target_key , headers = headers ) return PutObjectResult ( resp ) async def head_object ( self , key : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Mapping ] = None , ) -> \"HeadObjectResult\" : \"\"\"Get object metadata >>> result = bucket.head_object('readme.txt') >>> print(result.content_type) text/plain Args: key (str): object key headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Mapping], optional): Returns: HeadObjectResult: Raises: `NotFound <oss2.exceptions.NotFound>` if object does not exist. \"\"\" logger . debug ( \"Start to head object, bucket: %s , key: %s , headers: %s \" , self . bucket_name , to_string ( key ), headers , ) resp = await self . _do_object ( \"HEAD\" , key , headers = headers , params = params ) logger . debug ( \"Head object done, req_id: %s , status_code: %s \" , resp . request_id , resp . status , ) return await self . _parse_result ( resp , parse_dummy_result , HeadObjectResult ) async def abort_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> RequestResult : \"\"\"abort multipart uploading process\"\"\" return await abort_multipart_upload ( self , * args , ** kwargs ) async def complete_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"Complete multipart uploading process create a new file.\"\"\" return await complete_multipart_upload ( self , * args , ** kwargs ) async def init_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> InitMultipartUploadResult : \"\"\"initialize multipart uploading The returning upload id, bucket name and object name together defines this uploading event.\"\"\" return await init_multipart_upload ( self , * args , ** kwargs ) async def list_multipart_uploads ( self : \"AioBucket\" , * args , ** kwargs ) -> ListMultipartUploadsResult : \"\"\"List multipart uploading process\"\"\" return await list_multipart_uploads ( self , * args , ** kwargs ) async def list_parts ( self : \"AioBucket\" , * args , ** kwargs ) -> ListPartsResult : \"\"\"list uploaded parts in a part uploading progress.\"\"\" return await list_parts ( self , * args , ** kwargs ) async def upload_part ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"upload single part.\"\"\" return await upload_part ( self , * args , ** kwargs ) async def upload_part_copy ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"copy part or whole of a source file to a slice of a target file.\"\"\" return await upload_part_copy ( self , * args , ** kwargs )","title":"AioBucket"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.__init__","text":"Parameters: Name Type Description Default bucket_name str the bucket name to operate required Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 def __init__ ( self , auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ], endpoint : str , bucket_name : str , is_cname : bool = False , ** kwargs , ): \"\"\" Args: bucket_name (str): the bucket name to operate \"\"\" self . bucket_name = bucket_name . strip () if is_valid_bucket_name ( self . bucket_name ) is not True : raise ClientError ( f \"The bucket_name ' { self . bucket_name } ' is invalid, please check it.\" ) super () . __init__ ( auth , endpoint , is_cname , ** kwargs , )","title":"__init__()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.abort_multipart_upload","text":"abort multipart uploading process Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 799 800 801 802 803 async def abort_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> RequestResult : \"\"\"abort multipart uploading process\"\"\" return await abort_multipart_upload ( self , * args , ** kwargs )","title":"abort_multipart_upload()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.append_object","text":"Append value to an object (use case) :: >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' >>> resp = await bucket.append_object('object1', 11, \"!!!\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world!!!' Parameters: Name Type Description Default key str key of the object required position int position to append required data _type_ data to append required headers Optional [ Dict ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None init_crc Optional [ int ] init value of the crc None Returns: Name Type Description AppendObjectResult AppendObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 async def append_object ( self , key : str , position : int , data , headers : Optional [ Dict ] = None , progress_callback : Optional [ Callable ] = None , init_crc : Optional [ int ] = None , ) -> \"AppendObjectResult\" : \"\"\"Append value to an object (use case) :: >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' >>> resp = await bucket.append_object('object1', 11, \"!!!\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world!!!' Args: key (str): key of the object position (int): position to append data (_type_): data to append headers (Optional[Dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. init_crc (Optional[int], optional): init value of the crc Returns: AppendObjectResult: \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), key ) data = make_adapter ( data , progress_callback = progress_callback , enable_crc = self . enable_crc , ) resp = await self . _do_object ( \"POST\" , key , data = data , headers = headers , params = { \"append\" : \"\" , \"position\" : str ( position )}, ) logger . debug ( \"Append object done\" ) result = AppendObjectResult ( resp ) if self . enable_crc and result . crc is not None and init_crc is not None : check_crc ( \"append object\" , data . crc , result . crc , result . request_id ) return result","title":"append_object()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.batch_delete_objects","text":"Delete a batch of objects (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' 'object3' 'object4' 'object5' 'object6' >>> await aiobucket.batch_delete_objects([\"object1\", \"object2\", \"object3\"]) >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object4' 'object5' 'object6' Parameters: Name Type Description Default key_list List [ str ] list of objects to delete. required headers Optional [ Dict ] HTTP headers to specify. None Raises: Type Description ClientError Returns: Name Type Description BatchDeleteObjectResult BatchDeleteObjectsResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 async def batch_delete_objects ( self , key_list : List [ str ], headers : Optional [ Dict ] = None ) -> BatchDeleteObjectsResult : \"\"\"Delete a batch of objects (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' 'object3' 'object4' 'object5' 'object6' >>> await aiobucket.batch_delete_objects([\"object1\", \"object2\", \"object3\"]) >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object4' 'object5' 'object6' Args: key_list (List[str]): list of objects to delete. headers (Optional[Dict], optional): HTTP headers to specify. Raises: ClientError: Returns: BatchDeleteObjectResult: \"\"\" if not key_list : raise ClientError ( \"key_list should not be empty\" ) data = to_batch_delete_objects_request ( key_list , False ) header_dict : CaseInsensitiveDict = CaseInsensitiveDict ( headers ) header_dict [ \"Content-MD5\" ] = content_md5 ( data ) resp = await self . _do_object ( \"POST\" , \"\" , data = data , params = { \"delete\" : \"\" , \"encoding-type\" : \"url\" }, headers = header_dict , ) return await self . _parse_result ( resp , parse_batch_delete_objects , BatchDeleteObjectsResult )","title":"batch_delete_objects()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.complete_multipart_upload","text":"Complete multipart uploading process create a new file. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 805 806 807 808 809 async def complete_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"Complete multipart uploading process create a new file.\"\"\" return await complete_multipart_upload ( self , * args , ** kwargs )","title":"complete_multipart_upload()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.copy_object","text":"copy object to another place (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' >>> await aiobucket.copy_object(bucket_name, \"object1\", \"object2\") >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Parameters: Name Type Description Default source_bucket_name str source object bucket required source_key str source object key required target_key str target object key required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None params Optional [ Dict ] None Returns: Name Type Description PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 async def copy_object ( self , source_bucket_name : str , source_key : str , target_key : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Dict ] = None , ) -> PutObjectResult : \"\"\"copy object to another place (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' >>> await aiobucket.copy_object(bucket_name, \"object1\", \"object2\") >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Args: source_bucket_name (str): source object bucket source_key (str): source object key target_key (str): target object key headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Dict], optional): Returns: PutObjectResult: \"\"\" headers = CaseInsensitiveDict ( headers ) if params and Bucket . VERSIONID in params : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) + \"?versionId=\" + params [ Bucket . VERSIONID ] ) else : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) ) resp = await self . _do_object ( \"PUT\" , target_key , headers = headers ) return PutObjectResult ( resp )","title":"copy_object()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.delete_object","text":"delete an object (use case) :: >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object1' 'object2' >>> await bucket.delete_object(\"object1\") >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object2' Parameters: Name Type Description Default key str description required headers Optional [ Dict ] HTTP headers to specify. None params Union [ Dict , CaseInsensitiveDict ] None Returns: Name Type Description RequestResult RequestResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 async def delete_object ( self , key : str , params : Union [ Dict , CaseInsensitiveDict ] = None , headers : Optional [ Dict ] = None , ) -> \"RequestResult\" : \"\"\"delete an object (use case) :: >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object1' 'object2' >>> await bucket.delete_object(\"object1\") >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object2' Args: key (str): _description_ headers (Optional[Dict], optional): HTTP headers to specify. params (Union[Dict, CaseInsensitiveDict], optional): Returns: RequestResult: \"\"\" resp = await self . _do_object ( \"DELETE\" , key , params = params , headers = headers ) logger . debug ( \"Delete object done\" ) return RequestResult ( resp )","title":"delete_object()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.get_bucket_info","text":"Get bucket information, Create time , Endpoint , Owner , ACL (use case) :: >>> resp = await aiobucket.get_bucket_info() >>> print(resp.name) 'bucket_name' Returns: Type Description GetBucketInfoResult GetBucketInfoResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 async def get_bucket_info ( self ) -> GetBucketInfoResult : \"\"\"Get bucket information, `Create time`, `Endpoint`, `Owner`, `ACL` (use case) :: >>> resp = await aiobucket.get_bucket_info() >>> print(resp.name) 'bucket_name' Returns: GetBucketInfoResult \"\"\" resp = await self . _do_bucket ( \"GET\" , params = { Bucket . BUCKET_INFO : \"\" }) logger . debug ( \"Get bucket info done\" ) return await self . _parse_result ( resp , parse_get_bucket_info , GetBucketInfoResult )","title":"get_bucket_info()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.get_object","text":"download the contents of an object (use case) :: >>> async with await bucket.get_object(\"helloword\") as result >>> print(await result.read()) 'hello world' Parameters: Name Type Description Default key str object name to download. required byte_range Optional [ Sequence [ Optional [ int ]]] Range to download. None headers Optional [ dict ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None process _type_ oss file process method. None params Optional [ Union [ Dict , CaseInsensitiveDict ]] None Returns: Name Type Description AioGetObjectResult AioGetObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 async def get_object ( # pylint: disable=too-many-arguments self , key : str , byte_range : Optional [ Sequence [ Optional [ int ]]] = None , headers : Optional [ dict ] = None , progress_callback : Optional [ Callable ] = None , process = None , params : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> AioGetObjectResult : \"\"\"download the contents of an object (use case) :: >>> async with await bucket.get_object(\"helloword\") as result >>> print(await result.read()) 'hello world' Args: key (str): object name to download. byte_range (Optional[Sequence[Optional[int]]], optional): Range to download. headers (Optional[dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. process (_type_, optional): oss file process method. params (Optional[Union[Dict, CaseInsensitiveDict]], optional): Returns: AioGetObjectResult: \"\"\" headers_dict : CaseInsensitiveDict = CaseInsensitiveDict ( headers ) range_string = _make_range_string ( byte_range ) if range_string : headers_dict [ \"range\" ] = range_string params = {} if params is None else params if process : params [ Bucket . PROCESS ] = process resp = await self . _do_object ( \"GET\" , key , headers = headers_dict , params = params ) logger . debug ( \"Get object done\" ) return AioGetObjectResult ( resp , progress_callback , self . enable_crc )","title":"get_object()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.get_object_meta","text":"get meta data from object (use case) :: >>> result = await bucket.get_object_meta(object_name) >>> print(result.content_length ) 256 Parameters: Name Type Description Default key str object key required params Optional [ Union [ dict , CaseInsensitiveDict ]] None headers Optional [ Dict ] HTTP headers to specify. None Returns: Name Type Description GetObjectMetaResult GetObjectMetaResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 async def get_object_meta ( self , key : str , params : Optional [ Union [ dict , CaseInsensitiveDict ]] = None , headers : Optional [ Dict ] = None , ) -> \"GetObjectMetaResult\" : \"\"\"get meta data from object (use case) :: >>> result = await bucket.get_object_meta(object_name) >>> print(result.content_length ) 256 Args: key (str): object key params (Optional[Union[dict, CaseInsensitiveDict]], optional): headers (Optional[Dict], optional): HTTP headers to specify. Returns: GetObjectMetaResult: \"\"\" headers = CaseInsensitiveDict ( headers ) if params is None : params = {} if Bucket . OBJECTMETA not in params : params [ Bucket . OBJECTMETA ] = \"\" resp = await self . _do_object ( \"GET\" , key , params = params , headers = headers ) logger . debug ( \"Get object metadata done\" ) return GetObjectMetaResult ( resp )","title":"get_object_meta()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.get_object_to_file","text":"Download contents of object to file. (use case) :: >>> result = await aiobucket.get_object_to_file(\"object1\", \"file\") >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" Parameters: Name Type Description Default key str object name to download. required filename str filename to save the data downloaded. required byte_range Optional [ Sequence [ Optional [ int ]]] Range to download. None headers Optional [ dict ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None process _type_ oss file process method. None params Optional [ Union [ Dict , CaseInsensitiveDict ]] None Returns: Name Type Description AioGetObjectResult AioGetObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 async def get_object_to_file ( self , key : str , filename : str , byte_range : Optional [ Sequence [ int ]] = None , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , progress_callback : Optional [ Callable ] = None , process : Optional [ Callable ] = None , params : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> AioGetObjectResult : \"\"\"Download contents of object to file. (use case) :: >>> result = await aiobucket.get_object_to_file(\"object1\", \"file\") >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" Args: key (str): object name to download. filename (str): filename to save the data downloaded. byte_range (Optional[Sequence[Optional[int]]], optional): Range to download. headers (Optional[dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. process (_type_, optional): oss file process method. params (Optional[Union[Dict, CaseInsensitiveDict]], optional): Returns: AioGetObjectResult: \"\"\" with open ( to_unicode ( filename ), \"wb\" ) as f_w : result = await self . get_object ( key , byte_range = byte_range , headers = headers , progress_callback = progress_callback , process = process , params = params , ) if result . content_length is None : copyfileobj ( result , f_w ) else : await copyfileobj_and_verify ( result , f_w , result . content_length , request_id = result . request_id , ) if self . enable_crc and byte_range is None : if ( ( headers is None ) or ( \"Accept-Encoding\" not in headers ) or ( headers [ \"Accept-Encoding\" ] != \"gzip\" ) ): check_crc ( \"get\" , result . client_crc , result . server_crc , result . request_id , ) return result","title":"get_object_to_file()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.head_object","text":"Get object metadata >>> result = bucket.head_object('readme.txt') >>> print(result.content_type) text/plain Parameters: Name Type Description Default key str object key required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None params Optional [ Mapping ] None Returns: Name Type Description HeadObjectResult HeadObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 async def head_object ( self , key : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Mapping ] = None , ) -> \"HeadObjectResult\" : \"\"\"Get object metadata >>> result = bucket.head_object('readme.txt') >>> print(result.content_type) text/plain Args: key (str): object key headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Mapping], optional): Returns: HeadObjectResult: Raises: `NotFound <oss2.exceptions.NotFound>` if object does not exist. \"\"\" logger . debug ( \"Start to head object, bucket: %s , key: %s , headers: %s \" , self . bucket_name , to_string ( key ), headers , ) resp = await self . _do_object ( \"HEAD\" , key , headers = headers , params = params ) logger . debug ( \"Head object done, req_id: %s , status_code: %s \" , resp . request_id , resp . status , ) return await self . _parse_result ( resp , parse_dummy_result , HeadObjectResult )","title":"head_object()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.init_multipart_upload","text":"initialize multipart uploading The returning upload id, bucket name and object name together defines this uploading event. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 811 812 813 814 815 816 817 async def init_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> InitMultipartUploadResult : \"\"\"initialize multipart uploading The returning upload id, bucket name and object name together defines this uploading event.\"\"\" return await init_multipart_upload ( self , * args , ** kwargs )","title":"init_multipart_upload()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.list_multipart_uploads","text":"List multipart uploading process Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 819 820 821 822 823 async def list_multipart_uploads ( self : \"AioBucket\" , * args , ** kwargs ) -> ListMultipartUploadsResult : \"\"\"List multipart uploading process\"\"\" return await list_multipart_uploads ( self , * args , ** kwargs )","title":"list_multipart_uploads()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.list_objects","text":"list objects in a bucket (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Parameters: Name Type Description Default prefix str only list objects start with this prefix. '' delimiter str delimiter as a folder separator. '' marker str use in paginate. '' max_keys int numbers of objects for one page. 100 headers Optional [ Dict ] HTTP headers to specify. None Returns: Name Type Description ListObjectsResult ListObjectsResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 async def list_objects ( # pylint: disable=too-many-arguments self , prefix : str = \"\" , delimiter : str = \"\" , marker : str = \"\" , max_keys : int = 100 , headers : Optional [ Dict ] = None , ) -> \"ListObjectsResult\" : \"\"\"list objects in a bucket (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Args: prefix (str, optional): only list objects start with this prefix. delimiter (str, optional): delimiter as a folder separator. marker (str, optional): use in paginate. max_keys (int, optional): numbers of objects for one page. headers (Optional[Dict], optional): HTTP headers to specify. Returns: ListObjectsResult: \"\"\" headers = CaseInsensitiveDict ( headers ) resp = await self . _do_object ( \"GET\" , \"\" , params = { \"prefix\" : prefix , \"delimiter\" : delimiter , \"marker\" : marker , \"max-keys\" : str ( max_keys ), \"encoding-type\" : \"url\" , }, headers = headers , ) logger . debug ( \"List objects done\" ) return await self . _parse_result ( resp , parse_list_objects , ListObjectsResult )","title":"list_objects()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.list_parts","text":"list uploaded parts in a part uploading progress. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 825 826 827 async def list_parts ( self : \"AioBucket\" , * args , ** kwargs ) -> ListPartsResult : \"\"\"list uploaded parts in a part uploading progress.\"\"\" return await list_parts ( self , * args , ** kwargs )","title":"list_parts()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.object_exists","text":"Return True if key exists, False otherwise. raise Exception for other exceptions. (use case) :: >>> result = await bucket.object_exists('object1') >>> print(result) True Parameters: Name Type Description Default key str key of the object required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] None Returns: Name Type Description bool bool Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 async def object_exists ( self , key : str , headers : Optional [ Dict ] = None ) -> bool : \"\"\"Return True if key exists, False otherwise. raise Exception for other exceptions. (use case) :: >>> result = await bucket.object_exists('object1') >>> print(result) True Args: key (str): key of the object headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. Returns: bool: \"\"\" try : await self . get_object_meta ( key , headers = headers ) except NoSuchKey : return False return True","title":"object_exists()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.put_object","text":"upload some contents to an object (use case) :: >>> await bucket.put_object('readme.txt', 'content of readme.txt') >>> with open(u'local_file.txt', 'rb') as f: >>> await bucket.put_object('remote_file.txt', f) Parameters: Name Type Description Default key str object name to upload required data Union [ str , bytes , IO , Iterable ] contents to upload required headers Optional [ Mapping ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None Returns: Name Type Description PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 async def put_object ( self , key : str , data , headers : Optional [ Mapping ] = None , progress_callback : Optional [ Callable ] = None , ) -> \"PutObjectResult\" : \"\"\"upload some contents to an object (use case) :: >>> await bucket.put_object('readme.txt', 'content of readme.txt') >>> with open(u'local_file.txt', 'rb') as f: >>> await bucket.put_object('remote_file.txt', f) Args: key (str): object name to upload data (Union[str, bytes, IO, Iterable]): contents to upload headers (Optional[Mapping], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. Returns: PutObjectResult: \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), key ) data = make_adapter ( data , progress_callback = progress_callback , enable_crc = self . enable_crc , ) resp : \"AioResponse\" = await self . _do_object ( \"PUT\" , key , data = data , headers = headers ) logger . debug ( \"Put object done\" ) result = PutObjectResult ( resp ) if self . enable_crc and result . crc is not None : check_crc ( \"put object\" , data . crc , result . crc , result . request_id ) return result","title":"put_object()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.put_object_from_file","text":"Upload a local file to a oss key (use case) :: >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" >>> result = await aiobucket.put_object_from_file(\"object1\", \"file\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' Parameters: Name Type Description Default key str key of the oss required filename str filename to upload required headers Optional [ Mapping ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None Returns: Name Type Description _type_ PutObjectResult description Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 async def put_object_from_file ( self , key : str , filename : str , headers : Optional [ Mapping ] = None , progress_callback : Optional [ Callable ] = None , ) -> \"PutObjectResult\" : \"\"\"Upload a local file to a oss key (use case) :: >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" >>> result = await aiobucket.put_object_from_file(\"object1\", \"file\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' Args: key (str): key of the oss filename (str): filename to upload headers (Optional[Mapping], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. Returns: _type_: _description_ \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), filename ) with open ( to_unicode ( filename ), \"rb\" ) as f_stream : return await self . put_object ( key , f_stream , headers = headers , progress_callback = progress_callback , )","title":"put_object_from_file()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.upload_part","text":"upload single part. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 829 830 831 async def upload_part ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"upload single part.\"\"\" return await upload_part ( self , * args , ** kwargs )","title":"upload_part()"},{"location":"reference/aiooss2/#aiooss2.api.AioBucket.upload_part_copy","text":"copy part or whole of a source file to a slice of a target file. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 833 834 835 async def upload_part_copy ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"copy part or whole of a source file to a slice of a target file.\"\"\" return await upload_part_copy ( self , * args , ** kwargs )","title":"upload_part_copy()"},{"location":"reference/aiooss2/#aiooss2.AioBucketIterator","text":"Bases: _AioBaseIterator Iterate over buckets of an user Return SimplifiedBucketInfo <oss2.models.SimplifiedBucketInfo> every iteration Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 class AioBucketIterator ( _AioBaseIterator ): \"\"\"Iterate over buckets of an user Return `SimplifiedBucketInfo <oss2.models.SimplifiedBucketInfo>` every iteration \"\"\" def __init__ ( self , service : \"AioService\" , prefix : str = \"\" , marker : str = \"\" , max_keys : int = 100 , max_retries : Optional [ int ] = None , ): \"\"\" Args: service (AioService): Service class of a special user. prefix (str, optional): prefix to filter the buckets results. marker (str, optional): paginate separator. max_keys (int, optional): max return number per page. max_retries (Optional[int], optional): max retry count. \"\"\" super () . __init__ ( marker , max_retries ) self . service = service self . prefix = prefix self . max_keys = max_keys async def _fetch ( self ): result = await self . service . list_buckets ( prefix = self . prefix , marker = self . next_marker , max_keys = self . max_keys ) self . entries : List [ \"SimplifiedBucketInfo\" ] = result . buckets return result . is_truncated , result . next_marker","title":"AioBucketIterator"},{"location":"reference/aiooss2/#aiooss2.iterators.AioBucketIterator.__init__","text":"Parameters: Name Type Description Default service AioService Service class of a special user. required prefix str prefix to filter the buckets results. '' marker str paginate separator. '' max_keys int max return number per page. 100 max_retries Optional [ int ] max retry count. None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def __init__ ( self , service : \"AioService\" , prefix : str = \"\" , marker : str = \"\" , max_keys : int = 100 , max_retries : Optional [ int ] = None , ): \"\"\" Args: service (AioService): Service class of a special user. prefix (str, optional): prefix to filter the buckets results. marker (str, optional): paginate separator. max_keys (int, optional): max return number per page. max_retries (Optional[int], optional): max retry count. \"\"\" super () . __init__ ( marker , max_retries ) self . service = service self . prefix = prefix self . max_keys = max_keys","title":"__init__()"},{"location":"reference/aiooss2/#aiooss2.AioObjectIterator","text":"Bases: _AioBaseIterator Iterator to iterate objects from a bucket. Return SimplifiedObjectInfo <oss2.models.SimplifiedObjectInfo> object. if SimplifiedObjectInfo.is_prefix() is true, the object returned is a directory. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class AioObjectIterator ( _AioBaseIterator ): \"\"\"Iterator to iterate objects from a bucket. Return `SimplifiedObjectInfo <oss2.models.SimplifiedObjectInfo>` object. if `SimplifiedObjectInfo.is_prefix()` is true, the object returned is a directory. \"\"\" def __init__ ( self , bucket : \"AioBucket\" , prefix : str = \"\" , delimiter : str = \"\" , marker : str = \"\" , max_keys : int = 100 , max_retries : Optional [ int ] = None , headers : Optional [ Dict ] = None , ): \"\"\"_summary_ Args: bucket (AioBucket): bucket class to iterate prefix (str, optional): prefix to filter the object results delimiter (str, optional): delimiter in object name marker (str, optional): paginate separator max_keys (int, optional): key number returns from `list_objects` every time,to notice that iterator can return more objects than it max_retries (Optional[int], optional): retry number headers (Optional[Dict], optional): HTTP header \"\"\" super () . __init__ ( marker , max_retries ) self . bucket = bucket self . prefix = prefix self . delimiter = delimiter self . max_keys = max_keys self . headers = http . CaseInsensitiveDict ( headers ) async def _fetch ( self ): result : ListObjectsResult = await self . bucket . list_objects ( prefix = self . prefix , delimiter = self . delimiter , marker = self . next_marker , max_keys = self . max_keys , headers = self . headers , ) self . entries : List [ \"SimplifiedObjectInfo\" ] = result . object_list + [ SimplifiedObjectInfo ( prefix , None , None , None , None , None ) for prefix in result . prefix_list ] self . entries . sort ( key = lambda obj : obj . key ) return result . is_truncated , result . next_marker","title":"AioObjectIterator"},{"location":"reference/aiooss2/#aiooss2.iterators.AioObjectIterator.__init__","text":"summary Parameters: Name Type Description Default bucket AioBucket bucket class to iterate required prefix str prefix to filter the object results '' delimiter str delimiter in object name '' marker str paginate separator '' max_keys int key number returns from list_objects 100 max_retries Optional [ int ] retry number None headers Optional [ Dict ] HTTP header None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def __init__ ( self , bucket : \"AioBucket\" , prefix : str = \"\" , delimiter : str = \"\" , marker : str = \"\" , max_keys : int = 100 , max_retries : Optional [ int ] = None , headers : Optional [ Dict ] = None , ): \"\"\"_summary_ Args: bucket (AioBucket): bucket class to iterate prefix (str, optional): prefix to filter the object results delimiter (str, optional): delimiter in object name marker (str, optional): paginate separator max_keys (int, optional): key number returns from `list_objects` every time,to notice that iterator can return more objects than it max_retries (Optional[int], optional): retry number headers (Optional[Dict], optional): HTTP header \"\"\" super () . __init__ ( marker , max_retries ) self . bucket = bucket self . prefix = prefix self . delimiter = delimiter self . max_keys = max_keys self . headers = http . CaseInsensitiveDict ( headers )","title":"__init__()"},{"location":"reference/aiooss2/#aiooss2.AioService","text":"Bases: _AioBase Service class used for operations like list all bucket Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 class AioService ( _AioBase ): \"\"\"Service class used for operations like list all bucket\"\"\" def __init__ ( self , auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ], endpoint : str , session : Optional [ AioSession ] = None , connect_timeout : Optional [ int ] = None , app_name : str = \"\" , proxies = None , ): \"\"\"_summary_ Args: auth (Union[Auth, AnonymousAuth, StsAuth]): Auth class. endpoint (str): endpoint address or CNAME. session (Optional[AioSession], optional): reuse a custom session. connect_timeout (int): connection. app_name (str, optional): app name. proxies (_type_, optional): proxies settings. \"\"\" super () . __init__ ( auth , endpoint , False , session , connect_timeout , app_name = app_name , proxies = proxies , ) async def list_buckets ( self , prefix : str = \"\" , marker : str = \"\" , max_keys : int = 100 , params : Optional [ Dict ] = None , ) -> ListBucketsResult : \"\"\"List buckets with given prefix of an user (use case) :: >>> async for obj in AioBucketIterator(aioservice): >>> print(obj.name) 'bucket_name1' 'bucket_name2' ... Args: prefix (str, optional): prefix to filter the buckets results. marker (str, optional): paginate separator. max_keys (int, optional): max return number per page. params (Optional[Dict], optional): Some optional params. Returns: oss2.models.ListBucketsResult: \"\"\" list_param = {} list_param [ \"prefix\" ] = prefix list_param [ \"marker\" ] = marker list_param [ \"max-keys\" ] = str ( max_keys ) if params is not None : if \"tag-key\" in params : list_param [ \"tag-key\" ] = params [ \"tag-key\" ] if \"tag-value\" in params : list_param [ \"tag-value\" ] = params [ \"tag-value\" ] resp = await self . _do ( \"GET\" , \"\" , \"\" , params = list_param ) logger . debug ( \"List buckets done\" ) return await self . _parse_result ( resp , parse_list_buckets , ListBucketsResult )","title":"AioService"},{"location":"reference/aiooss2/#aiooss2.api.AioService.__init__","text":"summary Parameters: Name Type Description Default auth Union [ Auth , AnonymousAuth , StsAuth ] Auth class. required endpoint str endpoint address or CNAME. required session Optional [ AioSession ] reuse a custom session. None connect_timeout int connection. None app_name str app name. '' proxies _type_ proxies settings. None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 def __init__ ( self , auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ], endpoint : str , session : Optional [ AioSession ] = None , connect_timeout : Optional [ int ] = None , app_name : str = \"\" , proxies = None , ): \"\"\"_summary_ Args: auth (Union[Auth, AnonymousAuth, StsAuth]): Auth class. endpoint (str): endpoint address or CNAME. session (Optional[AioSession], optional): reuse a custom session. connect_timeout (int): connection. app_name (str, optional): app name. proxies (_type_, optional): proxies settings. \"\"\" super () . __init__ ( auth , endpoint , False , session , connect_timeout , app_name = app_name , proxies = proxies , )","title":"__init__()"},{"location":"reference/aiooss2/#aiooss2.api.AioService.list_buckets","text":"List buckets with given prefix of an user (use case) :: >>> async for obj in AioBucketIterator(aioservice): >>> print(obj.name) 'bucket_name1' 'bucket_name2' ... Parameters: Name Type Description Default prefix str prefix to filter the buckets results. '' marker str paginate separator. '' max_keys int max return number per page. 100 params Optional [ Dict ] Some optional params. None Returns: Type Description ListBucketsResult oss2.models.ListBucketsResult: Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 async def list_buckets ( self , prefix : str = \"\" , marker : str = \"\" , max_keys : int = 100 , params : Optional [ Dict ] = None , ) -> ListBucketsResult : \"\"\"List buckets with given prefix of an user (use case) :: >>> async for obj in AioBucketIterator(aioservice): >>> print(obj.name) 'bucket_name1' 'bucket_name2' ... Args: prefix (str, optional): prefix to filter the buckets results. marker (str, optional): paginate separator. max_keys (int, optional): max return number per page. params (Optional[Dict], optional): Some optional params. Returns: oss2.models.ListBucketsResult: \"\"\" list_param = {} list_param [ \"prefix\" ] = prefix list_param [ \"marker\" ] = marker list_param [ \"max-keys\" ] = str ( max_keys ) if params is not None : if \"tag-key\" in params : list_param [ \"tag-key\" ] = params [ \"tag-key\" ] if \"tag-value\" in params : list_param [ \"tag-value\" ] = params [ \"tag-value\" ] resp = await self . _do ( \"GET\" , \"\" , \"\" , params = list_param ) logger . debug ( \"List buckets done\" ) return await self . _parse_result ( resp , parse_list_buckets , ListBucketsResult )","title":"list_buckets()"},{"location":"reference/aiooss2/#aiooss2.AioSession","text":"Async session wrapper Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 class AioSession : \"\"\"Async session wrapper\"\"\" def __init__ ( self , psize : Optional [ int ] = None ): \"\"\" Args: psize: limit amount of simultaneously opened connections \"\"\" self . psize = psize or defaults . connection_pool_size self . session : Optional [ ClientSession ] = None self . conn : Optional [ TCPConnector ] = None async def do_request ( self , req : \"Request\" , timeout : Optional [ int ] = None ) -> \"AioResponse\" : \"\"\"Do request Args: req: request info timeout: timeout in seconds Raises: RequestError: Returns: AioResponse: Async Response wrapper \"\"\" logger . debug ( \"Send request, method: %s , url: %s , params: %s , headers: %s , \" \"timeout: %d \" , req . method , req . url , req . params , req . headers , timeout , ) try : assert self . session resp = await self . session . request ( req . method , req . url , data = req . data , params = req . params , headers = req . headers , timeout = timeout , ) return AioResponse ( resp ) except ClientResponseError as err : raise RequestError ( err ) from err async def __aenter__ ( self ): self . conn = TCPConnector ( limit = self . psize , limit_per_host = self . psize ) self . session = ClientSession ( connector = self . conn ) return self async def __aexit__ ( self , * args ): await self . close () async def close ( self ): \"\"\"gracefully close the AioSession class\"\"\" await self . conn . close () await self . session . close () @property def closed ( self ): \"\"\"Is client session closed. A readonly property. \"\"\" if self . session is None : return True return self . session . closed","title":"AioSession"},{"location":"reference/aiooss2/#aiooss2.http.AioSession.closed","text":"Is client session closed. A readonly property.","title":"closed"},{"location":"reference/aiooss2/#aiooss2.http.AioSession.__init__","text":"Parameters: Name Type Description Default psize Optional [ int ] limit amount of simultaneously opened connections None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 112 113 114 115 116 117 118 119 120 def __init__ ( self , psize : Optional [ int ] = None ): \"\"\" Args: psize: limit amount of simultaneously opened connections \"\"\" self . psize = psize or defaults . connection_pool_size self . session : Optional [ ClientSession ] = None self . conn : Optional [ TCPConnector ] = None","title":"__init__()"},{"location":"reference/aiooss2/#aiooss2.http.AioSession.close","text":"gracefully close the AioSession class Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 170 171 172 173 async def close ( self ): \"\"\"gracefully close the AioSession class\"\"\" await self . conn . close () await self . session . close ()","title":"close()"},{"location":"reference/aiooss2/#aiooss2.http.AioSession.do_request","text":"Do request Parameters: Name Type Description Default req Request request info required timeout Optional [ int ] timeout in seconds None Raises: Type Description RequestError Returns: Name Type Description AioResponse AioResponse Async Response wrapper Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 async def do_request ( self , req : \"Request\" , timeout : Optional [ int ] = None ) -> \"AioResponse\" : \"\"\"Do request Args: req: request info timeout: timeout in seconds Raises: RequestError: Returns: AioResponse: Async Response wrapper \"\"\" logger . debug ( \"Send request, method: %s , url: %s , params: %s , headers: %s , \" \"timeout: %d \" , req . method , req . url , req . params , req . headers , timeout , ) try : assert self . session resp = await self . session . request ( req . method , req . url , data = req . data , params = req . params , headers = req . headers , timeout = timeout , ) return AioResponse ( resp ) except ClientResponseError as err : raise RequestError ( err ) from err","title":"do_request()"},{"location":"reference/aiooss2/#aiooss2.resumable_download","text":"Resumable download object to local file, implementation method is to create a list temporary files whose name is formed by the original filename with some random surfix. If the downloading was interrupted by some reasons, only those remaied parts need to be downloaded.","title":"resumable_download()"},{"location":"reference/aiooss2/#aiooss2.resumable_download--using-cryptobucket-will-make-the-download-fallback-to-the-normal-one","text":"","title":"Using CryptoBucket will make the download fallback to the normal one."},{"location":"reference/aiooss2/#aiooss2.resumable_download--avoid-using-multi-threadingprocessing-as-the-temp-downloaded-file-might","text":"","title":"Avoid using multi-threading/processing as the temp downloaded file might"},{"location":"reference/aiooss2/#aiooss2.resumable_download--be-covered","text":"Parameters: Name Type Description Default bucket Union [ AioBucket , CryptoBucket ] bucket object to download required key Union [ str , bytes ] object key to store the file required filename Union [ str , bytes ] filename to download required store Optional [ ResumableStore ] ResumableStore object to keep the downloading info in the previous operation. None headers Optional [ Mapping ] HTTP headers to send. Defaults to None.","title":"be covered."},{"location":"reference/aiooss2/#aiooss2.resumable_download--download_part-only-accept-oss_request_payer","text":"","title":"download_part only accept OSS_REQUEST_PAYER"},{"location":"reference/aiooss2/#aiooss2.resumable_download--get_object-and-get_object_to_file-only-accept-oss_request_payer","text":"OSS_TRAFFIC_LIMIT None multipget_threshold Optional [ int ] threshold to use multipart download instead of a normal one. required part_size Optional [ int ] partition size of the multipart. None progress_callback Optional [ Callable ] callback function for progress bar. None num_threads Optional [ int ] concurrency number during the uploadinging None params Optional [ Mapping ] None Return None: Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/resumable.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 async def resumable_download ( # pylint: disable=too-many-arguments bucket : Union [ \"AioBucket\" , \"CryptoBucket\" ], key : Union [ str , bytes ], filename : Union [ str , bytes ], store : Optional [ \"ResumableStore\" ] = None , headers : Optional [ Mapping ] = None , part_size : Optional [ int ] = None , progress_callback : Optional [ Callable ] = None , num_threads : Optional [ int ] = None , params : Optional [ Mapping ] = None , multiget_threshold : Optional [ int ] = None , ) -> None : \"\"\"Resumable download object to local file, implementation method is to create a list temporary files whose name is formed by the original filename with some random surfix. If the downloading was interrupted by some reasons, only those remaied parts need to be downloaded. # Using `CryptoBucket` will make the download fallback to the normal one. # Avoid using multi-threading/processing as the temp downloaded file might # be covered. Args: bucket (Union[AioBucket, CryptoBucket]): bucket object to download key (Union[str, bytes]): object key to store the file filename (Union[str, bytes]): filename to download store (Optional[\"ResumableStore\"]): ResumableStore object to keep the downloading info in the previous operation. headers (Optional[Mapping]): HTTP headers to send. Defaults to None. # download_part only accept OSS_REQUEST_PAYER # get_object and get_object_to_file only accept OSS_REQUEST_PAYER, OSS_TRAFFIC_LIMIT multipget_threshold (Optional[int]): threshold to use multipart download instead of a normal one. part_size (Optional[int]): partition size of the multipart. progress_callback (Optional[Callable]): callback function for progress bar. num_threads (Optional[int]): concurrency number during the uploadinging params (Optional[Mapping]): Return: None: \"\"\" key_str : str = to_string ( key ) filename_str : str = to_unicode ( filename ) logger . debug ( \"Start to resumable download, bucket: %s , key: %s , filename: %s , \" \"multiget_threshold: %s , part_size: %s , num_threads: %s \" , bucket . bucket_name , key_str , filename_str , multiget_threshold , part_size , num_threads , ) multiget_threshold = multiget_threshold or MULTIGET_THRESHOLD valid_headers = _populate_valid_headers ( headers , [ OSS_REQUEST_PAYER , OSS_TRAFFIC_LIMIT ] ) result = await bucket . head_object ( key_str , params = params , headers = valid_headers ) logger . debug ( \"The size of object to download is: %s \" , result . content_length ) if result . content_length >= multiget_threshold : downloader = ResumableDownloader ( bucket , key , filename , _ObjectInfo . make ( result ), part_size = part_size , progress_callback = progress_callback , num_threads = num_threads , store = store , params = params , headers = valid_headers , ) await downloader . download ( result . server_crc ) else : await bucket . get_object_to_file ( key_str , filename_str , progress_callback = progress_callback , params = params , headers = valid_headers , )","title":"get_object and get_object_to_file only accept OSS_REQUEST_PAYER,"},{"location":"reference/aiooss2/#aiooss2.resumable_upload","text":"Resumable upload local file , The implementation is splitting local files to multipart, storing uploading information in local files. If the uploading was interrupted by some reasons, only those remaied parts need to be uploaded.","title":"resumable_upload()"},{"location":"reference/aiooss2/#aiooss2.resumable_upload--using-cryptobucket-will-make-the-upload-fallback-to-the-normal-one","text":"Parameters: Name Type Description Default bucket Union [ AioBucket , CryptoBucket ] bucket object to upload required key Union [ str , bytes ] object key to store the file required filename Union [ str , bytes ] filename to upload required store Optional [ ResumableStore ] ResumableStore object to keep the uploading info in the previous operation. Defaults to None. None headers Optional [ Mapping ] HTTP headers to send. Defaults to None.","title":"Using CryptoBucket will make the upload fallback to the normal one."},{"location":"reference/aiooss2/#aiooss2.resumable_upload--put_object-or-init_multipart_upload-can-make-use-of-the-whole","text":"headers","title":"put_object or init_multipart_upload can make use of the whole"},{"location":"reference/aiooss2/#aiooss2.resumable_upload--uplpad_part-only-accept-oss_request_payer-oss_traffic_limit","text":"","title":"uplpad_part only accept OSS_REQUEST_PAYER, OSS_TRAFFIC_LIMIT"},{"location":"reference/aiooss2/#aiooss2.resumable_upload--complete_multipart_upload-only-accept-oss_request_payer","text":"OSS_OBJECT_ACL None multipart_threshold Optional [ int ] threshold to use multipart upload instead of a normal one. Defaults to None. None part_size Optional [ int ] partition size of the multipart. Defaults to None. None progress_callback Optional [ Callable ] callback function for progress bar. Defaults to None. None num_threads Optional [ int ] concurrency number during the uploading Defaults to None. None params Optional [ Mapping ] Defaults to None. None Returns: Name Type Description PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/resumable.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 async def resumable_upload ( # pylint: disable=too-many-arguments bucket : Union [ \"AioBucket\" , \"CryptoBucket\" ], key : Union [ str , bytes ], filename : Union [ str , bytes ], store : Optional [ \"ResumableStore\" ] = None , headers : Optional [ Mapping ] = None , multipart_threshold : Optional [ int ] = None , part_size : Optional [ int ] = None , progress_callback : Optional [ Callable ] = None , num_threads : Optional [ int ] = None , params : Optional [ Mapping ] = None , ) -> \"PutObjectResult\" : \"\"\"Resumable upload local file , The implementation is splitting local files to multipart, storing uploading information in local files. If the uploading was interrupted by some reasons, only those remaied parts need to be uploaded. # Using `CryptoBucket` will make the upload fallback to the normal one. Args: bucket (Union[AioBucket, CryptoBucket]): bucket object to upload key (Union[str, bytes]): object key to store the file filename (Union[str, bytes]): filename to upload store (Optional[\"ResumableStore\"]): ResumableStore object to keep the uploading info in the previous operation. Defaults to None. headers (Optional[Mapping]): HTTP headers to send. Defaults to None. # put_object or init_multipart_upload can make use of the whole headers # uplpad_part only accept OSS_REQUEST_PAYER, OSS_TRAFFIC_LIMIT # complete_multipart_upload only accept OSS_REQUEST_PAYER, OSS_OBJECT_ACL multipart_threshold (Optional[int]): threshold to use multipart upload instead of a normal one. Defaults to None. part_size (Optional[int]): partition size of the multipart. Defaults to None. progress_callback (Optional[Callable]): callback function for progress bar. Defaults to None. num_threads (Optional[int]): concurrency number during the uploading Defaults to None. params (Optional[Mapping]): Defaults to None. Returns: PutObjectResult: \"\"\" key_str = to_string ( key ) filename_str = to_unicode ( filename ) size = os . path . getsize ( filename_str ) logger . debug ( \"Start to resumable upload, bucket: %s , key: %s , filename: %s , \" \"headers: %s , multipart_threshold: %s , part_size: %s , \" \"num_threads: %s , size of file to upload is %s \" , bucket . bucket_name , key_str , filename_str , headers , multipart_threshold , part_size , num_threads , size , ) multipart_threshold = multipart_threshold or MULTIPART_THRESHOLD num_threads = num_threads or MULTIPART_NUM_THREADS part_size = part_size or PART_SIZE if size >= multipart_threshold and not isinstance ( bucket , CryptoBucket ): store = store or ResumableStore () uploader = ResumableUploader ( bucket , key_str , filename_str , size , store , part_size = part_size , headers = headers , progress_callback = progress_callback , num_threads = num_threads , params = params , ) result = await uploader . upload () else : result = await bucket . put_object_from_file ( key_str , filename_str , headers = headers , progress_callback = progress_callback , ) return result","title":"complete_multipart_upload only accept OSS_REQUEST_PAYER,"},{"location":"reference/aiooss2/adapter/","text":"Adapter (crc check and progress bar call backs) for data types AsyncPayload Bases: Payload Payload of async data of unknown length Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/adapter.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 class AsyncPayload ( Payload ): \"\"\"Payload of async data of unknown length\"\"\" _value : StreamAdapter async def write ( self , writer : AbstractStreamWriter ) -> None : chunk = await self . _value . read () while chunk : if len ( chunk ) > TOO_LARGE_BYTES_BODY : logger . warning ( \"Sending a large body directly with raw bytes might\" \" lock the event loop. You should probably pass an \" \"io.BytesIO object instead.\" , ) await writer . write ( chunk ) chunk = await self . _value . read () FilelikeObjectAdapter Bases: StreamAdapter Adapter for file like data objects Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/adapter.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 class FilelikeObjectAdapter ( StreamAdapter ): \"\"\"Adapter for file like data objects\"\"\" def __init__ ( self , stream , ** kwargs ): super () . __init__ ( stream , ** kwargs ) self . _read_all = False if self . size is None : try : position = self . stream . tell () end = self . stream . seek ( 0 , os . SEEK_END ) self . stream . seek ( position ) self . size = end - position except AttributeError : pass def _length_to_read ( self , amt : int ) -> int : if self . size is None : return amt length_to_read = self . size - self . offset if amt > 0 : length_to_read = min ( amt , length_to_read ) return length_to_read async def read ( self , amt : int = - 1 ) -> bytes : if self . _read_all or ( self . size and self . offset >= self . size ): return b \"\" if self . offset < self . discard and amt : amt += self . discard - self . offset length_to_read = self . _length_to_read ( amt ) if inspect . iscoroutinefunction ( self . stream . read ): content = await self . stream . read ( length_to_read ) else : content = self . stream . read ( length_to_read ) if not content : self . _read_all = True return self . _invoke_callbacks ( content ) IterableAdapter Bases: StreamAdapter Adapter for Async Iterable data Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/adapter.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 class IterableAdapter ( StreamAdapter ): \"\"\"Adapter for Async Iterable data\"\"\" def __init__ ( self , stream , discard : int = 0 , ** kwargs ): if hasattr ( stream , \"__aiter__\" ): stream = stream . __aiter__ () elif hasattr ( stream , \"__iter__\" ): stream = iter ( stream ) super () . __init__ ( stream , ** kwargs ) if discard : raise ValueError ( \"discard not supported in Async \" f \"Iterable input { self . stream } \" ) async def read ( self , amt : int = - 1 ) -> bytes : try : if hasattr ( self . stream , \"__anext__\" ): content = await self . stream . __anext__ () elif hasattr ( self . stream , \"__next__\" ): content = next ( self . stream ) else : raise AttributeError ( f \" { self . stream . __class__ . __name__ } is neither\" \" an iterator nor an async iterator\" ) except ( StopIteration , StopAsyncIteration ): return b \"\" return self . _invoke_callbacks ( content ) SliceableAdapter Bases: StreamAdapter Adapter for data can get a slice via stream[a:b] Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/adapter.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class SliceableAdapter ( StreamAdapter ): \"\"\"Adapter for data can get a slice via `stream[a:b]`\"\"\" def __init__ ( self , stream : Union [ bytes , bytearray , memoryview ], ** kwargs , ): \"\"\" Args: size (Optional[int]): size of the data stream. \"\"\" super () . __init__ ( stream , ** kwargs ) self . size : int = self . size or len ( stream ) def _length_to_read ( self , amt : int ) -> int : length_to_read = self . size - self . offset if amt > 0 : length_to_read = min ( amt , length_to_read ) return length_to_read async def read ( self , amt : int = - 1 ) -> bytes : if self . offset >= self . size : return b \"\" length_to_read = self . _length_to_read ( amt ) content = self . stream [ self . offset : self . offset + length_to_read ] return self . _invoke_callbacks ( content ) __init__ ( stream , kwargs ) Parameters: Name Type Description Default size Optional [ int ] size of the data stream. required Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/adapter.py 105 106 107 108 109 110 111 112 113 114 115 116 def __init__ ( self , stream : Union [ bytes , bytearray , memoryview ], ** kwargs , ): \"\"\" Args: size (Optional[int]): size of the data stream. \"\"\" super () . __init__ ( stream , ** kwargs ) self . size : int = self . size or len ( stream ) StreamAdapter Bases: ABC Adapter for data types Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/adapter.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class StreamAdapter ( ABC ): \"\"\"Adapter for data types\"\"\" def __init__ ( # pylint: disable=too-many-arguments self , stream : Union [ bytes , IO ], progress_callback : Optional [ Callable ] = None , crc_callback : Optional [ Callable ] = None , cipher_callback : Optional [ Callable ] = None , size : Optional [ int ] = None , discard : int = 0 , ): \"\"\" Args: stream (Union[bytes, IO]): original data stream can be bytes or file like object. progress_callback (Optional[Callable], optional): function used for progress bar. crc_callback (Optional[Callable], optional): function used for crc calculation. cipher_callback (Optional[Callable], optional): function used for cipher calculation. size: Optional[int] = None, discard (int): bytes to discard. \"\"\" self . stream = to_bytes ( stream ) self . progress_callback : Optional [ Callable ] = progress_callback self . crc_callback : Optional [ Callable ] = crc_callback self . cipher_callback : Optional [ Callable ] = cipher_callback self . offset = 0 self . size = size self . discard = discard def __aiter__ ( self ) -> AsyncIterator : return self async def __anext__ ( self ) -> bytes : content = await self . read ( _CHUNK_SIZE ) if content : return content raise StopAsyncIteration def __len__ ( self ) -> Optional [ int ]: return self . size @abstractmethod async def read ( self , amt =- 1 ) -> bytes : \"\"\"async api to read a chunk from the data Args: amt (int): batch size of the data to read, -1 to read all \"\"\" @property def crc ( self ): \"\"\"return crc value of the data\"\"\" if self . crc_callback : return self . crc_callback . crc return None def _invoke_callbacks ( self , content : Union [ str , bytes ]): content = to_bytes ( content ) self . offset += len ( content ) real_discard = 0 if self . offset < self . discard : real_discard = ( len ( content ) if len ( content ) <= self . discard else self . discard ) self . discard -= real_discard _invoke_progress_callback ( self . progress_callback , self . offset , self . size ) _invoke_crc_callback ( self . crc_callback , content , real_discard ) content = _invoke_cipher_callback ( self . cipher_callback , content , real_discard ) return content crc property return crc value of the data __init__ ( stream , progress_callback = None , crc_callback = None , cipher_callback = None , size = None , discard = 0 ) Parameters: Name Type Description Default stream Union [ bytes , IO ] original data stream can be bytes or file like object. required progress_callback Optional [ Callable ] function used for progress bar. None crc_callback Optional [ Callable ] function used for crc calculation. None cipher_callback Optional [ Callable ] function used for cipher calculation. None size Optional [ int ] Optional[int] = None, None discard int bytes to discard. 0 Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/adapter.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( # pylint: disable=too-many-arguments self , stream : Union [ bytes , IO ], progress_callback : Optional [ Callable ] = None , crc_callback : Optional [ Callable ] = None , cipher_callback : Optional [ Callable ] = None , size : Optional [ int ] = None , discard : int = 0 , ): \"\"\" Args: stream (Union[bytes, IO]): original data stream can be bytes or file like object. progress_callback (Optional[Callable], optional): function used for progress bar. crc_callback (Optional[Callable], optional): function used for crc calculation. cipher_callback (Optional[Callable], optional): function used for cipher calculation. size: Optional[int] = None, discard (int): bytes to discard. \"\"\" self . stream = to_bytes ( stream ) self . progress_callback : Optional [ Callable ] = progress_callback self . crc_callback : Optional [ Callable ] = crc_callback self . cipher_callback : Optional [ Callable ] = cipher_callback self . offset = 0 self . size = size self . discard = discard read ( amt =- 1 ) abstractmethod async async api to read a chunk from the data Parameters: Name Type Description Default amt int batch size of the data to read, -1 to read all -1 Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/adapter.py 69 70 71 72 73 74 75 @abstractmethod async def read ( self , amt =- 1 ) -> bytes : \"\"\"async api to read a chunk from the data Args: amt (int): batch size of the data to read, -1 to read all \"\"\"","title":"Adapter"},{"location":"reference/aiooss2/adapter/#aiooss2.adapter.AsyncPayload","text":"Bases: Payload Payload of async data of unknown length Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/adapter.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 class AsyncPayload ( Payload ): \"\"\"Payload of async data of unknown length\"\"\" _value : StreamAdapter async def write ( self , writer : AbstractStreamWriter ) -> None : chunk = await self . _value . read () while chunk : if len ( chunk ) > TOO_LARGE_BYTES_BODY : logger . warning ( \"Sending a large body directly with raw bytes might\" \" lock the event loop. You should probably pass an \" \"io.BytesIO object instead.\" , ) await writer . write ( chunk ) chunk = await self . _value . read ()","title":"AsyncPayload"},{"location":"reference/aiooss2/adapter/#aiooss2.adapter.FilelikeObjectAdapter","text":"Bases: StreamAdapter Adapter for file like data objects Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/adapter.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 class FilelikeObjectAdapter ( StreamAdapter ): \"\"\"Adapter for file like data objects\"\"\" def __init__ ( self , stream , ** kwargs ): super () . __init__ ( stream , ** kwargs ) self . _read_all = False if self . size is None : try : position = self . stream . tell () end = self . stream . seek ( 0 , os . SEEK_END ) self . stream . seek ( position ) self . size = end - position except AttributeError : pass def _length_to_read ( self , amt : int ) -> int : if self . size is None : return amt length_to_read = self . size - self . offset if amt > 0 : length_to_read = min ( amt , length_to_read ) return length_to_read async def read ( self , amt : int = - 1 ) -> bytes : if self . _read_all or ( self . size and self . offset >= self . size ): return b \"\" if self . offset < self . discard and amt : amt += self . discard - self . offset length_to_read = self . _length_to_read ( amt ) if inspect . iscoroutinefunction ( self . stream . read ): content = await self . stream . read ( length_to_read ) else : content = self . stream . read ( length_to_read ) if not content : self . _read_all = True return self . _invoke_callbacks ( content )","title":"FilelikeObjectAdapter"},{"location":"reference/aiooss2/adapter/#aiooss2.adapter.IterableAdapter","text":"Bases: StreamAdapter Adapter for Async Iterable data Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/adapter.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 class IterableAdapter ( StreamAdapter ): \"\"\"Adapter for Async Iterable data\"\"\" def __init__ ( self , stream , discard : int = 0 , ** kwargs ): if hasattr ( stream , \"__aiter__\" ): stream = stream . __aiter__ () elif hasattr ( stream , \"__iter__\" ): stream = iter ( stream ) super () . __init__ ( stream , ** kwargs ) if discard : raise ValueError ( \"discard not supported in Async \" f \"Iterable input { self . stream } \" ) async def read ( self , amt : int = - 1 ) -> bytes : try : if hasattr ( self . stream , \"__anext__\" ): content = await self . stream . __anext__ () elif hasattr ( self . stream , \"__next__\" ): content = next ( self . stream ) else : raise AttributeError ( f \" { self . stream . __class__ . __name__ } is neither\" \" an iterator nor an async iterator\" ) except ( StopIteration , StopAsyncIteration ): return b \"\" return self . _invoke_callbacks ( content )","title":"IterableAdapter"},{"location":"reference/aiooss2/adapter/#aiooss2.adapter.SliceableAdapter","text":"Bases: StreamAdapter Adapter for data can get a slice via stream[a:b] Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/adapter.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class SliceableAdapter ( StreamAdapter ): \"\"\"Adapter for data can get a slice via `stream[a:b]`\"\"\" def __init__ ( self , stream : Union [ bytes , bytearray , memoryview ], ** kwargs , ): \"\"\" Args: size (Optional[int]): size of the data stream. \"\"\" super () . __init__ ( stream , ** kwargs ) self . size : int = self . size or len ( stream ) def _length_to_read ( self , amt : int ) -> int : length_to_read = self . size - self . offset if amt > 0 : length_to_read = min ( amt , length_to_read ) return length_to_read async def read ( self , amt : int = - 1 ) -> bytes : if self . offset >= self . size : return b \"\" length_to_read = self . _length_to_read ( amt ) content = self . stream [ self . offset : self . offset + length_to_read ] return self . _invoke_callbacks ( content )","title":"SliceableAdapter"},{"location":"reference/aiooss2/adapter/#aiooss2.adapter.SliceableAdapter.__init__","text":"Parameters: Name Type Description Default size Optional [ int ] size of the data stream. required Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/adapter.py 105 106 107 108 109 110 111 112 113 114 115 116 def __init__ ( self , stream : Union [ bytes , bytearray , memoryview ], ** kwargs , ): \"\"\" Args: size (Optional[int]): size of the data stream. \"\"\" super () . __init__ ( stream , ** kwargs ) self . size : int = self . size or len ( stream )","title":"__init__()"},{"location":"reference/aiooss2/adapter/#aiooss2.adapter.StreamAdapter","text":"Bases: ABC Adapter for data types Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/adapter.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class StreamAdapter ( ABC ): \"\"\"Adapter for data types\"\"\" def __init__ ( # pylint: disable=too-many-arguments self , stream : Union [ bytes , IO ], progress_callback : Optional [ Callable ] = None , crc_callback : Optional [ Callable ] = None , cipher_callback : Optional [ Callable ] = None , size : Optional [ int ] = None , discard : int = 0 , ): \"\"\" Args: stream (Union[bytes, IO]): original data stream can be bytes or file like object. progress_callback (Optional[Callable], optional): function used for progress bar. crc_callback (Optional[Callable], optional): function used for crc calculation. cipher_callback (Optional[Callable], optional): function used for cipher calculation. size: Optional[int] = None, discard (int): bytes to discard. \"\"\" self . stream = to_bytes ( stream ) self . progress_callback : Optional [ Callable ] = progress_callback self . crc_callback : Optional [ Callable ] = crc_callback self . cipher_callback : Optional [ Callable ] = cipher_callback self . offset = 0 self . size = size self . discard = discard def __aiter__ ( self ) -> AsyncIterator : return self async def __anext__ ( self ) -> bytes : content = await self . read ( _CHUNK_SIZE ) if content : return content raise StopAsyncIteration def __len__ ( self ) -> Optional [ int ]: return self . size @abstractmethod async def read ( self , amt =- 1 ) -> bytes : \"\"\"async api to read a chunk from the data Args: amt (int): batch size of the data to read, -1 to read all \"\"\" @property def crc ( self ): \"\"\"return crc value of the data\"\"\" if self . crc_callback : return self . crc_callback . crc return None def _invoke_callbacks ( self , content : Union [ str , bytes ]): content = to_bytes ( content ) self . offset += len ( content ) real_discard = 0 if self . offset < self . discard : real_discard = ( len ( content ) if len ( content ) <= self . discard else self . discard ) self . discard -= real_discard _invoke_progress_callback ( self . progress_callback , self . offset , self . size ) _invoke_crc_callback ( self . crc_callback , content , real_discard ) content = _invoke_cipher_callback ( self . cipher_callback , content , real_discard ) return content","title":"StreamAdapter"},{"location":"reference/aiooss2/adapter/#aiooss2.adapter.StreamAdapter.crc","text":"return crc value of the data","title":"crc"},{"location":"reference/aiooss2/adapter/#aiooss2.adapter.StreamAdapter.__init__","text":"Parameters: Name Type Description Default stream Union [ bytes , IO ] original data stream can be bytes or file like object. required progress_callback Optional [ Callable ] function used for progress bar. None crc_callback Optional [ Callable ] function used for crc calculation. None cipher_callback Optional [ Callable ] function used for cipher calculation. None size Optional [ int ] Optional[int] = None, None discard int bytes to discard. 0 Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/adapter.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( # pylint: disable=too-many-arguments self , stream : Union [ bytes , IO ], progress_callback : Optional [ Callable ] = None , crc_callback : Optional [ Callable ] = None , cipher_callback : Optional [ Callable ] = None , size : Optional [ int ] = None , discard : int = 0 , ): \"\"\" Args: stream (Union[bytes, IO]): original data stream can be bytes or file like object. progress_callback (Optional[Callable], optional): function used for progress bar. crc_callback (Optional[Callable], optional): function used for crc calculation. cipher_callback (Optional[Callable], optional): function used for cipher calculation. size: Optional[int] = None, discard (int): bytes to discard. \"\"\" self . stream = to_bytes ( stream ) self . progress_callback : Optional [ Callable ] = progress_callback self . crc_callback : Optional [ Callable ] = crc_callback self . cipher_callback : Optional [ Callable ] = cipher_callback self . offset = 0 self . size = size self . discard = discard","title":"__init__()"},{"location":"reference/aiooss2/adapter/#aiooss2.adapter.StreamAdapter.read","text":"async api to read a chunk from the data Parameters: Name Type Description Default amt int batch size of the data to read, -1 to read all -1 Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/adapter.py 69 70 71 72 73 74 75 @abstractmethod async def read ( self , amt =- 1 ) -> bytes : \"\"\"async api to read a chunk from the data Args: amt (int): batch size of the data to read, -1 to read all \"\"\"","title":"read()"},{"location":"reference/aiooss2/api/","text":"Module for Bucket and Service AioBucket Bases: _AioBase Used for Bucket and Object operations, creating\u3001deleting Bucket, uploading\u3001downloading Object, etc\u3002 use case (bucket in HangZhou area):: import oss2 import aiooss2 import asyncio auth = oss2.Auth('your-access-key-id', 'your-access-key-secret') bucket = aiooss2.Bucket(auth, 'http://oss-cn-hangzhou.aliyuncs.com', 'your-bucket') def upload(): data = b\"\u0001\" * 1024 resp = await bucket.put_object('readme.txt', 'content of the object') return resp loop = asyncio.get_event_loop() loop.run_until_complete(upload()) Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 class AioBucket ( _AioBase ): \"\"\"Used for Bucket and Object operations, creating\u3001deleting Bucket, uploading\u3001downloading Object, etc\u3002 use case (bucket in HangZhou area):: >>> import oss2 >>> import aiooss2 >>> import asyncio >>> auth = oss2.Auth('your-access-key-id', 'your-access-key-secret') >>> bucket = aiooss2.Bucket(auth, 'http://oss-cn-hangzhou.aliyuncs.com', >>> 'your-bucket') >>> def upload(): >>> data = b\"\\x01\" * 1024 >>> resp = await bucket.put_object('readme.txt', >>> 'content of the object') >>> return resp >>> loop = asyncio.get_event_loop() >>> loop.run_until_complete(upload()) <oss2.models.PutObjectResult object at 0x029B9930> \"\"\" auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ] def __init__ ( self , auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ], endpoint : str , bucket_name : str , is_cname : bool = False , ** kwargs , ): \"\"\" Args: bucket_name (str): the bucket name to operate \"\"\" self . bucket_name = bucket_name . strip () if is_valid_bucket_name ( self . bucket_name ) is not True : raise ClientError ( f \"The bucket_name ' { self . bucket_name } ' is invalid, please check it.\" ) super () . __init__ ( auth , endpoint , is_cname , ** kwargs , ) async def _do_object ( self , method : str , key : Union [ bytes , str ], ** kwargs ) -> \"AioResponse\" : return await self . _do ( method , self . bucket_name , key , ** kwargs ) async def _do_bucket ( self , method : str , ** kwargs ) -> \"AioResponse\" : return await self . _do ( method , self . bucket_name , \"\" , ** kwargs ) async def put_object ( self , key : str , data , headers : Optional [ Mapping ] = None , progress_callback : Optional [ Callable ] = None , ) -> \"PutObjectResult\" : \"\"\"upload some contents to an object (use case) :: >>> await bucket.put_object('readme.txt', 'content of readme.txt') >>> with open(u'local_file.txt', 'rb') as f: >>> await bucket.put_object('remote_file.txt', f) Args: key (str): object name to upload data (Union[str, bytes, IO, Iterable]): contents to upload headers (Optional[Mapping], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. Returns: PutObjectResult: \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), key ) data = make_adapter ( data , progress_callback = progress_callback , enable_crc = self . enable_crc , ) resp : \"AioResponse\" = await self . _do_object ( \"PUT\" , key , data = data , headers = headers ) logger . debug ( \"Put object done\" ) result = PutObjectResult ( resp ) if self . enable_crc and result . crc is not None : check_crc ( \"put object\" , data . crc , result . crc , result . request_id ) return result async def get_object ( # pylint: disable=too-many-arguments self , key : str , byte_range : Optional [ Sequence [ Optional [ int ]]] = None , headers : Optional [ dict ] = None , progress_callback : Optional [ Callable ] = None , process = None , params : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> AioGetObjectResult : \"\"\"download the contents of an object (use case) :: >>> async with await bucket.get_object(\"helloword\") as result >>> print(await result.read()) 'hello world' Args: key (str): object name to download. byte_range (Optional[Sequence[Optional[int]]], optional): Range to download. headers (Optional[dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. process (_type_, optional): oss file process method. params (Optional[Union[Dict, CaseInsensitiveDict]], optional): Returns: AioGetObjectResult: \"\"\" headers_dict : CaseInsensitiveDict = CaseInsensitiveDict ( headers ) range_string = _make_range_string ( byte_range ) if range_string : headers_dict [ \"range\" ] = range_string params = {} if params is None else params if process : params [ Bucket . PROCESS ] = process resp = await self . _do_object ( \"GET\" , key , headers = headers_dict , params = params ) logger . debug ( \"Get object done\" ) return AioGetObjectResult ( resp , progress_callback , self . enable_crc ) async def delete_object ( self , key : str , params : Union [ Dict , CaseInsensitiveDict ] = None , headers : Optional [ Dict ] = None , ) -> \"RequestResult\" : \"\"\"delete an object (use case) :: >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object1' 'object2' >>> await bucket.delete_object(\"object1\") >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object2' Args: key (str): _description_ headers (Optional[Dict], optional): HTTP headers to specify. params (Union[Dict, CaseInsensitiveDict], optional): Returns: RequestResult: \"\"\" resp = await self . _do_object ( \"DELETE\" , key , params = params , headers = headers ) logger . debug ( \"Delete object done\" ) return RequestResult ( resp ) async def list_objects ( # pylint: disable=too-many-arguments self , prefix : str = \"\" , delimiter : str = \"\" , marker : str = \"\" , max_keys : int = 100 , headers : Optional [ Dict ] = None , ) -> \"ListObjectsResult\" : \"\"\"list objects in a bucket (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Args: prefix (str, optional): only list objects start with this prefix. delimiter (str, optional): delimiter as a folder separator. marker (str, optional): use in paginate. max_keys (int, optional): numbers of objects for one page. headers (Optional[Dict], optional): HTTP headers to specify. Returns: ListObjectsResult: \"\"\" headers = CaseInsensitiveDict ( headers ) resp = await self . _do_object ( \"GET\" , \"\" , params = { \"prefix\" : prefix , \"delimiter\" : delimiter , \"marker\" : marker , \"max-keys\" : str ( max_keys ), \"encoding-type\" : \"url\" , }, headers = headers , ) logger . debug ( \"List objects done\" ) return await self . _parse_result ( resp , parse_list_objects , ListObjectsResult ) async def get_object_meta ( self , key : str , params : Optional [ Union [ dict , CaseInsensitiveDict ]] = None , headers : Optional [ Dict ] = None , ) -> \"GetObjectMetaResult\" : \"\"\"get meta data from object (use case) :: >>> result = await bucket.get_object_meta(object_name) >>> print(result.content_length ) 256 Args: key (str): object key params (Optional[Union[dict, CaseInsensitiveDict]], optional): headers (Optional[Dict], optional): HTTP headers to specify. Returns: GetObjectMetaResult: \"\"\" headers = CaseInsensitiveDict ( headers ) if params is None : params = {} if Bucket . OBJECTMETA not in params : params [ Bucket . OBJECTMETA ] = \"\" resp = await self . _do_object ( \"GET\" , key , params = params , headers = headers ) logger . debug ( \"Get object metadata done\" ) return GetObjectMetaResult ( resp ) async def object_exists ( self , key : str , headers : Optional [ Dict ] = None ) -> bool : \"\"\"Return True if key exists, False otherwise. raise Exception for other exceptions. (use case) :: >>> result = await bucket.object_exists('object1') >>> print(result) True Args: key (str): key of the object headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. Returns: bool: \"\"\" try : await self . get_object_meta ( key , headers = headers ) except NoSuchKey : return False return True async def get_bucket_info ( self ) -> GetBucketInfoResult : \"\"\"Get bucket information, `Create time`, `Endpoint`, `Owner`, `ACL` (use case) :: >>> resp = await aiobucket.get_bucket_info() >>> print(resp.name) 'bucket_name' Returns: GetBucketInfoResult \"\"\" resp = await self . _do_bucket ( \"GET\" , params = { Bucket . BUCKET_INFO : \"\" }) logger . debug ( \"Get bucket info done\" ) return await self . _parse_result ( resp , parse_get_bucket_info , GetBucketInfoResult ) async def append_object ( self , key : str , position : int , data , headers : Optional [ Dict ] = None , progress_callback : Optional [ Callable ] = None , init_crc : Optional [ int ] = None , ) -> \"AppendObjectResult\" : \"\"\"Append value to an object (use case) :: >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' >>> resp = await bucket.append_object('object1', 11, \"!!!\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world!!!' Args: key (str): key of the object position (int): position to append data (_type_): data to append headers (Optional[Dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. init_crc (Optional[int], optional): init value of the crc Returns: AppendObjectResult: \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), key ) data = make_adapter ( data , progress_callback = progress_callback , enable_crc = self . enable_crc , ) resp = await self . _do_object ( \"POST\" , key , data = data , headers = headers , params = { \"append\" : \"\" , \"position\" : str ( position )}, ) logger . debug ( \"Append object done\" ) result = AppendObjectResult ( resp ) if self . enable_crc and result . crc is not None and init_crc is not None : check_crc ( \"append object\" , data . crc , result . crc , result . request_id ) return result async def put_object_from_file ( self , key : str , filename : str , headers : Optional [ Mapping ] = None , progress_callback : Optional [ Callable ] = None , ) -> \"PutObjectResult\" : \"\"\"Upload a local file to a oss key (use case) :: >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" >>> result = await aiobucket.put_object_from_file(\"object1\", \"file\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' Args: key (str): key of the oss filename (str): filename to upload headers (Optional[Mapping], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. Returns: _type_: _description_ \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), filename ) with open ( to_unicode ( filename ), \"rb\" ) as f_stream : return await self . put_object ( key , f_stream , headers = headers , progress_callback = progress_callback , ) async def get_object_to_file ( self , key : str , filename : str , byte_range : Optional [ Sequence [ int ]] = None , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , progress_callback : Optional [ Callable ] = None , process : Optional [ Callable ] = None , params : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> AioGetObjectResult : \"\"\"Download contents of object to file. (use case) :: >>> result = await aiobucket.get_object_to_file(\"object1\", \"file\") >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" Args: key (str): object name to download. filename (str): filename to save the data downloaded. byte_range (Optional[Sequence[Optional[int]]], optional): Range to download. headers (Optional[dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. process (_type_, optional): oss file process method. params (Optional[Union[Dict, CaseInsensitiveDict]], optional): Returns: AioGetObjectResult: \"\"\" with open ( to_unicode ( filename ), \"wb\" ) as f_w : result = await self . get_object ( key , byte_range = byte_range , headers = headers , progress_callback = progress_callback , process = process , params = params , ) if result . content_length is None : copyfileobj ( result , f_w ) else : await copyfileobj_and_verify ( result , f_w , result . content_length , request_id = result . request_id , ) if self . enable_crc and byte_range is None : if ( ( headers is None ) or ( \"Accept-Encoding\" not in headers ) or ( headers [ \"Accept-Encoding\" ] != \"gzip\" ) ): check_crc ( \"get\" , result . client_crc , result . server_crc , result . request_id , ) return result async def batch_delete_objects ( self , key_list : List [ str ], headers : Optional [ Dict ] = None ) -> BatchDeleteObjectsResult : \"\"\"Delete a batch of objects (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' 'object3' 'object4' 'object5' 'object6' >>> await aiobucket.batch_delete_objects([\"object1\", \"object2\", \"object3\"]) >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object4' 'object5' 'object6' Args: key_list (List[str]): list of objects to delete. headers (Optional[Dict], optional): HTTP headers to specify. Raises: ClientError: Returns: BatchDeleteObjectResult: \"\"\" if not key_list : raise ClientError ( \"key_list should not be empty\" ) data = to_batch_delete_objects_request ( key_list , False ) header_dict : CaseInsensitiveDict = CaseInsensitiveDict ( headers ) header_dict [ \"Content-MD5\" ] = content_md5 ( data ) resp = await self . _do_object ( \"POST\" , \"\" , data = data , params = { \"delete\" : \"\" , \"encoding-type\" : \"url\" }, headers = header_dict , ) return await self . _parse_result ( resp , parse_batch_delete_objects , BatchDeleteObjectsResult ) async def copy_object ( self , source_bucket_name : str , source_key : str , target_key : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Dict ] = None , ) -> PutObjectResult : \"\"\"copy object to another place (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' >>> await aiobucket.copy_object(bucket_name, \"object1\", \"object2\") >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Args: source_bucket_name (str): source object bucket source_key (str): source object key target_key (str): target object key headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Dict], optional): Returns: PutObjectResult: \"\"\" headers = CaseInsensitiveDict ( headers ) if params and Bucket . VERSIONID in params : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) + \"?versionId=\" + params [ Bucket . VERSIONID ] ) else : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) ) resp = await self . _do_object ( \"PUT\" , target_key , headers = headers ) return PutObjectResult ( resp ) async def head_object ( self , key : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Mapping ] = None , ) -> \"HeadObjectResult\" : \"\"\"Get object metadata >>> result = bucket.head_object('readme.txt') >>> print(result.content_type) text/plain Args: key (str): object key headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Mapping], optional): Returns: HeadObjectResult: Raises: `NotFound <oss2.exceptions.NotFound>` if object does not exist. \"\"\" logger . debug ( \"Start to head object, bucket: %s , key: %s , headers: %s \" , self . bucket_name , to_string ( key ), headers , ) resp = await self . _do_object ( \"HEAD\" , key , headers = headers , params = params ) logger . debug ( \"Head object done, req_id: %s , status_code: %s \" , resp . request_id , resp . status , ) return await self . _parse_result ( resp , parse_dummy_result , HeadObjectResult ) async def abort_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> RequestResult : \"\"\"abort multipart uploading process\"\"\" return await abort_multipart_upload ( self , * args , ** kwargs ) async def complete_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"Complete multipart uploading process create a new file.\"\"\" return await complete_multipart_upload ( self , * args , ** kwargs ) async def init_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> InitMultipartUploadResult : \"\"\"initialize multipart uploading The returning upload id, bucket name and object name together defines this uploading event.\"\"\" return await init_multipart_upload ( self , * args , ** kwargs ) async def list_multipart_uploads ( self : \"AioBucket\" , * args , ** kwargs ) -> ListMultipartUploadsResult : \"\"\"List multipart uploading process\"\"\" return await list_multipart_uploads ( self , * args , ** kwargs ) async def list_parts ( self : \"AioBucket\" , * args , ** kwargs ) -> ListPartsResult : \"\"\"list uploaded parts in a part uploading progress.\"\"\" return await list_parts ( self , * args , ** kwargs ) async def upload_part ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"upload single part.\"\"\" return await upload_part ( self , * args , ** kwargs ) async def upload_part_copy ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"copy part or whole of a source file to a slice of a target file.\"\"\" return await upload_part_copy ( self , * args , ** kwargs ) __init__ ( auth , endpoint , bucket_name , is_cname = False , kwargs ) Parameters: Name Type Description Default bucket_name str the bucket name to operate required Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 def __init__ ( self , auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ], endpoint : str , bucket_name : str , is_cname : bool = False , ** kwargs , ): \"\"\" Args: bucket_name (str): the bucket name to operate \"\"\" self . bucket_name = bucket_name . strip () if is_valid_bucket_name ( self . bucket_name ) is not True : raise ClientError ( f \"The bucket_name ' { self . bucket_name } ' is invalid, please check it.\" ) super () . __init__ ( auth , endpoint , is_cname , ** kwargs , ) abort_multipart_upload ( args , kwargs ) async abort multipart uploading process Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 799 800 801 802 803 async def abort_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> RequestResult : \"\"\"abort multipart uploading process\"\"\" return await abort_multipart_upload ( self , * args , ** kwargs ) append_object ( key , position , data , headers = None , progress_callback = None , init_crc = None ) async Append value to an object (use case) :: >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' >>> resp = await bucket.append_object('object1', 11, \"!!!\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world!!!' Parameters: Name Type Description Default key str key of the object required position int position to append required data _type_ data to append required headers Optional [ Dict ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None init_crc Optional [ int ] init value of the crc None Returns: Name Type Description AppendObjectResult AppendObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 async def append_object ( self , key : str , position : int , data , headers : Optional [ Dict ] = None , progress_callback : Optional [ Callable ] = None , init_crc : Optional [ int ] = None , ) -> \"AppendObjectResult\" : \"\"\"Append value to an object (use case) :: >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' >>> resp = await bucket.append_object('object1', 11, \"!!!\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world!!!' Args: key (str): key of the object position (int): position to append data (_type_): data to append headers (Optional[Dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. init_crc (Optional[int], optional): init value of the crc Returns: AppendObjectResult: \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), key ) data = make_adapter ( data , progress_callback = progress_callback , enable_crc = self . enable_crc , ) resp = await self . _do_object ( \"POST\" , key , data = data , headers = headers , params = { \"append\" : \"\" , \"position\" : str ( position )}, ) logger . debug ( \"Append object done\" ) result = AppendObjectResult ( resp ) if self . enable_crc and result . crc is not None and init_crc is not None : check_crc ( \"append object\" , data . crc , result . crc , result . request_id ) return result batch_delete_objects ( key_list , headers = None ) async Delete a batch of objects (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' 'object3' 'object4' 'object5' 'object6' >>> await aiobucket.batch_delete_objects([\"object1\", \"object2\", \"object3\"]) >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object4' 'object5' 'object6' Parameters: Name Type Description Default key_list List [ str ] list of objects to delete. required headers Optional [ Dict ] HTTP headers to specify. None Raises: Type Description ClientError Returns: Name Type Description BatchDeleteObjectResult BatchDeleteObjectsResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 async def batch_delete_objects ( self , key_list : List [ str ], headers : Optional [ Dict ] = None ) -> BatchDeleteObjectsResult : \"\"\"Delete a batch of objects (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' 'object3' 'object4' 'object5' 'object6' >>> await aiobucket.batch_delete_objects([\"object1\", \"object2\", \"object3\"]) >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object4' 'object5' 'object6' Args: key_list (List[str]): list of objects to delete. headers (Optional[Dict], optional): HTTP headers to specify. Raises: ClientError: Returns: BatchDeleteObjectResult: \"\"\" if not key_list : raise ClientError ( \"key_list should not be empty\" ) data = to_batch_delete_objects_request ( key_list , False ) header_dict : CaseInsensitiveDict = CaseInsensitiveDict ( headers ) header_dict [ \"Content-MD5\" ] = content_md5 ( data ) resp = await self . _do_object ( \"POST\" , \"\" , data = data , params = { \"delete\" : \"\" , \"encoding-type\" : \"url\" }, headers = header_dict , ) return await self . _parse_result ( resp , parse_batch_delete_objects , BatchDeleteObjectsResult ) complete_multipart_upload ( args , kwargs ) async Complete multipart uploading process create a new file. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 805 806 807 808 809 async def complete_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"Complete multipart uploading process create a new file.\"\"\" return await complete_multipart_upload ( self , * args , ** kwargs ) copy_object ( source_bucket_name , source_key , target_key , headers = None , params = None ) async copy object to another place (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' >>> await aiobucket.copy_object(bucket_name, \"object1\", \"object2\") >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Parameters: Name Type Description Default source_bucket_name str source object bucket required source_key str source object key required target_key str target object key required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None params Optional [ Dict ] None Returns: Name Type Description PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 async def copy_object ( self , source_bucket_name : str , source_key : str , target_key : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Dict ] = None , ) -> PutObjectResult : \"\"\"copy object to another place (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' >>> await aiobucket.copy_object(bucket_name, \"object1\", \"object2\") >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Args: source_bucket_name (str): source object bucket source_key (str): source object key target_key (str): target object key headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Dict], optional): Returns: PutObjectResult: \"\"\" headers = CaseInsensitiveDict ( headers ) if params and Bucket . VERSIONID in params : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) + \"?versionId=\" + params [ Bucket . VERSIONID ] ) else : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) ) resp = await self . _do_object ( \"PUT\" , target_key , headers = headers ) return PutObjectResult ( resp ) delete_object ( key , params = None , headers = None ) async delete an object (use case) :: >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object1' 'object2' >>> await bucket.delete_object(\"object1\") >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object2' Parameters: Name Type Description Default key str description required headers Optional [ Dict ] HTTP headers to specify. None params Union [ Dict , CaseInsensitiveDict ] None Returns: Name Type Description RequestResult RequestResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 async def delete_object ( self , key : str , params : Union [ Dict , CaseInsensitiveDict ] = None , headers : Optional [ Dict ] = None , ) -> \"RequestResult\" : \"\"\"delete an object (use case) :: >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object1' 'object2' >>> await bucket.delete_object(\"object1\") >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object2' Args: key (str): _description_ headers (Optional[Dict], optional): HTTP headers to specify. params (Union[Dict, CaseInsensitiveDict], optional): Returns: RequestResult: \"\"\" resp = await self . _do_object ( \"DELETE\" , key , params = params , headers = headers ) logger . debug ( \"Delete object done\" ) return RequestResult ( resp ) get_bucket_info () async Get bucket information, Create time , Endpoint , Owner , ACL (use case) :: >>> resp = await aiobucket.get_bucket_info() >>> print(resp.name) 'bucket_name' Returns: Type Description GetBucketInfoResult GetBucketInfoResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 async def get_bucket_info ( self ) -> GetBucketInfoResult : \"\"\"Get bucket information, `Create time`, `Endpoint`, `Owner`, `ACL` (use case) :: >>> resp = await aiobucket.get_bucket_info() >>> print(resp.name) 'bucket_name' Returns: GetBucketInfoResult \"\"\" resp = await self . _do_bucket ( \"GET\" , params = { Bucket . BUCKET_INFO : \"\" }) logger . debug ( \"Get bucket info done\" ) return await self . _parse_result ( resp , parse_get_bucket_info , GetBucketInfoResult ) get_object ( key , byte_range = None , headers = None , progress_callback = None , process = None , params = None ) async download the contents of an object (use case) :: >>> async with await bucket.get_object(\"helloword\") as result >>> print(await result.read()) 'hello world' Parameters: Name Type Description Default key str object name to download. required byte_range Optional [ Sequence [ Optional [ int ]]] Range to download. None headers Optional [ dict ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None process _type_ oss file process method. None params Optional [ Union [ Dict , CaseInsensitiveDict ]] None Returns: Name Type Description AioGetObjectResult AioGetObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 async def get_object ( # pylint: disable=too-many-arguments self , key : str , byte_range : Optional [ Sequence [ Optional [ int ]]] = None , headers : Optional [ dict ] = None , progress_callback : Optional [ Callable ] = None , process = None , params : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> AioGetObjectResult : \"\"\"download the contents of an object (use case) :: >>> async with await bucket.get_object(\"helloword\") as result >>> print(await result.read()) 'hello world' Args: key (str): object name to download. byte_range (Optional[Sequence[Optional[int]]], optional): Range to download. headers (Optional[dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. process (_type_, optional): oss file process method. params (Optional[Union[Dict, CaseInsensitiveDict]], optional): Returns: AioGetObjectResult: \"\"\" headers_dict : CaseInsensitiveDict = CaseInsensitiveDict ( headers ) range_string = _make_range_string ( byte_range ) if range_string : headers_dict [ \"range\" ] = range_string params = {} if params is None else params if process : params [ Bucket . PROCESS ] = process resp = await self . _do_object ( \"GET\" , key , headers = headers_dict , params = params ) logger . debug ( \"Get object done\" ) return AioGetObjectResult ( resp , progress_callback , self . enable_crc ) get_object_meta ( key , params = None , headers = None ) async get meta data from object (use case) :: >>> result = await bucket.get_object_meta(object_name) >>> print(result.content_length ) 256 Parameters: Name Type Description Default key str object key required params Optional [ Union [ dict , CaseInsensitiveDict ]] None headers Optional [ Dict ] HTTP headers to specify. None Returns: Name Type Description GetObjectMetaResult GetObjectMetaResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 async def get_object_meta ( self , key : str , params : Optional [ Union [ dict , CaseInsensitiveDict ]] = None , headers : Optional [ Dict ] = None , ) -> \"GetObjectMetaResult\" : \"\"\"get meta data from object (use case) :: >>> result = await bucket.get_object_meta(object_name) >>> print(result.content_length ) 256 Args: key (str): object key params (Optional[Union[dict, CaseInsensitiveDict]], optional): headers (Optional[Dict], optional): HTTP headers to specify. Returns: GetObjectMetaResult: \"\"\" headers = CaseInsensitiveDict ( headers ) if params is None : params = {} if Bucket . OBJECTMETA not in params : params [ Bucket . OBJECTMETA ] = \"\" resp = await self . _do_object ( \"GET\" , key , params = params , headers = headers ) logger . debug ( \"Get object metadata done\" ) return GetObjectMetaResult ( resp ) get_object_to_file ( key , filename , byte_range = None , headers = None , progress_callback = None , process = None , params = None ) async Download contents of object to file. (use case) :: >>> result = await aiobucket.get_object_to_file(\"object1\", \"file\") >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" Parameters: Name Type Description Default key str object name to download. required filename str filename to save the data downloaded. required byte_range Optional [ Sequence [ Optional [ int ]]] Range to download. None headers Optional [ dict ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None process _type_ oss file process method. None params Optional [ Union [ Dict , CaseInsensitiveDict ]] None Returns: Name Type Description AioGetObjectResult AioGetObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 async def get_object_to_file ( self , key : str , filename : str , byte_range : Optional [ Sequence [ int ]] = None , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , progress_callback : Optional [ Callable ] = None , process : Optional [ Callable ] = None , params : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> AioGetObjectResult : \"\"\"Download contents of object to file. (use case) :: >>> result = await aiobucket.get_object_to_file(\"object1\", \"file\") >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" Args: key (str): object name to download. filename (str): filename to save the data downloaded. byte_range (Optional[Sequence[Optional[int]]], optional): Range to download. headers (Optional[dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. process (_type_, optional): oss file process method. params (Optional[Union[Dict, CaseInsensitiveDict]], optional): Returns: AioGetObjectResult: \"\"\" with open ( to_unicode ( filename ), \"wb\" ) as f_w : result = await self . get_object ( key , byte_range = byte_range , headers = headers , progress_callback = progress_callback , process = process , params = params , ) if result . content_length is None : copyfileobj ( result , f_w ) else : await copyfileobj_and_verify ( result , f_w , result . content_length , request_id = result . request_id , ) if self . enable_crc and byte_range is None : if ( ( headers is None ) or ( \"Accept-Encoding\" not in headers ) or ( headers [ \"Accept-Encoding\" ] != \"gzip\" ) ): check_crc ( \"get\" , result . client_crc , result . server_crc , result . request_id , ) return result head_object ( key , headers = None , params = None ) async Get object metadata >>> result = bucket.head_object('readme.txt') >>> print(result.content_type) text/plain Parameters: Name Type Description Default key str object key required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None params Optional [ Mapping ] None Returns: Name Type Description HeadObjectResult HeadObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 async def head_object ( self , key : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Mapping ] = None , ) -> \"HeadObjectResult\" : \"\"\"Get object metadata >>> result = bucket.head_object('readme.txt') >>> print(result.content_type) text/plain Args: key (str): object key headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Mapping], optional): Returns: HeadObjectResult: Raises: `NotFound <oss2.exceptions.NotFound>` if object does not exist. \"\"\" logger . debug ( \"Start to head object, bucket: %s , key: %s , headers: %s \" , self . bucket_name , to_string ( key ), headers , ) resp = await self . _do_object ( \"HEAD\" , key , headers = headers , params = params ) logger . debug ( \"Head object done, req_id: %s , status_code: %s \" , resp . request_id , resp . status , ) return await self . _parse_result ( resp , parse_dummy_result , HeadObjectResult ) init_multipart_upload ( args , kwargs ) async initialize multipart uploading The returning upload id, bucket name and object name together defines this uploading event. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 811 812 813 814 815 816 817 async def init_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> InitMultipartUploadResult : \"\"\"initialize multipart uploading The returning upload id, bucket name and object name together defines this uploading event.\"\"\" return await init_multipart_upload ( self , * args , ** kwargs ) list_multipart_uploads ( args , kwargs ) async List multipart uploading process Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 819 820 821 822 823 async def list_multipart_uploads ( self : \"AioBucket\" , * args , ** kwargs ) -> ListMultipartUploadsResult : \"\"\"List multipart uploading process\"\"\" return await list_multipart_uploads ( self , * args , ** kwargs ) list_objects ( prefix = '' , delimiter = '' , marker = '' , max_keys = 100 , headers = None ) async list objects in a bucket (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Parameters: Name Type Description Default prefix str only list objects start with this prefix. '' delimiter str delimiter as a folder separator. '' marker str use in paginate. '' max_keys int numbers of objects for one page. 100 headers Optional [ Dict ] HTTP headers to specify. None Returns: Name Type Description ListObjectsResult ListObjectsResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 async def list_objects ( # pylint: disable=too-many-arguments self , prefix : str = \"\" , delimiter : str = \"\" , marker : str = \"\" , max_keys : int = 100 , headers : Optional [ Dict ] = None , ) -> \"ListObjectsResult\" : \"\"\"list objects in a bucket (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Args: prefix (str, optional): only list objects start with this prefix. delimiter (str, optional): delimiter as a folder separator. marker (str, optional): use in paginate. max_keys (int, optional): numbers of objects for one page. headers (Optional[Dict], optional): HTTP headers to specify. Returns: ListObjectsResult: \"\"\" headers = CaseInsensitiveDict ( headers ) resp = await self . _do_object ( \"GET\" , \"\" , params = { \"prefix\" : prefix , \"delimiter\" : delimiter , \"marker\" : marker , \"max-keys\" : str ( max_keys ), \"encoding-type\" : \"url\" , }, headers = headers , ) logger . debug ( \"List objects done\" ) return await self . _parse_result ( resp , parse_list_objects , ListObjectsResult ) list_parts ( args , kwargs ) async list uploaded parts in a part uploading progress. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 825 826 827 async def list_parts ( self : \"AioBucket\" , * args , ** kwargs ) -> ListPartsResult : \"\"\"list uploaded parts in a part uploading progress.\"\"\" return await list_parts ( self , * args , ** kwargs ) object_exists ( key , headers = None ) async Return True if key exists, False otherwise. raise Exception for other exceptions. (use case) :: >>> result = await bucket.object_exists('object1') >>> print(result) True Parameters: Name Type Description Default key str key of the object required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] None Returns: Name Type Description bool bool Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 async def object_exists ( self , key : str , headers : Optional [ Dict ] = None ) -> bool : \"\"\"Return True if key exists, False otherwise. raise Exception for other exceptions. (use case) :: >>> result = await bucket.object_exists('object1') >>> print(result) True Args: key (str): key of the object headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. Returns: bool: \"\"\" try : await self . get_object_meta ( key , headers = headers ) except NoSuchKey : return False return True put_object ( key , data , headers = None , progress_callback = None ) async upload some contents to an object (use case) :: >>> await bucket.put_object('readme.txt', 'content of readme.txt') >>> with open(u'local_file.txt', 'rb') as f: >>> await bucket.put_object('remote_file.txt', f) Parameters: Name Type Description Default key str object name to upload required data Union [ str , bytes , IO , Iterable ] contents to upload required headers Optional [ Mapping ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None Returns: Name Type Description PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 async def put_object ( self , key : str , data , headers : Optional [ Mapping ] = None , progress_callback : Optional [ Callable ] = None , ) -> \"PutObjectResult\" : \"\"\"upload some contents to an object (use case) :: >>> await bucket.put_object('readme.txt', 'content of readme.txt') >>> with open(u'local_file.txt', 'rb') as f: >>> await bucket.put_object('remote_file.txt', f) Args: key (str): object name to upload data (Union[str, bytes, IO, Iterable]): contents to upload headers (Optional[Mapping], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. Returns: PutObjectResult: \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), key ) data = make_adapter ( data , progress_callback = progress_callback , enable_crc = self . enable_crc , ) resp : \"AioResponse\" = await self . _do_object ( \"PUT\" , key , data = data , headers = headers ) logger . debug ( \"Put object done\" ) result = PutObjectResult ( resp ) if self . enable_crc and result . crc is not None : check_crc ( \"put object\" , data . crc , result . crc , result . request_id ) return result put_object_from_file ( key , filename , headers = None , progress_callback = None ) async Upload a local file to a oss key (use case) :: >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" >>> result = await aiobucket.put_object_from_file(\"object1\", \"file\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' Parameters: Name Type Description Default key str key of the oss required filename str filename to upload required headers Optional [ Mapping ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None Returns: Name Type Description _type_ PutObjectResult description Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 async def put_object_from_file ( self , key : str , filename : str , headers : Optional [ Mapping ] = None , progress_callback : Optional [ Callable ] = None , ) -> \"PutObjectResult\" : \"\"\"Upload a local file to a oss key (use case) :: >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" >>> result = await aiobucket.put_object_from_file(\"object1\", \"file\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' Args: key (str): key of the oss filename (str): filename to upload headers (Optional[Mapping], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. Returns: _type_: _description_ \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), filename ) with open ( to_unicode ( filename ), \"rb\" ) as f_stream : return await self . put_object ( key , f_stream , headers = headers , progress_callback = progress_callback , ) upload_part ( args , kwargs ) async upload single part. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 829 830 831 async def upload_part ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"upload single part.\"\"\" return await upload_part ( self , * args , ** kwargs ) upload_part_copy ( args , kwargs ) async copy part or whole of a source file to a slice of a target file. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 833 834 835 async def upload_part_copy ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"copy part or whole of a source file to a slice of a target file.\"\"\" return await upload_part_copy ( self , * args , ** kwargs ) AioService Bases: _AioBase Service class used for operations like list all bucket Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 class AioService ( _AioBase ): \"\"\"Service class used for operations like list all bucket\"\"\" def __init__ ( self , auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ], endpoint : str , session : Optional [ AioSession ] = None , connect_timeout : Optional [ int ] = None , app_name : str = \"\" , proxies = None , ): \"\"\"_summary_ Args: auth (Union[Auth, AnonymousAuth, StsAuth]): Auth class. endpoint (str): endpoint address or CNAME. session (Optional[AioSession], optional): reuse a custom session. connect_timeout (int): connection. app_name (str, optional): app name. proxies (_type_, optional): proxies settings. \"\"\" super () . __init__ ( auth , endpoint , False , session , connect_timeout , app_name = app_name , proxies = proxies , ) async def list_buckets ( self , prefix : str = \"\" , marker : str = \"\" , max_keys : int = 100 , params : Optional [ Dict ] = None , ) -> ListBucketsResult : \"\"\"List buckets with given prefix of an user (use case) :: >>> async for obj in AioBucketIterator(aioservice): >>> print(obj.name) 'bucket_name1' 'bucket_name2' ... Args: prefix (str, optional): prefix to filter the buckets results. marker (str, optional): paginate separator. max_keys (int, optional): max return number per page. params (Optional[Dict], optional): Some optional params. Returns: oss2.models.ListBucketsResult: \"\"\" list_param = {} list_param [ \"prefix\" ] = prefix list_param [ \"marker\" ] = marker list_param [ \"max-keys\" ] = str ( max_keys ) if params is not None : if \"tag-key\" in params : list_param [ \"tag-key\" ] = params [ \"tag-key\" ] if \"tag-value\" in params : list_param [ \"tag-value\" ] = params [ \"tag-value\" ] resp = await self . _do ( \"GET\" , \"\" , \"\" , params = list_param ) logger . debug ( \"List buckets done\" ) return await self . _parse_result ( resp , parse_list_buckets , ListBucketsResult ) __init__ ( auth , endpoint , session = None , connect_timeout = None , app_name = '' , proxies = None ) summary Parameters: Name Type Description Default auth Union [ Auth , AnonymousAuth , StsAuth ] Auth class. required endpoint str endpoint address or CNAME. required session Optional [ AioSession ] reuse a custom session. None connect_timeout int connection. None app_name str app name. '' proxies _type_ proxies settings. None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 def __init__ ( self , auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ], endpoint : str , session : Optional [ AioSession ] = None , connect_timeout : Optional [ int ] = None , app_name : str = \"\" , proxies = None , ): \"\"\"_summary_ Args: auth (Union[Auth, AnonymousAuth, StsAuth]): Auth class. endpoint (str): endpoint address or CNAME. session (Optional[AioSession], optional): reuse a custom session. connect_timeout (int): connection. app_name (str, optional): app name. proxies (_type_, optional): proxies settings. \"\"\" super () . __init__ ( auth , endpoint , False , session , connect_timeout , app_name = app_name , proxies = proxies , ) list_buckets ( prefix = '' , marker = '' , max_keys = 100 , params = None ) async List buckets with given prefix of an user (use case) :: >>> async for obj in AioBucketIterator(aioservice): >>> print(obj.name) 'bucket_name1' 'bucket_name2' ... Parameters: Name Type Description Default prefix str prefix to filter the buckets results. '' marker str paginate separator. '' max_keys int max return number per page. 100 params Optional [ Dict ] Some optional params. None Returns: Type Description ListBucketsResult oss2.models.ListBucketsResult: Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 async def list_buckets ( self , prefix : str = \"\" , marker : str = \"\" , max_keys : int = 100 , params : Optional [ Dict ] = None , ) -> ListBucketsResult : \"\"\"List buckets with given prefix of an user (use case) :: >>> async for obj in AioBucketIterator(aioservice): >>> print(obj.name) 'bucket_name1' 'bucket_name2' ... Args: prefix (str, optional): prefix to filter the buckets results. marker (str, optional): paginate separator. max_keys (int, optional): max return number per page. params (Optional[Dict], optional): Some optional params. Returns: oss2.models.ListBucketsResult: \"\"\" list_param = {} list_param [ \"prefix\" ] = prefix list_param [ \"marker\" ] = marker list_param [ \"max-keys\" ] = str ( max_keys ) if params is not None : if \"tag-key\" in params : list_param [ \"tag-key\" ] = params [ \"tag-key\" ] if \"tag-value\" in params : list_param [ \"tag-value\" ] = params [ \"tag-value\" ] resp = await self . _do ( \"GET\" , \"\" , \"\" , params = list_param ) logger . debug ( \"List buckets done\" ) return await self . _parse_result ( resp , parse_list_buckets , ListBucketsResult )","title":"Api"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket","text":"Bases: _AioBase Used for Bucket and Object operations, creating\u3001deleting Bucket, uploading\u3001downloading Object, etc\u3002 use case (bucket in HangZhou area):: import oss2 import aiooss2 import asyncio auth = oss2.Auth('your-access-key-id', 'your-access-key-secret') bucket = aiooss2.Bucket(auth, 'http://oss-cn-hangzhou.aliyuncs.com', 'your-bucket') def upload(): data = b\"\u0001\" * 1024 resp = await bucket.put_object('readme.txt', 'content of the object') return resp loop = asyncio.get_event_loop() loop.run_until_complete(upload()) Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 class AioBucket ( _AioBase ): \"\"\"Used for Bucket and Object operations, creating\u3001deleting Bucket, uploading\u3001downloading Object, etc\u3002 use case (bucket in HangZhou area):: >>> import oss2 >>> import aiooss2 >>> import asyncio >>> auth = oss2.Auth('your-access-key-id', 'your-access-key-secret') >>> bucket = aiooss2.Bucket(auth, 'http://oss-cn-hangzhou.aliyuncs.com', >>> 'your-bucket') >>> def upload(): >>> data = b\"\\x01\" * 1024 >>> resp = await bucket.put_object('readme.txt', >>> 'content of the object') >>> return resp >>> loop = asyncio.get_event_loop() >>> loop.run_until_complete(upload()) <oss2.models.PutObjectResult object at 0x029B9930> \"\"\" auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ] def __init__ ( self , auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ], endpoint : str , bucket_name : str , is_cname : bool = False , ** kwargs , ): \"\"\" Args: bucket_name (str): the bucket name to operate \"\"\" self . bucket_name = bucket_name . strip () if is_valid_bucket_name ( self . bucket_name ) is not True : raise ClientError ( f \"The bucket_name ' { self . bucket_name } ' is invalid, please check it.\" ) super () . __init__ ( auth , endpoint , is_cname , ** kwargs , ) async def _do_object ( self , method : str , key : Union [ bytes , str ], ** kwargs ) -> \"AioResponse\" : return await self . _do ( method , self . bucket_name , key , ** kwargs ) async def _do_bucket ( self , method : str , ** kwargs ) -> \"AioResponse\" : return await self . _do ( method , self . bucket_name , \"\" , ** kwargs ) async def put_object ( self , key : str , data , headers : Optional [ Mapping ] = None , progress_callback : Optional [ Callable ] = None , ) -> \"PutObjectResult\" : \"\"\"upload some contents to an object (use case) :: >>> await bucket.put_object('readme.txt', 'content of readme.txt') >>> with open(u'local_file.txt', 'rb') as f: >>> await bucket.put_object('remote_file.txt', f) Args: key (str): object name to upload data (Union[str, bytes, IO, Iterable]): contents to upload headers (Optional[Mapping], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. Returns: PutObjectResult: \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), key ) data = make_adapter ( data , progress_callback = progress_callback , enable_crc = self . enable_crc , ) resp : \"AioResponse\" = await self . _do_object ( \"PUT\" , key , data = data , headers = headers ) logger . debug ( \"Put object done\" ) result = PutObjectResult ( resp ) if self . enable_crc and result . crc is not None : check_crc ( \"put object\" , data . crc , result . crc , result . request_id ) return result async def get_object ( # pylint: disable=too-many-arguments self , key : str , byte_range : Optional [ Sequence [ Optional [ int ]]] = None , headers : Optional [ dict ] = None , progress_callback : Optional [ Callable ] = None , process = None , params : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> AioGetObjectResult : \"\"\"download the contents of an object (use case) :: >>> async with await bucket.get_object(\"helloword\") as result >>> print(await result.read()) 'hello world' Args: key (str): object name to download. byte_range (Optional[Sequence[Optional[int]]], optional): Range to download. headers (Optional[dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. process (_type_, optional): oss file process method. params (Optional[Union[Dict, CaseInsensitiveDict]], optional): Returns: AioGetObjectResult: \"\"\" headers_dict : CaseInsensitiveDict = CaseInsensitiveDict ( headers ) range_string = _make_range_string ( byte_range ) if range_string : headers_dict [ \"range\" ] = range_string params = {} if params is None else params if process : params [ Bucket . PROCESS ] = process resp = await self . _do_object ( \"GET\" , key , headers = headers_dict , params = params ) logger . debug ( \"Get object done\" ) return AioGetObjectResult ( resp , progress_callback , self . enable_crc ) async def delete_object ( self , key : str , params : Union [ Dict , CaseInsensitiveDict ] = None , headers : Optional [ Dict ] = None , ) -> \"RequestResult\" : \"\"\"delete an object (use case) :: >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object1' 'object2' >>> await bucket.delete_object(\"object1\") >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object2' Args: key (str): _description_ headers (Optional[Dict], optional): HTTP headers to specify. params (Union[Dict, CaseInsensitiveDict], optional): Returns: RequestResult: \"\"\" resp = await self . _do_object ( \"DELETE\" , key , params = params , headers = headers ) logger . debug ( \"Delete object done\" ) return RequestResult ( resp ) async def list_objects ( # pylint: disable=too-many-arguments self , prefix : str = \"\" , delimiter : str = \"\" , marker : str = \"\" , max_keys : int = 100 , headers : Optional [ Dict ] = None , ) -> \"ListObjectsResult\" : \"\"\"list objects in a bucket (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Args: prefix (str, optional): only list objects start with this prefix. delimiter (str, optional): delimiter as a folder separator. marker (str, optional): use in paginate. max_keys (int, optional): numbers of objects for one page. headers (Optional[Dict], optional): HTTP headers to specify. Returns: ListObjectsResult: \"\"\" headers = CaseInsensitiveDict ( headers ) resp = await self . _do_object ( \"GET\" , \"\" , params = { \"prefix\" : prefix , \"delimiter\" : delimiter , \"marker\" : marker , \"max-keys\" : str ( max_keys ), \"encoding-type\" : \"url\" , }, headers = headers , ) logger . debug ( \"List objects done\" ) return await self . _parse_result ( resp , parse_list_objects , ListObjectsResult ) async def get_object_meta ( self , key : str , params : Optional [ Union [ dict , CaseInsensitiveDict ]] = None , headers : Optional [ Dict ] = None , ) -> \"GetObjectMetaResult\" : \"\"\"get meta data from object (use case) :: >>> result = await bucket.get_object_meta(object_name) >>> print(result.content_length ) 256 Args: key (str): object key params (Optional[Union[dict, CaseInsensitiveDict]], optional): headers (Optional[Dict], optional): HTTP headers to specify. Returns: GetObjectMetaResult: \"\"\" headers = CaseInsensitiveDict ( headers ) if params is None : params = {} if Bucket . OBJECTMETA not in params : params [ Bucket . OBJECTMETA ] = \"\" resp = await self . _do_object ( \"GET\" , key , params = params , headers = headers ) logger . debug ( \"Get object metadata done\" ) return GetObjectMetaResult ( resp ) async def object_exists ( self , key : str , headers : Optional [ Dict ] = None ) -> bool : \"\"\"Return True if key exists, False otherwise. raise Exception for other exceptions. (use case) :: >>> result = await bucket.object_exists('object1') >>> print(result) True Args: key (str): key of the object headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. Returns: bool: \"\"\" try : await self . get_object_meta ( key , headers = headers ) except NoSuchKey : return False return True async def get_bucket_info ( self ) -> GetBucketInfoResult : \"\"\"Get bucket information, `Create time`, `Endpoint`, `Owner`, `ACL` (use case) :: >>> resp = await aiobucket.get_bucket_info() >>> print(resp.name) 'bucket_name' Returns: GetBucketInfoResult \"\"\" resp = await self . _do_bucket ( \"GET\" , params = { Bucket . BUCKET_INFO : \"\" }) logger . debug ( \"Get bucket info done\" ) return await self . _parse_result ( resp , parse_get_bucket_info , GetBucketInfoResult ) async def append_object ( self , key : str , position : int , data , headers : Optional [ Dict ] = None , progress_callback : Optional [ Callable ] = None , init_crc : Optional [ int ] = None , ) -> \"AppendObjectResult\" : \"\"\"Append value to an object (use case) :: >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' >>> resp = await bucket.append_object('object1', 11, \"!!!\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world!!!' Args: key (str): key of the object position (int): position to append data (_type_): data to append headers (Optional[Dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. init_crc (Optional[int], optional): init value of the crc Returns: AppendObjectResult: \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), key ) data = make_adapter ( data , progress_callback = progress_callback , enable_crc = self . enable_crc , ) resp = await self . _do_object ( \"POST\" , key , data = data , headers = headers , params = { \"append\" : \"\" , \"position\" : str ( position )}, ) logger . debug ( \"Append object done\" ) result = AppendObjectResult ( resp ) if self . enable_crc and result . crc is not None and init_crc is not None : check_crc ( \"append object\" , data . crc , result . crc , result . request_id ) return result async def put_object_from_file ( self , key : str , filename : str , headers : Optional [ Mapping ] = None , progress_callback : Optional [ Callable ] = None , ) -> \"PutObjectResult\" : \"\"\"Upload a local file to a oss key (use case) :: >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" >>> result = await aiobucket.put_object_from_file(\"object1\", \"file\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' Args: key (str): key of the oss filename (str): filename to upload headers (Optional[Mapping], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. Returns: _type_: _description_ \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), filename ) with open ( to_unicode ( filename ), \"rb\" ) as f_stream : return await self . put_object ( key , f_stream , headers = headers , progress_callback = progress_callback , ) async def get_object_to_file ( self , key : str , filename : str , byte_range : Optional [ Sequence [ int ]] = None , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , progress_callback : Optional [ Callable ] = None , process : Optional [ Callable ] = None , params : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> AioGetObjectResult : \"\"\"Download contents of object to file. (use case) :: >>> result = await aiobucket.get_object_to_file(\"object1\", \"file\") >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" Args: key (str): object name to download. filename (str): filename to save the data downloaded. byte_range (Optional[Sequence[Optional[int]]], optional): Range to download. headers (Optional[dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. process (_type_, optional): oss file process method. params (Optional[Union[Dict, CaseInsensitiveDict]], optional): Returns: AioGetObjectResult: \"\"\" with open ( to_unicode ( filename ), \"wb\" ) as f_w : result = await self . get_object ( key , byte_range = byte_range , headers = headers , progress_callback = progress_callback , process = process , params = params , ) if result . content_length is None : copyfileobj ( result , f_w ) else : await copyfileobj_and_verify ( result , f_w , result . content_length , request_id = result . request_id , ) if self . enable_crc and byte_range is None : if ( ( headers is None ) or ( \"Accept-Encoding\" not in headers ) or ( headers [ \"Accept-Encoding\" ] != \"gzip\" ) ): check_crc ( \"get\" , result . client_crc , result . server_crc , result . request_id , ) return result async def batch_delete_objects ( self , key_list : List [ str ], headers : Optional [ Dict ] = None ) -> BatchDeleteObjectsResult : \"\"\"Delete a batch of objects (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' 'object3' 'object4' 'object5' 'object6' >>> await aiobucket.batch_delete_objects([\"object1\", \"object2\", \"object3\"]) >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object4' 'object5' 'object6' Args: key_list (List[str]): list of objects to delete. headers (Optional[Dict], optional): HTTP headers to specify. Raises: ClientError: Returns: BatchDeleteObjectResult: \"\"\" if not key_list : raise ClientError ( \"key_list should not be empty\" ) data = to_batch_delete_objects_request ( key_list , False ) header_dict : CaseInsensitiveDict = CaseInsensitiveDict ( headers ) header_dict [ \"Content-MD5\" ] = content_md5 ( data ) resp = await self . _do_object ( \"POST\" , \"\" , data = data , params = { \"delete\" : \"\" , \"encoding-type\" : \"url\" }, headers = header_dict , ) return await self . _parse_result ( resp , parse_batch_delete_objects , BatchDeleteObjectsResult ) async def copy_object ( self , source_bucket_name : str , source_key : str , target_key : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Dict ] = None , ) -> PutObjectResult : \"\"\"copy object to another place (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' >>> await aiobucket.copy_object(bucket_name, \"object1\", \"object2\") >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Args: source_bucket_name (str): source object bucket source_key (str): source object key target_key (str): target object key headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Dict], optional): Returns: PutObjectResult: \"\"\" headers = CaseInsensitiveDict ( headers ) if params and Bucket . VERSIONID in params : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) + \"?versionId=\" + params [ Bucket . VERSIONID ] ) else : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) ) resp = await self . _do_object ( \"PUT\" , target_key , headers = headers ) return PutObjectResult ( resp ) async def head_object ( self , key : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Mapping ] = None , ) -> \"HeadObjectResult\" : \"\"\"Get object metadata >>> result = bucket.head_object('readme.txt') >>> print(result.content_type) text/plain Args: key (str): object key headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Mapping], optional): Returns: HeadObjectResult: Raises: `NotFound <oss2.exceptions.NotFound>` if object does not exist. \"\"\" logger . debug ( \"Start to head object, bucket: %s , key: %s , headers: %s \" , self . bucket_name , to_string ( key ), headers , ) resp = await self . _do_object ( \"HEAD\" , key , headers = headers , params = params ) logger . debug ( \"Head object done, req_id: %s , status_code: %s \" , resp . request_id , resp . status , ) return await self . _parse_result ( resp , parse_dummy_result , HeadObjectResult ) async def abort_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> RequestResult : \"\"\"abort multipart uploading process\"\"\" return await abort_multipart_upload ( self , * args , ** kwargs ) async def complete_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"Complete multipart uploading process create a new file.\"\"\" return await complete_multipart_upload ( self , * args , ** kwargs ) async def init_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> InitMultipartUploadResult : \"\"\"initialize multipart uploading The returning upload id, bucket name and object name together defines this uploading event.\"\"\" return await init_multipart_upload ( self , * args , ** kwargs ) async def list_multipart_uploads ( self : \"AioBucket\" , * args , ** kwargs ) -> ListMultipartUploadsResult : \"\"\"List multipart uploading process\"\"\" return await list_multipart_uploads ( self , * args , ** kwargs ) async def list_parts ( self : \"AioBucket\" , * args , ** kwargs ) -> ListPartsResult : \"\"\"list uploaded parts in a part uploading progress.\"\"\" return await list_parts ( self , * args , ** kwargs ) async def upload_part ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"upload single part.\"\"\" return await upload_part ( self , * args , ** kwargs ) async def upload_part_copy ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"copy part or whole of a source file to a slice of a target file.\"\"\" return await upload_part_copy ( self , * args , ** kwargs )","title":"AioBucket"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.__init__","text":"Parameters: Name Type Description Default bucket_name str the bucket name to operate required Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 def __init__ ( self , auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ], endpoint : str , bucket_name : str , is_cname : bool = False , ** kwargs , ): \"\"\" Args: bucket_name (str): the bucket name to operate \"\"\" self . bucket_name = bucket_name . strip () if is_valid_bucket_name ( self . bucket_name ) is not True : raise ClientError ( f \"The bucket_name ' { self . bucket_name } ' is invalid, please check it.\" ) super () . __init__ ( auth , endpoint , is_cname , ** kwargs , )","title":"__init__()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.abort_multipart_upload","text":"abort multipart uploading process Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 799 800 801 802 803 async def abort_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> RequestResult : \"\"\"abort multipart uploading process\"\"\" return await abort_multipart_upload ( self , * args , ** kwargs )","title":"abort_multipart_upload()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.append_object","text":"Append value to an object (use case) :: >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' >>> resp = await bucket.append_object('object1', 11, \"!!!\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world!!!' Parameters: Name Type Description Default key str key of the object required position int position to append required data _type_ data to append required headers Optional [ Dict ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None init_crc Optional [ int ] init value of the crc None Returns: Name Type Description AppendObjectResult AppendObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 async def append_object ( self , key : str , position : int , data , headers : Optional [ Dict ] = None , progress_callback : Optional [ Callable ] = None , init_crc : Optional [ int ] = None , ) -> \"AppendObjectResult\" : \"\"\"Append value to an object (use case) :: >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' >>> resp = await bucket.append_object('object1', 11, \"!!!\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world!!!' Args: key (str): key of the object position (int): position to append data (_type_): data to append headers (Optional[Dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. init_crc (Optional[int], optional): init value of the crc Returns: AppendObjectResult: \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), key ) data = make_adapter ( data , progress_callback = progress_callback , enable_crc = self . enable_crc , ) resp = await self . _do_object ( \"POST\" , key , data = data , headers = headers , params = { \"append\" : \"\" , \"position\" : str ( position )}, ) logger . debug ( \"Append object done\" ) result = AppendObjectResult ( resp ) if self . enable_crc and result . crc is not None and init_crc is not None : check_crc ( \"append object\" , data . crc , result . crc , result . request_id ) return result","title":"append_object()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.batch_delete_objects","text":"Delete a batch of objects (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' 'object3' 'object4' 'object5' 'object6' >>> await aiobucket.batch_delete_objects([\"object1\", \"object2\", \"object3\"]) >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object4' 'object5' 'object6' Parameters: Name Type Description Default key_list List [ str ] list of objects to delete. required headers Optional [ Dict ] HTTP headers to specify. None Raises: Type Description ClientError Returns: Name Type Description BatchDeleteObjectResult BatchDeleteObjectsResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 async def batch_delete_objects ( self , key_list : List [ str ], headers : Optional [ Dict ] = None ) -> BatchDeleteObjectsResult : \"\"\"Delete a batch of objects (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' 'object3' 'object4' 'object5' 'object6' >>> await aiobucket.batch_delete_objects([\"object1\", \"object2\", \"object3\"]) >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object4' 'object5' 'object6' Args: key_list (List[str]): list of objects to delete. headers (Optional[Dict], optional): HTTP headers to specify. Raises: ClientError: Returns: BatchDeleteObjectResult: \"\"\" if not key_list : raise ClientError ( \"key_list should not be empty\" ) data = to_batch_delete_objects_request ( key_list , False ) header_dict : CaseInsensitiveDict = CaseInsensitiveDict ( headers ) header_dict [ \"Content-MD5\" ] = content_md5 ( data ) resp = await self . _do_object ( \"POST\" , \"\" , data = data , params = { \"delete\" : \"\" , \"encoding-type\" : \"url\" }, headers = header_dict , ) return await self . _parse_result ( resp , parse_batch_delete_objects , BatchDeleteObjectsResult )","title":"batch_delete_objects()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.complete_multipart_upload","text":"Complete multipart uploading process create a new file. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 805 806 807 808 809 async def complete_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"Complete multipart uploading process create a new file.\"\"\" return await complete_multipart_upload ( self , * args , ** kwargs )","title":"complete_multipart_upload()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.copy_object","text":"copy object to another place (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' >>> await aiobucket.copy_object(bucket_name, \"object1\", \"object2\") >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Parameters: Name Type Description Default source_bucket_name str source object bucket required source_key str source object key required target_key str target object key required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None params Optional [ Dict ] None Returns: Name Type Description PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 async def copy_object ( self , source_bucket_name : str , source_key : str , target_key : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Dict ] = None , ) -> PutObjectResult : \"\"\"copy object to another place (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' >>> await aiobucket.copy_object(bucket_name, \"object1\", \"object2\") >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Args: source_bucket_name (str): source object bucket source_key (str): source object key target_key (str): target object key headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Dict], optional): Returns: PutObjectResult: \"\"\" headers = CaseInsensitiveDict ( headers ) if params and Bucket . VERSIONID in params : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) + \"?versionId=\" + params [ Bucket . VERSIONID ] ) else : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) ) resp = await self . _do_object ( \"PUT\" , target_key , headers = headers ) return PutObjectResult ( resp )","title":"copy_object()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.delete_object","text":"delete an object (use case) :: >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object1' 'object2' >>> await bucket.delete_object(\"object1\") >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object2' Parameters: Name Type Description Default key str description required headers Optional [ Dict ] HTTP headers to specify. None params Union [ Dict , CaseInsensitiveDict ] None Returns: Name Type Description RequestResult RequestResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 async def delete_object ( self , key : str , params : Union [ Dict , CaseInsensitiveDict ] = None , headers : Optional [ Dict ] = None , ) -> \"RequestResult\" : \"\"\"delete an object (use case) :: >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object1' 'object2' >>> await bucket.delete_object(\"object1\") >>> async for obj in AioObjectIterator(bucket): >>> print(obj.key) 'object2' Args: key (str): _description_ headers (Optional[Dict], optional): HTTP headers to specify. params (Union[Dict, CaseInsensitiveDict], optional): Returns: RequestResult: \"\"\" resp = await self . _do_object ( \"DELETE\" , key , params = params , headers = headers ) logger . debug ( \"Delete object done\" ) return RequestResult ( resp )","title":"delete_object()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.get_bucket_info","text":"Get bucket information, Create time , Endpoint , Owner , ACL (use case) :: >>> resp = await aiobucket.get_bucket_info() >>> print(resp.name) 'bucket_name' Returns: Type Description GetBucketInfoResult GetBucketInfoResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 async def get_bucket_info ( self ) -> GetBucketInfoResult : \"\"\"Get bucket information, `Create time`, `Endpoint`, `Owner`, `ACL` (use case) :: >>> resp = await aiobucket.get_bucket_info() >>> print(resp.name) 'bucket_name' Returns: GetBucketInfoResult \"\"\" resp = await self . _do_bucket ( \"GET\" , params = { Bucket . BUCKET_INFO : \"\" }) logger . debug ( \"Get bucket info done\" ) return await self . _parse_result ( resp , parse_get_bucket_info , GetBucketInfoResult )","title":"get_bucket_info()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.get_object","text":"download the contents of an object (use case) :: >>> async with await bucket.get_object(\"helloword\") as result >>> print(await result.read()) 'hello world' Parameters: Name Type Description Default key str object name to download. required byte_range Optional [ Sequence [ Optional [ int ]]] Range to download. None headers Optional [ dict ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None process _type_ oss file process method. None params Optional [ Union [ Dict , CaseInsensitiveDict ]] None Returns: Name Type Description AioGetObjectResult AioGetObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 async def get_object ( # pylint: disable=too-many-arguments self , key : str , byte_range : Optional [ Sequence [ Optional [ int ]]] = None , headers : Optional [ dict ] = None , progress_callback : Optional [ Callable ] = None , process = None , params : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> AioGetObjectResult : \"\"\"download the contents of an object (use case) :: >>> async with await bucket.get_object(\"helloword\") as result >>> print(await result.read()) 'hello world' Args: key (str): object name to download. byte_range (Optional[Sequence[Optional[int]]], optional): Range to download. headers (Optional[dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. process (_type_, optional): oss file process method. params (Optional[Union[Dict, CaseInsensitiveDict]], optional): Returns: AioGetObjectResult: \"\"\" headers_dict : CaseInsensitiveDict = CaseInsensitiveDict ( headers ) range_string = _make_range_string ( byte_range ) if range_string : headers_dict [ \"range\" ] = range_string params = {} if params is None else params if process : params [ Bucket . PROCESS ] = process resp = await self . _do_object ( \"GET\" , key , headers = headers_dict , params = params ) logger . debug ( \"Get object done\" ) return AioGetObjectResult ( resp , progress_callback , self . enable_crc )","title":"get_object()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.get_object_meta","text":"get meta data from object (use case) :: >>> result = await bucket.get_object_meta(object_name) >>> print(result.content_length ) 256 Parameters: Name Type Description Default key str object key required params Optional [ Union [ dict , CaseInsensitiveDict ]] None headers Optional [ Dict ] HTTP headers to specify. None Returns: Name Type Description GetObjectMetaResult GetObjectMetaResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 async def get_object_meta ( self , key : str , params : Optional [ Union [ dict , CaseInsensitiveDict ]] = None , headers : Optional [ Dict ] = None , ) -> \"GetObjectMetaResult\" : \"\"\"get meta data from object (use case) :: >>> result = await bucket.get_object_meta(object_name) >>> print(result.content_length ) 256 Args: key (str): object key params (Optional[Union[dict, CaseInsensitiveDict]], optional): headers (Optional[Dict], optional): HTTP headers to specify. Returns: GetObjectMetaResult: \"\"\" headers = CaseInsensitiveDict ( headers ) if params is None : params = {} if Bucket . OBJECTMETA not in params : params [ Bucket . OBJECTMETA ] = \"\" resp = await self . _do_object ( \"GET\" , key , params = params , headers = headers ) logger . debug ( \"Get object metadata done\" ) return GetObjectMetaResult ( resp )","title":"get_object_meta()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.get_object_to_file","text":"Download contents of object to file. (use case) :: >>> result = await aiobucket.get_object_to_file(\"object1\", \"file\") >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" Parameters: Name Type Description Default key str object name to download. required filename str filename to save the data downloaded. required byte_range Optional [ Sequence [ Optional [ int ]]] Range to download. None headers Optional [ dict ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None process _type_ oss file process method. None params Optional [ Union [ Dict , CaseInsensitiveDict ]] None Returns: Name Type Description AioGetObjectResult AioGetObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 async def get_object_to_file ( self , key : str , filename : str , byte_range : Optional [ Sequence [ int ]] = None , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , progress_callback : Optional [ Callable ] = None , process : Optional [ Callable ] = None , params : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> AioGetObjectResult : \"\"\"Download contents of object to file. (use case) :: >>> result = await aiobucket.get_object_to_file(\"object1\", \"file\") >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" Args: key (str): object name to download. filename (str): filename to save the data downloaded. byte_range (Optional[Sequence[Optional[int]]], optional): Range to download. headers (Optional[dict], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. process (_type_, optional): oss file process method. params (Optional[Union[Dict, CaseInsensitiveDict]], optional): Returns: AioGetObjectResult: \"\"\" with open ( to_unicode ( filename ), \"wb\" ) as f_w : result = await self . get_object ( key , byte_range = byte_range , headers = headers , progress_callback = progress_callback , process = process , params = params , ) if result . content_length is None : copyfileobj ( result , f_w ) else : await copyfileobj_and_verify ( result , f_w , result . content_length , request_id = result . request_id , ) if self . enable_crc and byte_range is None : if ( ( headers is None ) or ( \"Accept-Encoding\" not in headers ) or ( headers [ \"Accept-Encoding\" ] != \"gzip\" ) ): check_crc ( \"get\" , result . client_crc , result . server_crc , result . request_id , ) return result","title":"get_object_to_file()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.head_object","text":"Get object metadata >>> result = bucket.head_object('readme.txt') >>> print(result.content_type) text/plain Parameters: Name Type Description Default key str object key required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None params Optional [ Mapping ] None Returns: Name Type Description HeadObjectResult HeadObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 async def head_object ( self , key : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Mapping ] = None , ) -> \"HeadObjectResult\" : \"\"\"Get object metadata >>> result = bucket.head_object('readme.txt') >>> print(result.content_type) text/plain Args: key (str): object key headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Mapping], optional): Returns: HeadObjectResult: Raises: `NotFound <oss2.exceptions.NotFound>` if object does not exist. \"\"\" logger . debug ( \"Start to head object, bucket: %s , key: %s , headers: %s \" , self . bucket_name , to_string ( key ), headers , ) resp = await self . _do_object ( \"HEAD\" , key , headers = headers , params = params ) logger . debug ( \"Head object done, req_id: %s , status_code: %s \" , resp . request_id , resp . status , ) return await self . _parse_result ( resp , parse_dummy_result , HeadObjectResult )","title":"head_object()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.init_multipart_upload","text":"initialize multipart uploading The returning upload id, bucket name and object name together defines this uploading event. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 811 812 813 814 815 816 817 async def init_multipart_upload ( self : \"AioBucket\" , * args , ** kwargs ) -> InitMultipartUploadResult : \"\"\"initialize multipart uploading The returning upload id, bucket name and object name together defines this uploading event.\"\"\" return await init_multipart_upload ( self , * args , ** kwargs )","title":"init_multipart_upload()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.list_multipart_uploads","text":"List multipart uploading process Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 819 820 821 822 823 async def list_multipart_uploads ( self : \"AioBucket\" , * args , ** kwargs ) -> ListMultipartUploadsResult : \"\"\"List multipart uploading process\"\"\" return await list_multipart_uploads ( self , * args , ** kwargs )","title":"list_multipart_uploads()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.list_objects","text":"list objects in a bucket (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Parameters: Name Type Description Default prefix str only list objects start with this prefix. '' delimiter str delimiter as a folder separator. '' marker str use in paginate. '' max_keys int numbers of objects for one page. 100 headers Optional [ Dict ] HTTP headers to specify. None Returns: Name Type Description ListObjectsResult ListObjectsResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 async def list_objects ( # pylint: disable=too-many-arguments self , prefix : str = \"\" , delimiter : str = \"\" , marker : str = \"\" , max_keys : int = 100 , headers : Optional [ Dict ] = None , ) -> \"ListObjectsResult\" : \"\"\"list objects in a bucket (use case) :: >>> async for obj in AioObjectIterator(bucket, prefix=object_path): >>> print(obj.key) 'object1' 'object2' Args: prefix (str, optional): only list objects start with this prefix. delimiter (str, optional): delimiter as a folder separator. marker (str, optional): use in paginate. max_keys (int, optional): numbers of objects for one page. headers (Optional[Dict], optional): HTTP headers to specify. Returns: ListObjectsResult: \"\"\" headers = CaseInsensitiveDict ( headers ) resp = await self . _do_object ( \"GET\" , \"\" , params = { \"prefix\" : prefix , \"delimiter\" : delimiter , \"marker\" : marker , \"max-keys\" : str ( max_keys ), \"encoding-type\" : \"url\" , }, headers = headers , ) logger . debug ( \"List objects done\" ) return await self . _parse_result ( resp , parse_list_objects , ListObjectsResult )","title":"list_objects()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.list_parts","text":"list uploaded parts in a part uploading progress. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 825 826 827 async def list_parts ( self : \"AioBucket\" , * args , ** kwargs ) -> ListPartsResult : \"\"\"list uploaded parts in a part uploading progress.\"\"\" return await list_parts ( self , * args , ** kwargs )","title":"list_parts()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.object_exists","text":"Return True if key exists, False otherwise. raise Exception for other exceptions. (use case) :: >>> result = await bucket.object_exists('object1') >>> print(result) True Parameters: Name Type Description Default key str key of the object required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] None Returns: Name Type Description bool bool Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 async def object_exists ( self , key : str , headers : Optional [ Dict ] = None ) -> bool : \"\"\"Return True if key exists, False otherwise. raise Exception for other exceptions. (use case) :: >>> result = await bucket.object_exists('object1') >>> print(result) True Args: key (str): key of the object headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. Returns: bool: \"\"\" try : await self . get_object_meta ( key , headers = headers ) except NoSuchKey : return False return True","title":"object_exists()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.put_object","text":"upload some contents to an object (use case) :: >>> await bucket.put_object('readme.txt', 'content of readme.txt') >>> with open(u'local_file.txt', 'rb') as f: >>> await bucket.put_object('remote_file.txt', f) Parameters: Name Type Description Default key str object name to upload required data Union [ str , bytes , IO , Iterable ] contents to upload required headers Optional [ Mapping ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None Returns: Name Type Description PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 async def put_object ( self , key : str , data , headers : Optional [ Mapping ] = None , progress_callback : Optional [ Callable ] = None , ) -> \"PutObjectResult\" : \"\"\"upload some contents to an object (use case) :: >>> await bucket.put_object('readme.txt', 'content of readme.txt') >>> with open(u'local_file.txt', 'rb') as f: >>> await bucket.put_object('remote_file.txt', f) Args: key (str): object name to upload data (Union[str, bytes, IO, Iterable]): contents to upload headers (Optional[Mapping], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. Returns: PutObjectResult: \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), key ) data = make_adapter ( data , progress_callback = progress_callback , enable_crc = self . enable_crc , ) resp : \"AioResponse\" = await self . _do_object ( \"PUT\" , key , data = data , headers = headers ) logger . debug ( \"Put object done\" ) result = PutObjectResult ( resp ) if self . enable_crc and result . crc is not None : check_crc ( \"put object\" , data . crc , result . crc , result . request_id ) return result","title":"put_object()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.put_object_from_file","text":"Upload a local file to a oss key (use case) :: >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" >>> result = await aiobucket.put_object_from_file(\"object1\", \"file\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' Parameters: Name Type Description Default key str key of the oss required filename str filename to upload required headers Optional [ Mapping ] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None Returns: Name Type Description _type_ PutObjectResult description Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 async def put_object_from_file ( self , key : str , filename : str , headers : Optional [ Mapping ] = None , progress_callback : Optional [ Callable ] = None , ) -> \"PutObjectResult\" : \"\"\"Upload a local file to a oss key (use case) :: >>> with open(\"file\") as f: >>> print(f.read()) \"hello world\" >>> result = await aiobucket.put_object_from_file(\"object1\", \"file\") >>> async with await bucket.get_object(\"object1\") as result >>> print(await result.read()) 'hello world' Args: key (str): key of the oss filename (str): filename to upload headers (Optional[Mapping], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. Returns: _type_: _description_ \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), filename ) with open ( to_unicode ( filename ), \"rb\" ) as f_stream : return await self . put_object ( key , f_stream , headers = headers , progress_callback = progress_callback , )","title":"put_object_from_file()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.upload_part","text":"upload single part. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 829 830 831 async def upload_part ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"upload single part.\"\"\" return await upload_part ( self , * args , ** kwargs )","title":"upload_part()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioBucket.upload_part_copy","text":"copy part or whole of a source file to a slice of a target file. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 833 834 835 async def upload_part_copy ( self : \"AioBucket\" , * args , ** kwargs ) -> PutObjectResult : \"\"\"copy part or whole of a source file to a slice of a target file.\"\"\" return await upload_part_copy ( self , * args , ** kwargs )","title":"upload_part_copy()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioService","text":"Bases: _AioBase Service class used for operations like list all bucket Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 class AioService ( _AioBase ): \"\"\"Service class used for operations like list all bucket\"\"\" def __init__ ( self , auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ], endpoint : str , session : Optional [ AioSession ] = None , connect_timeout : Optional [ int ] = None , app_name : str = \"\" , proxies = None , ): \"\"\"_summary_ Args: auth (Union[Auth, AnonymousAuth, StsAuth]): Auth class. endpoint (str): endpoint address or CNAME. session (Optional[AioSession], optional): reuse a custom session. connect_timeout (int): connection. app_name (str, optional): app name. proxies (_type_, optional): proxies settings. \"\"\" super () . __init__ ( auth , endpoint , False , session , connect_timeout , app_name = app_name , proxies = proxies , ) async def list_buckets ( self , prefix : str = \"\" , marker : str = \"\" , max_keys : int = 100 , params : Optional [ Dict ] = None , ) -> ListBucketsResult : \"\"\"List buckets with given prefix of an user (use case) :: >>> async for obj in AioBucketIterator(aioservice): >>> print(obj.name) 'bucket_name1' 'bucket_name2' ... Args: prefix (str, optional): prefix to filter the buckets results. marker (str, optional): paginate separator. max_keys (int, optional): max return number per page. params (Optional[Dict], optional): Some optional params. Returns: oss2.models.ListBucketsResult: \"\"\" list_param = {} list_param [ \"prefix\" ] = prefix list_param [ \"marker\" ] = marker list_param [ \"max-keys\" ] = str ( max_keys ) if params is not None : if \"tag-key\" in params : list_param [ \"tag-key\" ] = params [ \"tag-key\" ] if \"tag-value\" in params : list_param [ \"tag-value\" ] = params [ \"tag-value\" ] resp = await self . _do ( \"GET\" , \"\" , \"\" , params = list_param ) logger . debug ( \"List buckets done\" ) return await self . _parse_result ( resp , parse_list_buckets , ListBucketsResult )","title":"AioService"},{"location":"reference/aiooss2/api/#aiooss2.api.AioService.__init__","text":"summary Parameters: Name Type Description Default auth Union [ Auth , AnonymousAuth , StsAuth ] Auth class. required endpoint str endpoint address or CNAME. required session Optional [ AioSession ] reuse a custom session. None connect_timeout int connection. None app_name str app name. '' proxies _type_ proxies settings. None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 def __init__ ( self , auth : Union [ \"Auth\" , \"AnonymousAuth\" , \"StsAuth\" ], endpoint : str , session : Optional [ AioSession ] = None , connect_timeout : Optional [ int ] = None , app_name : str = \"\" , proxies = None , ): \"\"\"_summary_ Args: auth (Union[Auth, AnonymousAuth, StsAuth]): Auth class. endpoint (str): endpoint address or CNAME. session (Optional[AioSession], optional): reuse a custom session. connect_timeout (int): connection. app_name (str, optional): app name. proxies (_type_, optional): proxies settings. \"\"\" super () . __init__ ( auth , endpoint , False , session , connect_timeout , app_name = app_name , proxies = proxies , )","title":"__init__()"},{"location":"reference/aiooss2/api/#aiooss2.api.AioService.list_buckets","text":"List buckets with given prefix of an user (use case) :: >>> async for obj in AioBucketIterator(aioservice): >>> print(obj.name) 'bucket_name1' 'bucket_name2' ... Parameters: Name Type Description Default prefix str prefix to filter the buckets results. '' marker str paginate separator. '' max_keys int max return number per page. 100 params Optional [ Dict ] Some optional params. None Returns: Type Description ListBucketsResult oss2.models.ListBucketsResult: Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/api.py 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 async def list_buckets ( self , prefix : str = \"\" , marker : str = \"\" , max_keys : int = 100 , params : Optional [ Dict ] = None , ) -> ListBucketsResult : \"\"\"List buckets with given prefix of an user (use case) :: >>> async for obj in AioBucketIterator(aioservice): >>> print(obj.name) 'bucket_name1' 'bucket_name2' ... Args: prefix (str, optional): prefix to filter the buckets results. marker (str, optional): paginate separator. max_keys (int, optional): max return number per page. params (Optional[Dict], optional): Some optional params. Returns: oss2.models.ListBucketsResult: \"\"\" list_param = {} list_param [ \"prefix\" ] = prefix list_param [ \"marker\" ] = marker list_param [ \"max-keys\" ] = str ( max_keys ) if params is not None : if \"tag-key\" in params : list_param [ \"tag-key\" ] = params [ \"tag-key\" ] if \"tag-value\" in params : list_param [ \"tag-value\" ] = params [ \"tag-value\" ] resp = await self . _do ( \"GET\" , \"\" , \"\" , params = list_param ) logger . debug ( \"List buckets done\" ) return await self . _parse_result ( resp , parse_list_buckets , ListBucketsResult )","title":"list_buckets()"},{"location":"reference/aiooss2/exceptions/","text":"Module for exceptions classes used in aiooss2 InvalidEncryptionRequest Bases: _InvalidEncryptionRequest Invalid encryption request error class. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/exceptions.py 10 11 12 13 14 15 16 17 18 19 20 21 22 class InvalidEncryptionRequest ( _InvalidEncryptionRequest ): \"\"\"Invalid encryption request error class.\"\"\" def __init__ ( self , message , details ): super () . __init__ ( self . status , {}, f \"InconsistentError: { message } \" , { \"details\" : details }, ) def __str__ ( self ): return self . _str_with_body () make_exception ( resp ) async read the body and raise a proper exception Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/exceptions.py 25 26 27 28 29 30 31 32 33 34 35 36 37 async def make_exception ( resp ): \"\"\"read the body and raise a proper exception\"\"\" status = resp . status headers = resp . headers body = await resp . read () details = _parse_error_body ( body ) code = details . get ( \"Code\" , \"\" ) try : klass = _OSS_ERROR_TO_EXCEPTION [( status , code )] return klass ( status , headers , body , details ) except KeyError : return ServerError ( status , headers , body , details )","title":"Exceptions"},{"location":"reference/aiooss2/exceptions/#aiooss2.exceptions.InvalidEncryptionRequest","text":"Bases: _InvalidEncryptionRequest Invalid encryption request error class. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/exceptions.py 10 11 12 13 14 15 16 17 18 19 20 21 22 class InvalidEncryptionRequest ( _InvalidEncryptionRequest ): \"\"\"Invalid encryption request error class.\"\"\" def __init__ ( self , message , details ): super () . __init__ ( self . status , {}, f \"InconsistentError: { message } \" , { \"details\" : details }, ) def __str__ ( self ): return self . _str_with_body ()","title":"InvalidEncryptionRequest"},{"location":"reference/aiooss2/exceptions/#aiooss2.exceptions.make_exception","text":"read the body and raise a proper exception Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/exceptions.py 25 26 27 28 29 30 31 32 33 34 35 36 37 async def make_exception ( resp ): \"\"\"read the body and raise a proper exception\"\"\" status = resp . status headers = resp . headers body = await resp . read () details = _parse_error_body ( body ) code = details . get ( \"Code\" , \"\" ) try : klass = _OSS_ERROR_TO_EXCEPTION [( status , code )] return klass ( status , headers , body , details ) except KeyError : return ServerError ( status , headers , body , details )","title":"make_exception()"},{"location":"reference/aiooss2/http/","text":"aiooss2.http AioResponse Async Version of the response wrapper adapting to the aiooss2 api Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class AioResponse : \"\"\"Async Version of the response wrapper adapting to the aiooss2 api \"\"\" response : \"ClientResponse\" def __init__ ( self , response : \"ClientResponse\" ): self . response = response self . status = response . status self . headers = response . headers self . request_id = response . headers . get ( \"x-oss-request-id\" , \"\" ) self . __all_read = False logger . debug ( \"Get response headers, req-id: %s , status: %d , headers: %s \" , self . request_id , self . status , self . headers , ) async def read ( self , amt = None ) -> bytes : \"\"\"read the contents from the response\"\"\" if self . __all_read : return b \"\" if amt : async for chunk in self . response . content . iter_chunked ( amt ): return chunk self . __all_read = True return b \"\" content_list = [] async for chunk in self . response . content . iter_chunked ( _CHUNK_SIZE ): content_list . append ( chunk ) content = b \"\" . join ( content_list ) self . __all_read = True return content async def __aiter__ ( self ): \"\"\"content iterator\"\"\" return await self . response . content . iter_chunked ( _CHUNK_SIZE ) def release ( self ): \"\"\"release the response\"\"\" self . response . release () __aiter__ () async content iterator Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 100 101 102 async def __aiter__ ( self ): \"\"\"content iterator\"\"\" return await self . response . content . iter_chunked ( _CHUNK_SIZE ) read ( amt = None ) async read the contents from the response Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 async def read ( self , amt = None ) -> bytes : \"\"\"read the contents from the response\"\"\" if self . __all_read : return b \"\" if amt : async for chunk in self . response . content . iter_chunked ( amt ): return chunk self . __all_read = True return b \"\" content_list = [] async for chunk in self . response . content . iter_chunked ( _CHUNK_SIZE ): content_list . append ( chunk ) content = b \"\" . join ( content_list ) self . __all_read = True return content release () release the response Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 104 105 106 def release ( self ): \"\"\"release the response\"\"\" self . response . release () AioSession Async session wrapper Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 class AioSession : \"\"\"Async session wrapper\"\"\" def __init__ ( self , psize : Optional [ int ] = None ): \"\"\" Args: psize: limit amount of simultaneously opened connections \"\"\" self . psize = psize or defaults . connection_pool_size self . session : Optional [ ClientSession ] = None self . conn : Optional [ TCPConnector ] = None async def do_request ( self , req : \"Request\" , timeout : Optional [ int ] = None ) -> \"AioResponse\" : \"\"\"Do request Args: req: request info timeout: timeout in seconds Raises: RequestError: Returns: AioResponse: Async Response wrapper \"\"\" logger . debug ( \"Send request, method: %s , url: %s , params: %s , headers: %s , \" \"timeout: %d \" , req . method , req . url , req . params , req . headers , timeout , ) try : assert self . session resp = await self . session . request ( req . method , req . url , data = req . data , params = req . params , headers = req . headers , timeout = timeout , ) return AioResponse ( resp ) except ClientResponseError as err : raise RequestError ( err ) from err async def __aenter__ ( self ): self . conn = TCPConnector ( limit = self . psize , limit_per_host = self . psize ) self . session = ClientSession ( connector = self . conn ) return self async def __aexit__ ( self , * args ): await self . close () async def close ( self ): \"\"\"gracefully close the AioSession class\"\"\" await self . conn . close () await self . session . close () @property def closed ( self ): \"\"\"Is client session closed. A readonly property. \"\"\" if self . session is None : return True return self . session . closed closed property Is client session closed. A readonly property. __init__ ( psize = None ) Parameters: Name Type Description Default psize Optional [ int ] limit amount of simultaneously opened connections None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 112 113 114 115 116 117 118 119 120 def __init__ ( self , psize : Optional [ int ] = None ): \"\"\" Args: psize: limit amount of simultaneously opened connections \"\"\" self . psize = psize or defaults . connection_pool_size self . session : Optional [ ClientSession ] = None self . conn : Optional [ TCPConnector ] = None close () async gracefully close the AioSession class Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 170 171 172 173 async def close ( self ): \"\"\"gracefully close the AioSession class\"\"\" await self . conn . close () await self . session . close () do_request ( req , timeout = None ) async Do request Parameters: Name Type Description Default req Request request info required timeout Optional [ int ] timeout in seconds None Raises: Type Description RequestError Returns: Name Type Description AioResponse AioResponse Async Response wrapper Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 async def do_request ( self , req : \"Request\" , timeout : Optional [ int ] = None ) -> \"AioResponse\" : \"\"\"Do request Args: req: request info timeout: timeout in seconds Raises: RequestError: Returns: AioResponse: Async Response wrapper \"\"\" logger . debug ( \"Send request, method: %s , url: %s , params: %s , headers: %s , \" \"timeout: %d \" , req . method , req . url , req . params , req . headers , timeout , ) try : assert self . session resp = await self . session . request ( req . method , req . url , data = req . data , params = req . params , headers = req . headers , timeout = timeout , ) return AioResponse ( resp ) except ClientResponseError as err : raise RequestError ( err ) from err Request Request class for aiohttp Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class Request : # pylint: disable=too-few-public-methods \"\"\"Request class for aiohttp\"\"\" def __init__ ( # pylint: disable=too-many-arguments self , method , url , data = None , params = None , headers = None , app_name = \"\" , proxies = None , ): self . method = method self . url = url self . data = data self . params = params or {} self . proxies = proxies if not isinstance ( headers , CaseInsensitiveDict ): self . headers = CaseInsensitiveDict ( headers ) else : self . headers = headers if \"Content-Type\" not in self . headers : self . headers [ \"Content-Type\" ] = \"application/octet-stream\" if \"User-Agent\" not in self . headers : if app_name : self . headers [ \"User-Agent\" ] = USER_AGENT + \"/\" + app_name else : self . headers [ \"User-Agent\" ] = USER_AGENT logger . debug ( \"Init request, method: %s , url: %s , params: %s , headers: %s \" , method , url , params , headers , )","title":"Http"},{"location":"reference/aiooss2/http/#aiooss2.http.AioResponse","text":"Async Version of the response wrapper adapting to the aiooss2 api Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class AioResponse : \"\"\"Async Version of the response wrapper adapting to the aiooss2 api \"\"\" response : \"ClientResponse\" def __init__ ( self , response : \"ClientResponse\" ): self . response = response self . status = response . status self . headers = response . headers self . request_id = response . headers . get ( \"x-oss-request-id\" , \"\" ) self . __all_read = False logger . debug ( \"Get response headers, req-id: %s , status: %d , headers: %s \" , self . request_id , self . status , self . headers , ) async def read ( self , amt = None ) -> bytes : \"\"\"read the contents from the response\"\"\" if self . __all_read : return b \"\" if amt : async for chunk in self . response . content . iter_chunked ( amt ): return chunk self . __all_read = True return b \"\" content_list = [] async for chunk in self . response . content . iter_chunked ( _CHUNK_SIZE ): content_list . append ( chunk ) content = b \"\" . join ( content_list ) self . __all_read = True return content async def __aiter__ ( self ): \"\"\"content iterator\"\"\" return await self . response . content . iter_chunked ( _CHUNK_SIZE ) def release ( self ): \"\"\"release the response\"\"\" self . response . release ()","title":"AioResponse"},{"location":"reference/aiooss2/http/#aiooss2.http.AioResponse.__aiter__","text":"content iterator Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 100 101 102 async def __aiter__ ( self ): \"\"\"content iterator\"\"\" return await self . response . content . iter_chunked ( _CHUNK_SIZE )","title":"__aiter__()"},{"location":"reference/aiooss2/http/#aiooss2.http.AioResponse.read","text":"read the contents from the response Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 async def read ( self , amt = None ) -> bytes : \"\"\"read the contents from the response\"\"\" if self . __all_read : return b \"\" if amt : async for chunk in self . response . content . iter_chunked ( amt ): return chunk self . __all_read = True return b \"\" content_list = [] async for chunk in self . response . content . iter_chunked ( _CHUNK_SIZE ): content_list . append ( chunk ) content = b \"\" . join ( content_list ) self . __all_read = True return content","title":"read()"},{"location":"reference/aiooss2/http/#aiooss2.http.AioResponse.release","text":"release the response Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 104 105 106 def release ( self ): \"\"\"release the response\"\"\" self . response . release ()","title":"release()"},{"location":"reference/aiooss2/http/#aiooss2.http.AioSession","text":"Async session wrapper Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 class AioSession : \"\"\"Async session wrapper\"\"\" def __init__ ( self , psize : Optional [ int ] = None ): \"\"\" Args: psize: limit amount of simultaneously opened connections \"\"\" self . psize = psize or defaults . connection_pool_size self . session : Optional [ ClientSession ] = None self . conn : Optional [ TCPConnector ] = None async def do_request ( self , req : \"Request\" , timeout : Optional [ int ] = None ) -> \"AioResponse\" : \"\"\"Do request Args: req: request info timeout: timeout in seconds Raises: RequestError: Returns: AioResponse: Async Response wrapper \"\"\" logger . debug ( \"Send request, method: %s , url: %s , params: %s , headers: %s , \" \"timeout: %d \" , req . method , req . url , req . params , req . headers , timeout , ) try : assert self . session resp = await self . session . request ( req . method , req . url , data = req . data , params = req . params , headers = req . headers , timeout = timeout , ) return AioResponse ( resp ) except ClientResponseError as err : raise RequestError ( err ) from err async def __aenter__ ( self ): self . conn = TCPConnector ( limit = self . psize , limit_per_host = self . psize ) self . session = ClientSession ( connector = self . conn ) return self async def __aexit__ ( self , * args ): await self . close () async def close ( self ): \"\"\"gracefully close the AioSession class\"\"\" await self . conn . close () await self . session . close () @property def closed ( self ): \"\"\"Is client session closed. A readonly property. \"\"\" if self . session is None : return True return self . session . closed","title":"AioSession"},{"location":"reference/aiooss2/http/#aiooss2.http.AioSession.closed","text":"Is client session closed. A readonly property.","title":"closed"},{"location":"reference/aiooss2/http/#aiooss2.http.AioSession.__init__","text":"Parameters: Name Type Description Default psize Optional [ int ] limit amount of simultaneously opened connections None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 112 113 114 115 116 117 118 119 120 def __init__ ( self , psize : Optional [ int ] = None ): \"\"\" Args: psize: limit amount of simultaneously opened connections \"\"\" self . psize = psize or defaults . connection_pool_size self . session : Optional [ ClientSession ] = None self . conn : Optional [ TCPConnector ] = None","title":"__init__()"},{"location":"reference/aiooss2/http/#aiooss2.http.AioSession.close","text":"gracefully close the AioSession class Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 170 171 172 173 async def close ( self ): \"\"\"gracefully close the AioSession class\"\"\" await self . conn . close () await self . session . close ()","title":"close()"},{"location":"reference/aiooss2/http/#aiooss2.http.AioSession.do_request","text":"Do request Parameters: Name Type Description Default req Request request info required timeout Optional [ int ] timeout in seconds None Raises: Type Description RequestError Returns: Name Type Description AioResponse AioResponse Async Response wrapper Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 async def do_request ( self , req : \"Request\" , timeout : Optional [ int ] = None ) -> \"AioResponse\" : \"\"\"Do request Args: req: request info timeout: timeout in seconds Raises: RequestError: Returns: AioResponse: Async Response wrapper \"\"\" logger . debug ( \"Send request, method: %s , url: %s , params: %s , headers: %s , \" \"timeout: %d \" , req . method , req . url , req . params , req . headers , timeout , ) try : assert self . session resp = await self . session . request ( req . method , req . url , data = req . data , params = req . params , headers = req . headers , timeout = timeout , ) return AioResponse ( resp ) except ClientResponseError as err : raise RequestError ( err ) from err","title":"do_request()"},{"location":"reference/aiooss2/http/#aiooss2.http.Request","text":"Request class for aiohttp Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/http.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class Request : # pylint: disable=too-few-public-methods \"\"\"Request class for aiohttp\"\"\" def __init__ ( # pylint: disable=too-many-arguments self , method , url , data = None , params = None , headers = None , app_name = \"\" , proxies = None , ): self . method = method self . url = url self . data = data self . params = params or {} self . proxies = proxies if not isinstance ( headers , CaseInsensitiveDict ): self . headers = CaseInsensitiveDict ( headers ) else : self . headers = headers if \"Content-Type\" not in self . headers : self . headers [ \"Content-Type\" ] = \"application/octet-stream\" if \"User-Agent\" not in self . headers : if app_name : self . headers [ \"User-Agent\" ] = USER_AGENT + \"/\" + app_name else : self . headers [ \"User-Agent\" ] = USER_AGENT logger . debug ( \"Init request, method: %s , url: %s , params: %s , headers: %s \" , method , url , params , headers , )","title":"Request"},{"location":"reference/aiooss2/iterators/","text":"Contains some useful iteraotrs, can be used to iterate buckets\u3001 files or file parts etc. AioBucketIterator Bases: _AioBaseIterator Iterate over buckets of an user Return SimplifiedBucketInfo <oss2.models.SimplifiedBucketInfo> every iteration Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 class AioBucketIterator ( _AioBaseIterator ): \"\"\"Iterate over buckets of an user Return `SimplifiedBucketInfo <oss2.models.SimplifiedBucketInfo>` every iteration \"\"\" def __init__ ( self , service : \"AioService\" , prefix : str = \"\" , marker : str = \"\" , max_keys : int = 100 , max_retries : Optional [ int ] = None , ): \"\"\" Args: service (AioService): Service class of a special user. prefix (str, optional): prefix to filter the buckets results. marker (str, optional): paginate separator. max_keys (int, optional): max return number per page. max_retries (Optional[int], optional): max retry count. \"\"\" super () . __init__ ( marker , max_retries ) self . service = service self . prefix = prefix self . max_keys = max_keys async def _fetch ( self ): result = await self . service . list_buckets ( prefix = self . prefix , marker = self . next_marker , max_keys = self . max_keys ) self . entries : List [ \"SimplifiedBucketInfo\" ] = result . buckets return result . is_truncated , result . next_marker __init__ ( service , prefix = '' , marker = '' , max_keys = 100 , max_retries = None ) Parameters: Name Type Description Default service AioService Service class of a special user. required prefix str prefix to filter the buckets results. '' marker str paginate separator. '' max_keys int max return number per page. 100 max_retries Optional [ int ] max retry count. None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def __init__ ( self , service : \"AioService\" , prefix : str = \"\" , marker : str = \"\" , max_keys : int = 100 , max_retries : Optional [ int ] = None , ): \"\"\" Args: service (AioService): Service class of a special user. prefix (str, optional): prefix to filter the buckets results. marker (str, optional): paginate separator. max_keys (int, optional): max return number per page. max_retries (Optional[int], optional): max retry count. \"\"\" super () . __init__ ( marker , max_retries ) self . service = service self . prefix = prefix self . max_keys = max_keys AioObjectIterator Bases: _AioBaseIterator Iterator to iterate objects from a bucket. Return SimplifiedObjectInfo <oss2.models.SimplifiedObjectInfo> object. if SimplifiedObjectInfo.is_prefix() is true, the object returned is a directory. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class AioObjectIterator ( _AioBaseIterator ): \"\"\"Iterator to iterate objects from a bucket. Return `SimplifiedObjectInfo <oss2.models.SimplifiedObjectInfo>` object. if `SimplifiedObjectInfo.is_prefix()` is true, the object returned is a directory. \"\"\" def __init__ ( self , bucket : \"AioBucket\" , prefix : str = \"\" , delimiter : str = \"\" , marker : str = \"\" , max_keys : int = 100 , max_retries : Optional [ int ] = None , headers : Optional [ Dict ] = None , ): \"\"\"_summary_ Args: bucket (AioBucket): bucket class to iterate prefix (str, optional): prefix to filter the object results delimiter (str, optional): delimiter in object name marker (str, optional): paginate separator max_keys (int, optional): key number returns from `list_objects` every time,to notice that iterator can return more objects than it max_retries (Optional[int], optional): retry number headers (Optional[Dict], optional): HTTP header \"\"\" super () . __init__ ( marker , max_retries ) self . bucket = bucket self . prefix = prefix self . delimiter = delimiter self . max_keys = max_keys self . headers = http . CaseInsensitiveDict ( headers ) async def _fetch ( self ): result : ListObjectsResult = await self . bucket . list_objects ( prefix = self . prefix , delimiter = self . delimiter , marker = self . next_marker , max_keys = self . max_keys , headers = self . headers , ) self . entries : List [ \"SimplifiedObjectInfo\" ] = result . object_list + [ SimplifiedObjectInfo ( prefix , None , None , None , None , None ) for prefix in result . prefix_list ] self . entries . sort ( key = lambda obj : obj . key ) return result . is_truncated , result . next_marker __init__ ( bucket , prefix = '' , delimiter = '' , marker = '' , max_keys = 100 , max_retries = None , headers = None ) summary Parameters: Name Type Description Default bucket AioBucket bucket class to iterate required prefix str prefix to filter the object results '' delimiter str delimiter in object name '' marker str paginate separator '' max_keys int key number returns from list_objects 100 max_retries Optional [ int ] retry number None headers Optional [ Dict ] HTTP header None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def __init__ ( self , bucket : \"AioBucket\" , prefix : str = \"\" , delimiter : str = \"\" , marker : str = \"\" , max_keys : int = 100 , max_retries : Optional [ int ] = None , headers : Optional [ Dict ] = None , ): \"\"\"_summary_ Args: bucket (AioBucket): bucket class to iterate prefix (str, optional): prefix to filter the object results delimiter (str, optional): delimiter in object name marker (str, optional): paginate separator max_keys (int, optional): key number returns from `list_objects` every time,to notice that iterator can return more objects than it max_retries (Optional[int], optional): retry number headers (Optional[Dict], optional): HTTP header \"\"\" super () . __init__ ( marker , max_retries ) self . bucket = bucket self . prefix = prefix self . delimiter = delimiter self . max_keys = max_keys self . headers = http . CaseInsensitiveDict ( headers ) AioPartIterator Bases: _AioBaseIterator Iterator over all parts from a partial upload session Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 class AioPartIterator ( _AioBaseIterator ): \"\"\"Iterator over all parts from a partial upload session\"\"\" def __init__ ( self , bucket : \"AioBucket\" , key : str , upload_id : str , marker : str = \"0\" , max_parts : int = 1000 , max_retries : Optional [ int ] = None , headers : Optional [ Dict ] = None , ): \"\"\" Args: bucket (AioBucket): bucket to operate. key (str): key of the object. upload_id (str): upload id of the partial upload session. marker (str, optional): paginate separator. max_parts (int, optional): key number returns from `list_parts` max_retries (Optional[int], optional): max retry count. headers (Optional[Dict], optional): HTTP header. \"\"\" super () . __init__ ( marker , max_retries ) self . bucket = bucket self . key = key self . upload_id = upload_id self . max_parts = max_parts self . headers = http . CaseInsensitiveDict ( headers ) async def _fetch ( self ): result = await self . bucket . list_parts ( self . key , self . upload_id , marker = self . next_marker , max_parts = self . max_parts , headers = self . headers , ) self . entries : List [ \"PartInfo\" ] = result . parts return result . is_truncated , result . next_marker __init__ ( bucket , key , upload_id , marker = '0' , max_parts = 1000 , max_retries = None , headers = None ) Parameters: Name Type Description Default bucket AioBucket bucket to operate. required key str key of the object. required upload_id str upload id of the partial upload session. required marker str paginate separator. '0' max_parts int key number returns from list_parts 1000 max_retries Optional [ int ] max retry count. None headers Optional [ Dict ] HTTP header. None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def __init__ ( self , bucket : \"AioBucket\" , key : str , upload_id : str , marker : str = \"0\" , max_parts : int = 1000 , max_retries : Optional [ int ] = None , headers : Optional [ Dict ] = None , ): \"\"\" Args: bucket (AioBucket): bucket to operate. key (str): key of the object. upload_id (str): upload id of the partial upload session. marker (str, optional): paginate separator. max_parts (int, optional): key number returns from `list_parts` max_retries (Optional[int], optional): max retry count. headers (Optional[Dict], optional): HTTP header. \"\"\" super () . __init__ ( marker , max_retries ) self . bucket = bucket self . key = key self . upload_id = upload_id self . max_parts = max_parts self . headers = http . CaseInsensitiveDict ( headers )","title":"Iterators"},{"location":"reference/aiooss2/iterators/#aiooss2.iterators.AioBucketIterator","text":"Bases: _AioBaseIterator Iterate over buckets of an user Return SimplifiedBucketInfo <oss2.models.SimplifiedBucketInfo> every iteration Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 class AioBucketIterator ( _AioBaseIterator ): \"\"\"Iterate over buckets of an user Return `SimplifiedBucketInfo <oss2.models.SimplifiedBucketInfo>` every iteration \"\"\" def __init__ ( self , service : \"AioService\" , prefix : str = \"\" , marker : str = \"\" , max_keys : int = 100 , max_retries : Optional [ int ] = None , ): \"\"\" Args: service (AioService): Service class of a special user. prefix (str, optional): prefix to filter the buckets results. marker (str, optional): paginate separator. max_keys (int, optional): max return number per page. max_retries (Optional[int], optional): max retry count. \"\"\" super () . __init__ ( marker , max_retries ) self . service = service self . prefix = prefix self . max_keys = max_keys async def _fetch ( self ): result = await self . service . list_buckets ( prefix = self . prefix , marker = self . next_marker , max_keys = self . max_keys ) self . entries : List [ \"SimplifiedBucketInfo\" ] = result . buckets return result . is_truncated , result . next_marker","title":"AioBucketIterator"},{"location":"reference/aiooss2/iterators/#aiooss2.iterators.AioBucketIterator.__init__","text":"Parameters: Name Type Description Default service AioService Service class of a special user. required prefix str prefix to filter the buckets results. '' marker str paginate separator. '' max_keys int max return number per page. 100 max_retries Optional [ int ] max retry count. None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def __init__ ( self , service : \"AioService\" , prefix : str = \"\" , marker : str = \"\" , max_keys : int = 100 , max_retries : Optional [ int ] = None , ): \"\"\" Args: service (AioService): Service class of a special user. prefix (str, optional): prefix to filter the buckets results. marker (str, optional): paginate separator. max_keys (int, optional): max return number per page. max_retries (Optional[int], optional): max retry count. \"\"\" super () . __init__ ( marker , max_retries ) self . service = service self . prefix = prefix self . max_keys = max_keys","title":"__init__()"},{"location":"reference/aiooss2/iterators/#aiooss2.iterators.AioObjectIterator","text":"Bases: _AioBaseIterator Iterator to iterate objects from a bucket. Return SimplifiedObjectInfo <oss2.models.SimplifiedObjectInfo> object. if SimplifiedObjectInfo.is_prefix() is true, the object returned is a directory. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class AioObjectIterator ( _AioBaseIterator ): \"\"\"Iterator to iterate objects from a bucket. Return `SimplifiedObjectInfo <oss2.models.SimplifiedObjectInfo>` object. if `SimplifiedObjectInfo.is_prefix()` is true, the object returned is a directory. \"\"\" def __init__ ( self , bucket : \"AioBucket\" , prefix : str = \"\" , delimiter : str = \"\" , marker : str = \"\" , max_keys : int = 100 , max_retries : Optional [ int ] = None , headers : Optional [ Dict ] = None , ): \"\"\"_summary_ Args: bucket (AioBucket): bucket class to iterate prefix (str, optional): prefix to filter the object results delimiter (str, optional): delimiter in object name marker (str, optional): paginate separator max_keys (int, optional): key number returns from `list_objects` every time,to notice that iterator can return more objects than it max_retries (Optional[int], optional): retry number headers (Optional[Dict], optional): HTTP header \"\"\" super () . __init__ ( marker , max_retries ) self . bucket = bucket self . prefix = prefix self . delimiter = delimiter self . max_keys = max_keys self . headers = http . CaseInsensitiveDict ( headers ) async def _fetch ( self ): result : ListObjectsResult = await self . bucket . list_objects ( prefix = self . prefix , delimiter = self . delimiter , marker = self . next_marker , max_keys = self . max_keys , headers = self . headers , ) self . entries : List [ \"SimplifiedObjectInfo\" ] = result . object_list + [ SimplifiedObjectInfo ( prefix , None , None , None , None , None ) for prefix in result . prefix_list ] self . entries . sort ( key = lambda obj : obj . key ) return result . is_truncated , result . next_marker","title":"AioObjectIterator"},{"location":"reference/aiooss2/iterators/#aiooss2.iterators.AioObjectIterator.__init__","text":"summary Parameters: Name Type Description Default bucket AioBucket bucket class to iterate required prefix str prefix to filter the object results '' delimiter str delimiter in object name '' marker str paginate separator '' max_keys int key number returns from list_objects 100 max_retries Optional [ int ] retry number None headers Optional [ Dict ] HTTP header None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def __init__ ( self , bucket : \"AioBucket\" , prefix : str = \"\" , delimiter : str = \"\" , marker : str = \"\" , max_keys : int = 100 , max_retries : Optional [ int ] = None , headers : Optional [ Dict ] = None , ): \"\"\"_summary_ Args: bucket (AioBucket): bucket class to iterate prefix (str, optional): prefix to filter the object results delimiter (str, optional): delimiter in object name marker (str, optional): paginate separator max_keys (int, optional): key number returns from `list_objects` every time,to notice that iterator can return more objects than it max_retries (Optional[int], optional): retry number headers (Optional[Dict], optional): HTTP header \"\"\" super () . __init__ ( marker , max_retries ) self . bucket = bucket self . prefix = prefix self . delimiter = delimiter self . max_keys = max_keys self . headers = http . CaseInsensitiveDict ( headers )","title":"__init__()"},{"location":"reference/aiooss2/iterators/#aiooss2.iterators.AioPartIterator","text":"Bases: _AioBaseIterator Iterator over all parts from a partial upload session Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 class AioPartIterator ( _AioBaseIterator ): \"\"\"Iterator over all parts from a partial upload session\"\"\" def __init__ ( self , bucket : \"AioBucket\" , key : str , upload_id : str , marker : str = \"0\" , max_parts : int = 1000 , max_retries : Optional [ int ] = None , headers : Optional [ Dict ] = None , ): \"\"\" Args: bucket (AioBucket): bucket to operate. key (str): key of the object. upload_id (str): upload id of the partial upload session. marker (str, optional): paginate separator. max_parts (int, optional): key number returns from `list_parts` max_retries (Optional[int], optional): max retry count. headers (Optional[Dict], optional): HTTP header. \"\"\" super () . __init__ ( marker , max_retries ) self . bucket = bucket self . key = key self . upload_id = upload_id self . max_parts = max_parts self . headers = http . CaseInsensitiveDict ( headers ) async def _fetch ( self ): result = await self . bucket . list_parts ( self . key , self . upload_id , marker = self . next_marker , max_parts = self . max_parts , headers = self . headers , ) self . entries : List [ \"PartInfo\" ] = result . parts return result . is_truncated , result . next_marker","title":"AioPartIterator"},{"location":"reference/aiooss2/iterators/#aiooss2.iterators.AioPartIterator.__init__","text":"Parameters: Name Type Description Default bucket AioBucket bucket to operate. required key str key of the object. required upload_id str upload id of the partial upload session. required marker str paginate separator. '0' max_parts int key number returns from list_parts 1000 max_retries Optional [ int ] max retry count. None headers Optional [ Dict ] HTTP header. None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/iterators.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def __init__ ( self , bucket : \"AioBucket\" , key : str , upload_id : str , marker : str = \"0\" , max_parts : int = 1000 , max_retries : Optional [ int ] = None , headers : Optional [ Dict ] = None , ): \"\"\" Args: bucket (AioBucket): bucket to operate. key (str): key of the object. upload_id (str): upload id of the partial upload session. marker (str, optional): paginate separator. max_parts (int, optional): key number returns from `list_parts` max_retries (Optional[int], optional): max retry count. headers (Optional[Dict], optional): HTTP header. \"\"\" super () . __init__ ( marker , max_retries ) self . bucket = bucket self . key = key self . upload_id = upload_id self . max_parts = max_parts self . headers = http . CaseInsensitiveDict ( headers )","title":"__init__()"},{"location":"reference/aiooss2/models/","text":"Module for all input and output classes for the Python SDK API AioGetObjectResult Bases: HeadObjectResult class for the result of api get_object Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/models.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class AioGetObjectResult ( HeadObjectResult ): \"\"\"class for the result of api get_object\"\"\" resp : \"AioResponse\" def __init__ ( # pylint: disable=too-many-arguments self , resp : \"AioResponse\" , progress_callback = None , crc_enabled = False , crypto_provider = None , discard = 0 , ): super () . __init__ ( resp ) self . __crypto_provider = crypto_provider self . __crc_enabled = crc_enabled self . content_range = _hget ( resp . headers , \"Content-Range\" ) if self . content_range : byte_range = self . _parse_range_str ( self . content_range ) self . stream = make_adapter ( self . resp , progress_callback = progress_callback , size = self . content_length , enable_crc = crc_enabled , discard = discard , ) if self . __crypto_provider : self . _make_crypto ( byte_range , discard ) else : if ( OSS_CLIENT_SIDE_ENCRYPTION_KEY in resp . headers or DEPRECATED_CLIENT_SIDE_ENCRYPTION_KEY in resp . headers ): logger . warning ( \"Using Bucket to get an encrypted object will return \" \"raw data, please confirm if you really want to do this\" ) def _make_crypto ( self , byte_range , discard ): content_crypto_material = ContentCryptoMaterial ( self . __crypto_provider . cipher , self . __crypto_provider . wrap_alg ) content_crypto_material . from_object_meta ( self . resp . headers ) if content_crypto_material . is_unencrypted (): logger . info ( \"The object is not encrypted, use crypto provider is not recommended\" ) else : crypto_provider = self . __crypto_provider if content_crypto_material . mat_desc != self . __crypto_provider . mat_desc : logger . warning ( \"The material description of the object \" \"and the provider is inconsistent\" ) encryption_materials = self . __crypto_provider . get_encryption_materials ( content_crypto_material . mat_desc ) if encryption_materials : crypto_provider = self . __crypto_provider . reset_encryption_materials ( encryption_materials ) else : raise ClientError ( \"There is no encryption materials \" \"match the material description of the object\" ) plain_key = crypto_provider . decrypt_encrypted_key ( content_crypto_material . encrypted_key ) if content_crypto_material . deprecated : if content_crypto_material . wrap_alg == KMS_ALI_WRAP_ALGORITHM : plain_counter = int ( crypto_provider . decrypt_encrypted_iv ( content_crypto_material . encrypted_iv , True ) ) else : plain_counter = int ( crypto_provider . decrypt_encrypted_iv ( content_crypto_material . encrypted_iv ) ) else : plain_iv = crypto_provider . decrypt_encrypted_iv ( content_crypto_material . encrypted_iv ) offset = 0 if self . content_range : start , _ = crypto_provider . adjust_range ( byte_range [ 0 ], byte_range [ 1 ]) offset = content_crypto_material . cipher . calc_offset ( start ) cipher = copy . copy ( content_crypto_material . cipher ) if content_crypto_material . deprecated : cipher . initial_by_counter ( plain_key , plain_counter + offset ) else : cipher . initialize ( plain_key , plain_iv , offset ) self . stream = crypto_provider . make_decrypt_adapter ( self . stream , cipher , discard ) async def __aenter__ ( self ): return self async def __aexit__ ( self , * args ): self . resp . release () async def __aiter__ ( self ): async for data in self . stream : return data @staticmethod def _parse_range_str ( content_range ): # :param str content_range: sample 'bytes 0-128/1024' range_data = ( content_range . split ( \" \" , 2 )[ 1 ]) . split ( \"/\" , 2 )[ 0 ] range_start , range_end = range_data . split ( \"-\" , 2 ) return int ( range_start ), int ( range_end ) def close ( self ): \"\"\"close the response response\"\"\" self . resp . response . close () @property def client_crc ( self ): \"\"\"the client crc\"\"\" if self . __crc_enabled : return self . stream . crc return None async def read ( self , amt : int = - 1 ) -> bytes : \"\"\"async read data from stream Args: amt int: batch size of the data to read Returns: Awaitable[bytes]: \"\"\" return await self . stream . read ( amt ) client_crc property the client crc close () close the response response Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/models.py 148 149 150 def close ( self ): \"\"\"close the response response\"\"\" self . resp . response . close () read ( amt =- 1 ) async async read data from stream Parameters: Name Type Description Default amt int batch size of the data to read -1 Awaitable[bytes]: Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/models.py 159 160 161 162 163 164 165 166 167 168 async def read ( self , amt : int = - 1 ) -> bytes : \"\"\"async read data from stream Args: amt int: batch size of the data to read Returns: Awaitable[bytes]: \"\"\" return await self . stream . read ( amt )","title":"Models"},{"location":"reference/aiooss2/models/#aiooss2.models.AioGetObjectResult","text":"Bases: HeadObjectResult class for the result of api get_object Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/models.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class AioGetObjectResult ( HeadObjectResult ): \"\"\"class for the result of api get_object\"\"\" resp : \"AioResponse\" def __init__ ( # pylint: disable=too-many-arguments self , resp : \"AioResponse\" , progress_callback = None , crc_enabled = False , crypto_provider = None , discard = 0 , ): super () . __init__ ( resp ) self . __crypto_provider = crypto_provider self . __crc_enabled = crc_enabled self . content_range = _hget ( resp . headers , \"Content-Range\" ) if self . content_range : byte_range = self . _parse_range_str ( self . content_range ) self . stream = make_adapter ( self . resp , progress_callback = progress_callback , size = self . content_length , enable_crc = crc_enabled , discard = discard , ) if self . __crypto_provider : self . _make_crypto ( byte_range , discard ) else : if ( OSS_CLIENT_SIDE_ENCRYPTION_KEY in resp . headers or DEPRECATED_CLIENT_SIDE_ENCRYPTION_KEY in resp . headers ): logger . warning ( \"Using Bucket to get an encrypted object will return \" \"raw data, please confirm if you really want to do this\" ) def _make_crypto ( self , byte_range , discard ): content_crypto_material = ContentCryptoMaterial ( self . __crypto_provider . cipher , self . __crypto_provider . wrap_alg ) content_crypto_material . from_object_meta ( self . resp . headers ) if content_crypto_material . is_unencrypted (): logger . info ( \"The object is not encrypted, use crypto provider is not recommended\" ) else : crypto_provider = self . __crypto_provider if content_crypto_material . mat_desc != self . __crypto_provider . mat_desc : logger . warning ( \"The material description of the object \" \"and the provider is inconsistent\" ) encryption_materials = self . __crypto_provider . get_encryption_materials ( content_crypto_material . mat_desc ) if encryption_materials : crypto_provider = self . __crypto_provider . reset_encryption_materials ( encryption_materials ) else : raise ClientError ( \"There is no encryption materials \" \"match the material description of the object\" ) plain_key = crypto_provider . decrypt_encrypted_key ( content_crypto_material . encrypted_key ) if content_crypto_material . deprecated : if content_crypto_material . wrap_alg == KMS_ALI_WRAP_ALGORITHM : plain_counter = int ( crypto_provider . decrypt_encrypted_iv ( content_crypto_material . encrypted_iv , True ) ) else : plain_counter = int ( crypto_provider . decrypt_encrypted_iv ( content_crypto_material . encrypted_iv ) ) else : plain_iv = crypto_provider . decrypt_encrypted_iv ( content_crypto_material . encrypted_iv ) offset = 0 if self . content_range : start , _ = crypto_provider . adjust_range ( byte_range [ 0 ], byte_range [ 1 ]) offset = content_crypto_material . cipher . calc_offset ( start ) cipher = copy . copy ( content_crypto_material . cipher ) if content_crypto_material . deprecated : cipher . initial_by_counter ( plain_key , plain_counter + offset ) else : cipher . initialize ( plain_key , plain_iv , offset ) self . stream = crypto_provider . make_decrypt_adapter ( self . stream , cipher , discard ) async def __aenter__ ( self ): return self async def __aexit__ ( self , * args ): self . resp . release () async def __aiter__ ( self ): async for data in self . stream : return data @staticmethod def _parse_range_str ( content_range ): # :param str content_range: sample 'bytes 0-128/1024' range_data = ( content_range . split ( \" \" , 2 )[ 1 ]) . split ( \"/\" , 2 )[ 0 ] range_start , range_end = range_data . split ( \"-\" , 2 ) return int ( range_start ), int ( range_end ) def close ( self ): \"\"\"close the response response\"\"\" self . resp . response . close () @property def client_crc ( self ): \"\"\"the client crc\"\"\" if self . __crc_enabled : return self . stream . crc return None async def read ( self , amt : int = - 1 ) -> bytes : \"\"\"async read data from stream Args: amt int: batch size of the data to read Returns: Awaitable[bytes]: \"\"\" return await self . stream . read ( amt )","title":"AioGetObjectResult"},{"location":"reference/aiooss2/models/#aiooss2.models.AioGetObjectResult.client_crc","text":"the client crc","title":"client_crc"},{"location":"reference/aiooss2/models/#aiooss2.models.AioGetObjectResult.close","text":"close the response response Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/models.py 148 149 150 def close ( self ): \"\"\"close the response response\"\"\" self . resp . response . close ()","title":"close()"},{"location":"reference/aiooss2/models/#aiooss2.models.AioGetObjectResult.read","text":"async read data from stream Parameters: Name Type Description Default amt int batch size of the data to read -1 Awaitable[bytes]: Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/models.py 159 160 161 162 163 164 165 166 167 168 async def read ( self , amt : int = - 1 ) -> bytes : \"\"\"async read data from stream Args: amt int: batch size of the data to read Returns: Awaitable[bytes]: \"\"\" return await self . stream . read ( amt )","title":"read()"},{"location":"reference/aiooss2/multipart/","text":"Module for multipart operations abort_multipart_upload ( self , key , upload_id , headers = None ) async abort multipart uploading process Parameters: Name Type Description Default key str key value of the object required upload_id str the upload event id required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None Returns: Name Type Description RequestResult RequestResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/multipart.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 async def abort_multipart_upload ( self : \"AioBucket\" , key : str , upload_id : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> RequestResult : \"\"\"abort multipart uploading process Args: key (str): key value of the object upload_id (str): the upload event id headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. Returns: RequestResult: \"\"\" logger . debug ( \"Start to abort multipart upload\" ) headers = CaseInsensitiveDict ( headers ) resp = await self . _do_object ( \"DELETE\" , key , params = { \"uploadId\" : upload_id }, headers = headers ) logger . debug ( \"Abort multipart done\" ) return RequestResult ( resp ) complete_multipart_upload ( self , key , upload_id , parts , headers = None ) async Complete multipart uploading process create a new file. Parameters: Name Type Description Default key str key value of the object required upload_id str the upload event id required parts List [ PartInfo ] list of PathInfo required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None Returns: Name Type Description PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/multipart.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 async def complete_multipart_upload ( self : \"AioBucket\" , key : str , upload_id : str , parts : List [ \"PartInfo\" ], headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> PutObjectResult : \"\"\"Complete multipart uploading process create a new file. Args: key (str): key value of the object upload_id (str): the upload event id parts (List[PartInfo]): list of PathInfo headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. Returns: PutObjectResult: \"\"\" headers = CaseInsensitiveDict ( headers ) parts = sorted ( parts , key = lambda p : p . part_number ) data = to_complete_upload_request ( parts ) logger . debug ( \"Start to complete multipart upload\" ) resp = await self . _do_object ( \"POST\" , key , params = { \"uploadId\" : upload_id }, data = data , headers = headers , ) logger . debug ( \"Complete multipart upload %s done.\" , upload_id ) result = PutObjectResult ( resp ) if self . enable_crc and parts is not None : object_crc = calc_obj_crc_from_parts ( parts ) check_crc ( \"multipart upload\" , object_crc , result . crc , result . request_id ) return result init_multipart_upload ( self , key , headers = None , params = None ) async initialize multipart uploading The returning upload id, bucket name and object name together defines this uploading event. Parameters: Name Type Description Default key str key value of the object required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None params Optional [ Dict ] None Returns: Name Type Description InitMultipartUploadResult InitMultipartUploadResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/multipart.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 async def init_multipart_upload ( self : \"AioBucket\" , key : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Dict ] = None , ) -> InitMultipartUploadResult : \"\"\"initialize multipart uploading The returning upload id, bucket name and object name together defines this uploading event. Args: key (str): key value of the object headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Dict], optional): Returns: InitMultipartUploadResult: \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), key ) if params is None : tmp_params = {} else : tmp_params = params . copy () tmp_params [ \"uploads\" ] = \"\" logger . debug ( \"Start to init multipart upload\" ) resp = await self . _do_object ( \"POST\" , key , params = tmp_params , headers = headers ) logger . debug ( \"Init multipart upload done\" ) return await self . _parse_result ( resp , parse_init_multipart_upload , InitMultipartUploadResult ) list_multipart_uploads ( self , prefix = '' , delimiter = '' , key_marker = '' , upload_id_marker = '' , max_uploads = 1000 , headers = None ) async List multipart uploading process Parameters: Name Type Description Default prefix str Only list the parts with this prefix. '' delimiter str Directory delimiter. '' key_marker str Filename for paging. Do not required '' upload_id_marker str paging separator, Do not required '' max_uploads int max return numbers per page. 1000 headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None Returns: Name Type Description ListMultipartUploadsResult ListMultipartUploadsResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/multipart.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 async def list_multipart_uploads ( self : \"AioBucket\" , prefix : str = \"\" , delimiter : str = \"\" , key_marker : str = \"\" , upload_id_marker : str = \"\" , max_uploads : int = 1000 , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> ListMultipartUploadsResult : \"\"\"List multipart uploading process Args: prefix (str, optional): Only list the parts with this prefix. delimiter (str, optional): Directory delimiter. key_marker (str, optional): Filename for paging. Do not required in the first call of this function. Set to `next_key_marker` from result in the following calls. upload_id_marker (str, optional): paging separator, Do not required in the first call of this function. Set to `next_upload_id_marker` from result in the following calls. max_uploads (int, optional): max return numbers per page. headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. Returns: ListMultipartUploadsResult: \"\"\" logger . debug ( \"Start to list multipart uploads\" ) headers = CaseInsensitiveDict ( headers ) resp = await self . _do_object ( \"GET\" , \"\" , params = { \"uploads\" : \"\" , \"prefix\" : prefix , \"delimiter\" : delimiter , \"key-marker\" : key_marker , \"upload-id-marker\" : upload_id_marker , \"max-uploads\" : str ( max_uploads ), \"encoding-type\" : \"url\" , }, headers = headers , ) logger . debug ( \"List multipart uploads done, req_id\" ) return await self . _parse_result ( resp , parse_list_multipart_uploads , ListMultipartUploadsResult , ) list_parts ( self , key , upload_id , marker = '' , max_parts = 1000 , headers = None ) async list uploaded parts in a part uploading progress. Parameters: Name Type Description Default key str key value of the object. required upload_id str the upload event id. required marker str paging separator. '' max_uploads int max return numbers per page. required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None Returns: Name Type Description ListPartsResult ListPartsResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/multipart.py 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 async def list_parts ( self : \"AioBucket\" , key : str , upload_id : str , marker : str = \"\" , max_parts : int = 1000 , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> ListPartsResult : \"\"\"list uploaded parts in a part uploading progress. Args: key (str): key value of the object. upload_id (str): the upload event id. marker (str, optional): paging separator. max_uploads (int, optional): max return numbers per page. headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. Returns: ListPartsResult: \"\"\" logger . debug ( \"Start to list parts\" ) headers = CaseInsensitiveDict ( headers ) resp = await self . _do_object ( \"GET\" , key , params = { \"uploadId\" : upload_id , \"part-number-marker\" : marker , \"max-parts\" : str ( max_parts ), }, headers = headers , ) logger . debug ( \"List parts done.\" ) return await self . _parse_result ( resp , parse_list_parts , ListPartsResult ) upload_part ( self , key , upload_id , part_number , data , progress_callback = None , headers = None ) async upload one part Parameters: Name Type Description Default key str key value of the object required upload_id str the upload event id required part_number int the part number of this upload required data Any contents to upload required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None Returns: Name Type Description PutObjectResult PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/multipart.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 async def upload_part ( self : \"AioBucket\" , key : str , upload_id : str , part_number : int , data : Any , progress_callback : Optional [ Callable ] = None , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> PutObjectResult : \"\"\"upload one part Args: key (str): key value of the object upload_id (str): the upload event id part_number (int): the part number of this upload data (Any): contents to upload headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. Returns: PutObjectResult: PutObjectResult \"\"\" headers = CaseInsensitiveDict ( headers ) data = make_adapter ( data , progress_callback = progress_callback , enable_crc = self . enable_crc , ) logger . debug ( \"Start to upload multipart\" ) resp = await self . _do_object ( \"PUT\" , key , params = { \"uploadId\" : upload_id , \"partNumber\" : str ( part_number )}, headers = headers , data = data , ) logger . debug ( \"Upload multipart done\" ) result = PutObjectResult ( resp ) if self . enable_crc and result . crc is not None : check_crc ( \"upload part\" , data . crc , result . crc , result . request_id ) return result upload_part_copy ( self , source_bucket_name , source_key , byte_range , target_key , target_upload_id , target_part_number , headers = None , params = None ) async copy part or whole of a source file to a slice of a target file. Parameters: Name Type Description Default source_bucket_name str bucket name of the source file required source_key str key of the source file required byte_range Sequence [ int ] range of the source file to copy required target_key str key of the target file required target_upload_id str upload id of the target file required target_part_number int part number of the target file required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None params Optional [ Dict ] None Returns: Name Type Description PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/multipart.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 async def upload_part_copy ( self : \"AioBucket\" , source_bucket_name : str , source_key : str , byte_range : Sequence [ int ], target_key : str , target_upload_id : str , target_part_number : int , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Dict ] = None , ) -> PutObjectResult : \"\"\"copy part or whole of a source file to a slice of a target file. Args: source_bucket_name (str): bucket name of the source file source_key (str): key of the source file byte_range (Sequence[int]): range of the source file to copy target_key (str): key of the target file target_upload_id (str): upload id of the target file target_part_number (int): part number of the target file headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Dict], optional): Returns: PutObjectResult: \"\"\" headers = CaseInsensitiveDict ( headers ) parameters : Dict [ str , Any ] = params or {} if Bucket . VERSIONID in parameters : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) + \"?versionId=\" + parameters [ Bucket . VERSIONID ] ) else : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) ) range_string = _make_range_string ( byte_range ) if range_string : headers [ OSS_COPY_OBJECT_SOURCE_RANGE ] = range_string logger . debug ( \"Start to upload part copy\" ) parameters [ \"uploadId\" ] = target_upload_id parameters [ \"partNumber\" ] = str ( target_part_number ) resp = await self . _do_object ( \"PUT\" , target_key , params = parameters , headers = headers ) logger . debug ( \"Upload part copy done\" ) return PutObjectResult ( resp )","title":"Multipart"},{"location":"reference/aiooss2/multipart/#aiooss2.multipart.abort_multipart_upload","text":"abort multipart uploading process Parameters: Name Type Description Default key str key value of the object required upload_id str the upload event id required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None Returns: Name Type Description RequestResult RequestResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/multipart.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 async def abort_multipart_upload ( self : \"AioBucket\" , key : str , upload_id : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> RequestResult : \"\"\"abort multipart uploading process Args: key (str): key value of the object upload_id (str): the upload event id headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. Returns: RequestResult: \"\"\" logger . debug ( \"Start to abort multipart upload\" ) headers = CaseInsensitiveDict ( headers ) resp = await self . _do_object ( \"DELETE\" , key , params = { \"uploadId\" : upload_id }, headers = headers ) logger . debug ( \"Abort multipart done\" ) return RequestResult ( resp )","title":"abort_multipart_upload()"},{"location":"reference/aiooss2/multipart/#aiooss2.multipart.complete_multipart_upload","text":"Complete multipart uploading process create a new file. Parameters: Name Type Description Default key str key value of the object required upload_id str the upload event id required parts List [ PartInfo ] list of PathInfo required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None Returns: Name Type Description PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/multipart.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 async def complete_multipart_upload ( self : \"AioBucket\" , key : str , upload_id : str , parts : List [ \"PartInfo\" ], headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> PutObjectResult : \"\"\"Complete multipart uploading process create a new file. Args: key (str): key value of the object upload_id (str): the upload event id parts (List[PartInfo]): list of PathInfo headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. Returns: PutObjectResult: \"\"\" headers = CaseInsensitiveDict ( headers ) parts = sorted ( parts , key = lambda p : p . part_number ) data = to_complete_upload_request ( parts ) logger . debug ( \"Start to complete multipart upload\" ) resp = await self . _do_object ( \"POST\" , key , params = { \"uploadId\" : upload_id }, data = data , headers = headers , ) logger . debug ( \"Complete multipart upload %s done.\" , upload_id ) result = PutObjectResult ( resp ) if self . enable_crc and parts is not None : object_crc = calc_obj_crc_from_parts ( parts ) check_crc ( \"multipart upload\" , object_crc , result . crc , result . request_id ) return result","title":"complete_multipart_upload()"},{"location":"reference/aiooss2/multipart/#aiooss2.multipart.init_multipart_upload","text":"initialize multipart uploading The returning upload id, bucket name and object name together defines this uploading event. Parameters: Name Type Description Default key str key value of the object required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None params Optional [ Dict ] None Returns: Name Type Description InitMultipartUploadResult InitMultipartUploadResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/multipart.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 async def init_multipart_upload ( self : \"AioBucket\" , key : str , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Dict ] = None , ) -> InitMultipartUploadResult : \"\"\"initialize multipart uploading The returning upload id, bucket name and object name together defines this uploading event. Args: key (str): key value of the object headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Dict], optional): Returns: InitMultipartUploadResult: \"\"\" headers = set_content_type ( CaseInsensitiveDict ( headers ), key ) if params is None : tmp_params = {} else : tmp_params = params . copy () tmp_params [ \"uploads\" ] = \"\" logger . debug ( \"Start to init multipart upload\" ) resp = await self . _do_object ( \"POST\" , key , params = tmp_params , headers = headers ) logger . debug ( \"Init multipart upload done\" ) return await self . _parse_result ( resp , parse_init_multipart_upload , InitMultipartUploadResult )","title":"init_multipart_upload()"},{"location":"reference/aiooss2/multipart/#aiooss2.multipart.list_multipart_uploads","text":"List multipart uploading process Parameters: Name Type Description Default prefix str Only list the parts with this prefix. '' delimiter str Directory delimiter. '' key_marker str Filename for paging. Do not required '' upload_id_marker str paging separator, Do not required '' max_uploads int max return numbers per page. 1000 headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None Returns: Name Type Description ListMultipartUploadsResult ListMultipartUploadsResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/multipart.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 async def list_multipart_uploads ( self : \"AioBucket\" , prefix : str = \"\" , delimiter : str = \"\" , key_marker : str = \"\" , upload_id_marker : str = \"\" , max_uploads : int = 1000 , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> ListMultipartUploadsResult : \"\"\"List multipart uploading process Args: prefix (str, optional): Only list the parts with this prefix. delimiter (str, optional): Directory delimiter. key_marker (str, optional): Filename for paging. Do not required in the first call of this function. Set to `next_key_marker` from result in the following calls. upload_id_marker (str, optional): paging separator, Do not required in the first call of this function. Set to `next_upload_id_marker` from result in the following calls. max_uploads (int, optional): max return numbers per page. headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. Returns: ListMultipartUploadsResult: \"\"\" logger . debug ( \"Start to list multipart uploads\" ) headers = CaseInsensitiveDict ( headers ) resp = await self . _do_object ( \"GET\" , \"\" , params = { \"uploads\" : \"\" , \"prefix\" : prefix , \"delimiter\" : delimiter , \"key-marker\" : key_marker , \"upload-id-marker\" : upload_id_marker , \"max-uploads\" : str ( max_uploads ), \"encoding-type\" : \"url\" , }, headers = headers , ) logger . debug ( \"List multipart uploads done, req_id\" ) return await self . _parse_result ( resp , parse_list_multipart_uploads , ListMultipartUploadsResult , )","title":"list_multipart_uploads()"},{"location":"reference/aiooss2/multipart/#aiooss2.multipart.list_parts","text":"list uploaded parts in a part uploading progress. Parameters: Name Type Description Default key str key value of the object. required upload_id str the upload event id. required marker str paging separator. '' max_uploads int max return numbers per page. required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None Returns: Name Type Description ListPartsResult ListPartsResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/multipart.py 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 async def list_parts ( self : \"AioBucket\" , key : str , upload_id : str , marker : str = \"\" , max_parts : int = 1000 , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> ListPartsResult : \"\"\"list uploaded parts in a part uploading progress. Args: key (str): key value of the object. upload_id (str): the upload event id. marker (str, optional): paging separator. max_uploads (int, optional): max return numbers per page. headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. Returns: ListPartsResult: \"\"\" logger . debug ( \"Start to list parts\" ) headers = CaseInsensitiveDict ( headers ) resp = await self . _do_object ( \"GET\" , key , params = { \"uploadId\" : upload_id , \"part-number-marker\" : marker , \"max-parts\" : str ( max_parts ), }, headers = headers , ) logger . debug ( \"List parts done.\" ) return await self . _parse_result ( resp , parse_list_parts , ListPartsResult )","title":"list_parts()"},{"location":"reference/aiooss2/multipart/#aiooss2.multipart.upload_part","text":"upload one part Parameters: Name Type Description Default key str key value of the object required upload_id str the upload event id required part_number int the part number of this upload required data Any contents to upload required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None progress_callback Optional [ Callable ] callback function for progress bar. None Returns: Name Type Description PutObjectResult PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/multipart.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 async def upload_part ( self : \"AioBucket\" , key : str , upload_id : str , part_number : int , data : Any , progress_callback : Optional [ Callable ] = None , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , ) -> PutObjectResult : \"\"\"upload one part Args: key (str): key value of the object upload_id (str): the upload event id part_number (int): the part number of this upload data (Any): contents to upload headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. progress_callback (Optional[Callable], optional): callback function for progress bar. Returns: PutObjectResult: PutObjectResult \"\"\" headers = CaseInsensitiveDict ( headers ) data = make_adapter ( data , progress_callback = progress_callback , enable_crc = self . enable_crc , ) logger . debug ( \"Start to upload multipart\" ) resp = await self . _do_object ( \"PUT\" , key , params = { \"uploadId\" : upload_id , \"partNumber\" : str ( part_number )}, headers = headers , data = data , ) logger . debug ( \"Upload multipart done\" ) result = PutObjectResult ( resp ) if self . enable_crc and result . crc is not None : check_crc ( \"upload part\" , data . crc , result . crc , result . request_id ) return result","title":"upload_part()"},{"location":"reference/aiooss2/multipart/#aiooss2.multipart.upload_part_copy","text":"copy part or whole of a source file to a slice of a target file. Parameters: Name Type Description Default source_bucket_name str bucket name of the source file required source_key str key of the source file required byte_range Sequence [ int ] range of the source file to copy required target_key str key of the target file required target_upload_id str upload id of the target file required target_part_number int part number of the target file required headers Optional [ Union [ Dict , CaseInsensitiveDict ]] HTTP headers to specify. None params Optional [ Dict ] None Returns: Name Type Description PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/multipart.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 async def upload_part_copy ( self : \"AioBucket\" , source_bucket_name : str , source_key : str , byte_range : Sequence [ int ], target_key : str , target_upload_id : str , target_part_number : int , headers : Optional [ Union [ Dict , CaseInsensitiveDict ]] = None , params : Optional [ Dict ] = None , ) -> PutObjectResult : \"\"\"copy part or whole of a source file to a slice of a target file. Args: source_bucket_name (str): bucket name of the source file source_key (str): key of the source file byte_range (Sequence[int]): range of the source file to copy target_key (str): key of the target file target_upload_id (str): upload id of the target file target_part_number (int): part number of the target file headers (Optional[Union[Dict, CaseInsensitiveDict]], optional): HTTP headers to specify. params (Optional[Dict], optional): Returns: PutObjectResult: \"\"\" headers = CaseInsensitiveDict ( headers ) parameters : Dict [ str , Any ] = params or {} if Bucket . VERSIONID in parameters : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) + \"?versionId=\" + parameters [ Bucket . VERSIONID ] ) else : headers [ OSS_COPY_OBJECT_SOURCE ] = ( \"/\" + source_bucket_name + \"/\" + urlquote ( source_key , \"\" ) ) range_string = _make_range_string ( byte_range ) if range_string : headers [ OSS_COPY_OBJECT_SOURCE_RANGE ] = range_string logger . debug ( \"Start to upload part copy\" ) parameters [ \"uploadId\" ] = target_upload_id parameters [ \"partNumber\" ] = str ( target_part_number ) resp = await self . _do_object ( \"PUT\" , target_key , params = parameters , headers = headers ) logger . debug ( \"Upload part copy done\" ) return PutObjectResult ( resp )","title":"upload_part_copy()"},{"location":"reference/aiooss2/resumable/","text":"Module for resumable operations ResumableDownloader Bases: _ResumableDownloader Resumable Downloader Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/resumable.py 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 class ResumableDownloader ( _ResumableDownloader ): \"\"\"Resumable Downloader\"\"\" bucket : \"AioBucket\" async def download ( # pylint: disable=invalid-overridden-method self , server_crc = None ): \"\"\"Resumable download object from oss server to a local file. Args: server_crc (_type_, optional): _description_. Defaults to None. \"\"\" self . __load_record () parts_to_download = self . __get_parts_to_download () logger . debug ( \"Parts need to download: %s \" , parts_to_download ) with open ( self . __tmp_file , \"ab\" ): pass sem = asyncio . Semaphore ( self . __num_threads ) tasks = [ asyncio . ensure_future ( self . _download_task ( sem , part_to_download )) for part_to_download in parts_to_download ] await asyncio . gather ( * tasks ) if self . bucket . enable_crc : parts = sorted ( self . __finished_parts , key = lambda p : p . part_number ) object_crc = calc_obj_crc_from_parts ( parts ) check_crc ( \"resume download\" , object_crc , server_crc , None ) force_rename ( self . __tmp_file , self . filename ) self . _report_progress ( self . size ) self . _del_record () async def _download_task ( self , sem : \"asyncio.Semaphore\" , part_to_upload : _PartToProcess ): async with sem : return await self . _download_part ( part_to_upload ) async def _download_part ( self , part : _PartToProcess ): self . _report_progress ( self . __finished_size ) with open ( self . __tmp_file , \"rb+\" ) as f_r : f_r . seek ( part . start , os . SEEK_SET ) headers = _populate_valid_headers ( self . __headers , [ OSS_REQUEST_PAYER , OSS_TRAFFIC_LIMIT ] ) if headers is None : headers = CaseInsensitiveDict () headers [ IF_MATCH ] = self . objectInfo . etag headers [ IF_UNMODIFIED_SINCE ] = http_date ( self . objectInfo . mtime ) result = await self . bucket . get_object ( self . key , byte_range = ( part . start , part . end - 1 ), headers = headers , params = self . __params , ) await copyfileobj_and_verify ( result , f_r , part . end - part . start , request_id = result . request_id , ) part . part_crc = result . client_crc logger . debug ( \"down part success, add part info to record, part_number:\" \" %s , start: %s , end: %s \" , part . part_number , part . start , part . end , ) self . __finish_part ( part ) download ( server_crc = None ) async Resumable download object from oss server to a local file. Parameters: Name Type Description Default server_crc _type_ description . Defaults to None. None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/resumable.py 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 async def download ( # pylint: disable=invalid-overridden-method self , server_crc = None ): \"\"\"Resumable download object from oss server to a local file. Args: server_crc (_type_, optional): _description_. Defaults to None. \"\"\" self . __load_record () parts_to_download = self . __get_parts_to_download () logger . debug ( \"Parts need to download: %s \" , parts_to_download ) with open ( self . __tmp_file , \"ab\" ): pass sem = asyncio . Semaphore ( self . __num_threads ) tasks = [ asyncio . ensure_future ( self . _download_task ( sem , part_to_download )) for part_to_download in parts_to_download ] await asyncio . gather ( * tasks ) if self . bucket . enable_crc : parts = sorted ( self . __finished_parts , key = lambda p : p . part_number ) object_crc = calc_obj_crc_from_parts ( parts ) check_crc ( \"resume download\" , object_crc , server_crc , None ) force_rename ( self . __tmp_file , self . filename ) self . _report_progress ( self . size ) self . _del_record () ResumableUploader Bases: _ResumableUploader Resumable Uploader Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/resumable.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 class ResumableUploader ( _ResumableUploader ): \"\"\"Resumable Uploader\"\"\" bucket : \"AioBucket\" async def upload ( # pylint: disable=invalid-overridden-method self , ) -> \"PutObjectResult\" : \"\"\"resumable upload file to oss storage Returns: PutObjectResult: \"\"\" await self . _load_record () parts_to_upload : Collection [ \"_PartToProcess\" ] = self . __get_parts_to_upload ( self . __finished_parts ) parts_to_upload = sorted ( parts_to_upload , key = lambda p : p . part_number ) logger . debug ( \"Parts need to upload: %s \" , parts_to_upload ) sem = asyncio . Semaphore ( self . __num_threads ) tasks = [ asyncio . ensure_future ( self . _upload_task ( sem , part_to_upload )) for part_to_upload in parts_to_upload ] await asyncio . gather ( * tasks ) self . _report_progress ( self . size ) headers = _populate_valid_headers ( self . __headers , [ OSS_REQUEST_PAYER , OSS_OBJECT_ACL ] ) result = await self . bucket . complete_multipart_upload ( self . key , self . __upload_id , self . __finished_parts , headers = headers ) self . _del_record () return result async def _upload_task ( self , sem : \"asyncio.Semaphore\" , part_to_upload : _PartToProcess ): async with sem : return await self . _upload_part ( part_to_upload ) async def _upload_part ( self , part : _PartToProcess ): with open ( to_unicode ( self . filename ), \"rb\" ) as f_r : self . _report_progress ( self . __finished_size ) f_r . seek ( part . start , os . SEEK_SET ) headers = _populate_valid_headers ( self . __headers , [ OSS_REQUEST_PAYER , OSS_TRAFFIC_LIMIT ] ) if self . __encryption : result = await self . bucket . upload_part ( self . key , self . __upload_id , part . part_number , FilelikeObjectAdapter ( f_r , size = part . size ), headers = headers , upload_context = self . __upload_context , ) else : result = await self . bucket . upload_part ( self . key , self . __upload_id , part . part_number , FilelikeObjectAdapter ( f_r , size = part . size ), headers = headers , ) logger . debug ( \"Upload part success, add part info to record, part_number: \" \" %s , etag: %s , size: %s \" , part . part_number , result . etag , part . size , ) self . __finish_part ( PartInfo ( part . part_number , result . etag , size = part . size , part_crc = result . crc , ) ) def _verify_record ( self , record : Optional [ Dict ]): if record and not self . __is_record_sane ( record ): logger . warning ( \"The content of record is invalid, delete the record\" ) self . _del_record () return None if record and self . __file_changed ( record ): logger . warning ( \"File: %s has been changed, delete the record\" , self . filename ) self . _del_record () return None if record and not self . __upload_exists ( record [ \"upload_id\" ]): logger . warning ( \"Multipart upload: %s does not exist, delete the record\" , record [ \"upload_id\" ], ) self . _del_record () return None return record async def init_record ( self ) -> Dict : \"\"\"Initialization record for the file to upload.\"\"\" params = _populate_valid_params ( self . __params , [ Bucket . SEQUENTIAL ]) part_size = determine_part_size ( self . size , self . __part_size ) logger . debug ( \"Upload File size: %d , User-specify part_size: %d , \" \"Calculated part_size: %d \" , self . size , self . __part_size , part_size , ) if self . __encryption : upload_context = MultipartUploadCryptoContext ( self . size , part_size ) init_result = await self . bucket . init_multipart_upload ( self . key , self . __headers , params , upload_context ) upload_id = init_result . upload_id if self . __record_upload_context : material = upload_context . content_crypto_material material_record = { \"wrap_alg\" : material . wrap_alg , \"cek_alg\" : material . cek_alg , \"encrypted_key\" : b64encode_as_string ( material . encrypted_key ), \"encrypted_iv\" : b64encode_as_string ( material . encrypted_iv ), \"mat_desc\" : material . mat_desc , } else : init_result = await self . bucket . init_multipart_upload ( self . key , self . __headers , params ) upload_id = init_result . upload_id record = { \"op_type\" : self . __op , \"upload_id\" : upload_id , \"file_path\" : self . _abspath , \"size\" : self . size , \"mtime\" : self . __mtime , \"bucket\" : self . bucket . bucket_name , \"key\" : self . key , \"part_size\" : part_size , } if self . __record_upload_context : record [ \"content_crypto_material\" ] = material_record logger . debug ( \"Add new record, bucket: %s , key: %s , upload_id: %s , part_size: %d \" , self . bucket . bucket_name , self . key , upload_id , part_size , ) self . _put_record ( record ) return record async def _get_finished_parts ( self ): parts = [] valid_headers = _filter_invalid_headers ( self . __headers , [ OSS_SERVER_SIDE_ENCRYPTION , OSS_SERVER_SIDE_DATA_ENCRYPTION ], ) async for part in AioPartIterator ( self . bucket , self . key , self . __upload_id , headers = valid_headers ): parts . append ( part ) return parts async def _load_record ( self ): record : Optional [ Dict ] = self . _get_record () logger . debug ( \"Load record return %s \" , record ) record : Optional [ Dict ] = self . _verify_record ( record ) record : Dict = record or await self . init_record () self . __record : Dict = record self . __part_size : int = self . __record [ \"part_size\" ] self . __upload_id = self . __record [ \"upload_id\" ] if self . __record_upload_context : if \"content_crypto_material\" in self . __record : material_record = self . __record [ \"content_crypto_material\" ] wrap_alg = material_record [ \"wrap_alg\" ] cek_alg = material_record [ \"cek_alg\" ] if ( cek_alg != self . bucket . crypto_provider . cipher . alg or wrap_alg != self . bucket . crypto_provider . wrap_alg ): err_msg = ( \"Envelope or data encryption/decryption \" \"algorithm is inconsistent\" ) raise InconsistentError ( err_msg , self ) content_crypto_material = ContentCryptoMaterial ( self . bucket . crypto_provider . cipher , material_record [ \"wrap_alg\" ], b64decode_from_string ( material_record [ \"encrypted_key\" ]), b64decode_from_string ( material_record [ \"encrypted_iv\" ]), material_record [ \"mat_desc\" ], ) self . __upload_context = MultipartUploadCryptoContext ( self . size , self . __part_size , content_crypto_material ) else : err_msg = ( \"If record_upload_context flag is true, \" \"content_crypto_material must in the the record\" ) raise InconsistentError ( err_msg , self ) else : if \"content_crypto_material\" in self . __record : err_msg = ( \"content_crypto_material must in the the record, \" \"but record_upload_context flat is false\" ) raise InvalidEncryptionRequest ( err_msg , self ) self . __finished_parts = await self . _get_finished_parts () self . __finished_size = sum ( p . size for p in self . __finished_parts ) init_record () async Initialization record for the file to upload. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/resumable.py 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 async def init_record ( self ) -> Dict : \"\"\"Initialization record for the file to upload.\"\"\" params = _populate_valid_params ( self . __params , [ Bucket . SEQUENTIAL ]) part_size = determine_part_size ( self . size , self . __part_size ) logger . debug ( \"Upload File size: %d , User-specify part_size: %d , \" \"Calculated part_size: %d \" , self . size , self . __part_size , part_size , ) if self . __encryption : upload_context = MultipartUploadCryptoContext ( self . size , part_size ) init_result = await self . bucket . init_multipart_upload ( self . key , self . __headers , params , upload_context ) upload_id = init_result . upload_id if self . __record_upload_context : material = upload_context . content_crypto_material material_record = { \"wrap_alg\" : material . wrap_alg , \"cek_alg\" : material . cek_alg , \"encrypted_key\" : b64encode_as_string ( material . encrypted_key ), \"encrypted_iv\" : b64encode_as_string ( material . encrypted_iv ), \"mat_desc\" : material . mat_desc , } else : init_result = await self . bucket . init_multipart_upload ( self . key , self . __headers , params ) upload_id = init_result . upload_id record = { \"op_type\" : self . __op , \"upload_id\" : upload_id , \"file_path\" : self . _abspath , \"size\" : self . size , \"mtime\" : self . __mtime , \"bucket\" : self . bucket . bucket_name , \"key\" : self . key , \"part_size\" : part_size , } if self . __record_upload_context : record [ \"content_crypto_material\" ] = material_record logger . debug ( \"Add new record, bucket: %s , key: %s , upload_id: %s , part_size: %d \" , self . bucket . bucket_name , self . key , upload_id , part_size , ) self . _put_record ( record ) return record upload () async resumable upload file to oss storage Returns: Name Type Description PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/resumable.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 async def upload ( # pylint: disable=invalid-overridden-method self , ) -> \"PutObjectResult\" : \"\"\"resumable upload file to oss storage Returns: PutObjectResult: \"\"\" await self . _load_record () parts_to_upload : Collection [ \"_PartToProcess\" ] = self . __get_parts_to_upload ( self . __finished_parts ) parts_to_upload = sorted ( parts_to_upload , key = lambda p : p . part_number ) logger . debug ( \"Parts need to upload: %s \" , parts_to_upload ) sem = asyncio . Semaphore ( self . __num_threads ) tasks = [ asyncio . ensure_future ( self . _upload_task ( sem , part_to_upload )) for part_to_upload in parts_to_upload ] await asyncio . gather ( * tasks ) self . _report_progress ( self . size ) headers = _populate_valid_headers ( self . __headers , [ OSS_REQUEST_PAYER , OSS_OBJECT_ACL ] ) result = await self . bucket . complete_multipart_upload ( self . key , self . __upload_id , self . __finished_parts , headers = headers ) self . _del_record () return result resumable_download ( bucket , key , filename , store = None , headers = None , part_size = None , progress_callback = None , num_threads = None , params = None , multiget_threshold = None ) async Resumable download object to local file, implementation method is to create a list temporary files whose name is formed by the original filename with some random surfix. If the downloading was interrupted by some reasons, only those remaied parts need to be downloaded. Using CryptoBucket will make the download fallback to the normal one. Avoid using multi-threading/processing as the temp downloaded file might be covered. Parameters: Name Type Description Default bucket Union [ AioBucket , CryptoBucket ] bucket object to download required key Union [ str , bytes ] object key to store the file required filename Union [ str , bytes ] filename to download required store Optional [ ResumableStore ] ResumableStore object to keep the downloading info in the previous operation. None headers Optional [ Mapping ] HTTP headers to send. Defaults to None. download_part only accept OSS_REQUEST_PAYER get_object and get_object_to_file only accept OSS_REQUEST_PAYER, OSS_TRAFFIC_LIMIT None multipget_threshold Optional [ int ] threshold to use multipart download instead of a normal one. required part_size Optional [ int ] partition size of the multipart. None progress_callback Optional [ Callable ] callback function for progress bar. None num_threads Optional [ int ] concurrency number during the uploadinging None params Optional [ Mapping ] None Return None: Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/resumable.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 async def resumable_download ( # pylint: disable=too-many-arguments bucket : Union [ \"AioBucket\" , \"CryptoBucket\" ], key : Union [ str , bytes ], filename : Union [ str , bytes ], store : Optional [ \"ResumableStore\" ] = None , headers : Optional [ Mapping ] = None , part_size : Optional [ int ] = None , progress_callback : Optional [ Callable ] = None , num_threads : Optional [ int ] = None , params : Optional [ Mapping ] = None , multiget_threshold : Optional [ int ] = None , ) -> None : \"\"\"Resumable download object to local file, implementation method is to create a list temporary files whose name is formed by the original filename with some random surfix. If the downloading was interrupted by some reasons, only those remaied parts need to be downloaded. # Using `CryptoBucket` will make the download fallback to the normal one. # Avoid using multi-threading/processing as the temp downloaded file might # be covered. Args: bucket (Union[AioBucket, CryptoBucket]): bucket object to download key (Union[str, bytes]): object key to store the file filename (Union[str, bytes]): filename to download store (Optional[\"ResumableStore\"]): ResumableStore object to keep the downloading info in the previous operation. headers (Optional[Mapping]): HTTP headers to send. Defaults to None. # download_part only accept OSS_REQUEST_PAYER # get_object and get_object_to_file only accept OSS_REQUEST_PAYER, OSS_TRAFFIC_LIMIT multipget_threshold (Optional[int]): threshold to use multipart download instead of a normal one. part_size (Optional[int]): partition size of the multipart. progress_callback (Optional[Callable]): callback function for progress bar. num_threads (Optional[int]): concurrency number during the uploadinging params (Optional[Mapping]): Return: None: \"\"\" key_str : str = to_string ( key ) filename_str : str = to_unicode ( filename ) logger . debug ( \"Start to resumable download, bucket: %s , key: %s , filename: %s , \" \"multiget_threshold: %s , part_size: %s , num_threads: %s \" , bucket . bucket_name , key_str , filename_str , multiget_threshold , part_size , num_threads , ) multiget_threshold = multiget_threshold or MULTIGET_THRESHOLD valid_headers = _populate_valid_headers ( headers , [ OSS_REQUEST_PAYER , OSS_TRAFFIC_LIMIT ] ) result = await bucket . head_object ( key_str , params = params , headers = valid_headers ) logger . debug ( \"The size of object to download is: %s \" , result . content_length ) if result . content_length >= multiget_threshold : downloader = ResumableDownloader ( bucket , key , filename , _ObjectInfo . make ( result ), part_size = part_size , progress_callback = progress_callback , num_threads = num_threads , store = store , params = params , headers = valid_headers , ) await downloader . download ( result . server_crc ) else : await bucket . get_object_to_file ( key_str , filename_str , progress_callback = progress_callback , params = params , headers = valid_headers , ) resumable_upload ( bucket , key , filename , store = None , headers = None , multipart_threshold = None , part_size = None , progress_callback = None , num_threads = None , params = None ) async Resumable upload local file , The implementation is splitting local files to multipart, storing uploading information in local files. If the uploading was interrupted by some reasons, only those remaied parts need to be uploaded. Using CryptoBucket will make the upload fallback to the normal one. Parameters: Name Type Description Default bucket Union [ AioBucket , CryptoBucket ] bucket object to upload required key Union [ str , bytes ] object key to store the file required filename Union [ str , bytes ] filename to upload required store Optional [ ResumableStore ] ResumableStore object to keep the uploading info in the previous operation. Defaults to None. None headers Optional [ Mapping ] HTTP headers to send. Defaults to None. put_object or init_multipart_upload can make use of the whole headers uplpad_part only accept OSS_REQUEST_PAYER, OSS_TRAFFIC_LIMIT complete_multipart_upload only accept OSS_REQUEST_PAYER, OSS_OBJECT_ACL None multipart_threshold Optional [ int ] threshold to use multipart upload instead of a normal one. Defaults to None. None part_size Optional [ int ] partition size of the multipart. Defaults to None. None progress_callback Optional [ Callable ] callback function for progress bar. Defaults to None. None num_threads Optional [ int ] concurrency number during the uploading Defaults to None. None params Optional [ Mapping ] Defaults to None. None Returns: Name Type Description PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/resumable.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 async def resumable_upload ( # pylint: disable=too-many-arguments bucket : Union [ \"AioBucket\" , \"CryptoBucket\" ], key : Union [ str , bytes ], filename : Union [ str , bytes ], store : Optional [ \"ResumableStore\" ] = None , headers : Optional [ Mapping ] = None , multipart_threshold : Optional [ int ] = None , part_size : Optional [ int ] = None , progress_callback : Optional [ Callable ] = None , num_threads : Optional [ int ] = None , params : Optional [ Mapping ] = None , ) -> \"PutObjectResult\" : \"\"\"Resumable upload local file , The implementation is splitting local files to multipart, storing uploading information in local files. If the uploading was interrupted by some reasons, only those remaied parts need to be uploaded. # Using `CryptoBucket` will make the upload fallback to the normal one. Args: bucket (Union[AioBucket, CryptoBucket]): bucket object to upload key (Union[str, bytes]): object key to store the file filename (Union[str, bytes]): filename to upload store (Optional[\"ResumableStore\"]): ResumableStore object to keep the uploading info in the previous operation. Defaults to None. headers (Optional[Mapping]): HTTP headers to send. Defaults to None. # put_object or init_multipart_upload can make use of the whole headers # uplpad_part only accept OSS_REQUEST_PAYER, OSS_TRAFFIC_LIMIT # complete_multipart_upload only accept OSS_REQUEST_PAYER, OSS_OBJECT_ACL multipart_threshold (Optional[int]): threshold to use multipart upload instead of a normal one. Defaults to None. part_size (Optional[int]): partition size of the multipart. Defaults to None. progress_callback (Optional[Callable]): callback function for progress bar. Defaults to None. num_threads (Optional[int]): concurrency number during the uploading Defaults to None. params (Optional[Mapping]): Defaults to None. Returns: PutObjectResult: \"\"\" key_str = to_string ( key ) filename_str = to_unicode ( filename ) size = os . path . getsize ( filename_str ) logger . debug ( \"Start to resumable upload, bucket: %s , key: %s , filename: %s , \" \"headers: %s , multipart_threshold: %s , part_size: %s , \" \"num_threads: %s , size of file to upload is %s \" , bucket . bucket_name , key_str , filename_str , headers , multipart_threshold , part_size , num_threads , size , ) multipart_threshold = multipart_threshold or MULTIPART_THRESHOLD num_threads = num_threads or MULTIPART_NUM_THREADS part_size = part_size or PART_SIZE if size >= multipart_threshold and not isinstance ( bucket , CryptoBucket ): store = store or ResumableStore () uploader = ResumableUploader ( bucket , key_str , filename_str , size , store , part_size = part_size , headers = headers , progress_callback = progress_callback , num_threads = num_threads , params = params , ) result = await uploader . upload () else : result = await bucket . put_object_from_file ( key_str , filename_str , headers = headers , progress_callback = progress_callback , ) return result","title":"Resumable"},{"location":"reference/aiooss2/resumable/#aiooss2.resumable.ResumableDownloader","text":"Bases: _ResumableDownloader Resumable Downloader Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/resumable.py 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 class ResumableDownloader ( _ResumableDownloader ): \"\"\"Resumable Downloader\"\"\" bucket : \"AioBucket\" async def download ( # pylint: disable=invalid-overridden-method self , server_crc = None ): \"\"\"Resumable download object from oss server to a local file. Args: server_crc (_type_, optional): _description_. Defaults to None. \"\"\" self . __load_record () parts_to_download = self . __get_parts_to_download () logger . debug ( \"Parts need to download: %s \" , parts_to_download ) with open ( self . __tmp_file , \"ab\" ): pass sem = asyncio . Semaphore ( self . __num_threads ) tasks = [ asyncio . ensure_future ( self . _download_task ( sem , part_to_download )) for part_to_download in parts_to_download ] await asyncio . gather ( * tasks ) if self . bucket . enable_crc : parts = sorted ( self . __finished_parts , key = lambda p : p . part_number ) object_crc = calc_obj_crc_from_parts ( parts ) check_crc ( \"resume download\" , object_crc , server_crc , None ) force_rename ( self . __tmp_file , self . filename ) self . _report_progress ( self . size ) self . _del_record () async def _download_task ( self , sem : \"asyncio.Semaphore\" , part_to_upload : _PartToProcess ): async with sem : return await self . _download_part ( part_to_upload ) async def _download_part ( self , part : _PartToProcess ): self . _report_progress ( self . __finished_size ) with open ( self . __tmp_file , \"rb+\" ) as f_r : f_r . seek ( part . start , os . SEEK_SET ) headers = _populate_valid_headers ( self . __headers , [ OSS_REQUEST_PAYER , OSS_TRAFFIC_LIMIT ] ) if headers is None : headers = CaseInsensitiveDict () headers [ IF_MATCH ] = self . objectInfo . etag headers [ IF_UNMODIFIED_SINCE ] = http_date ( self . objectInfo . mtime ) result = await self . bucket . get_object ( self . key , byte_range = ( part . start , part . end - 1 ), headers = headers , params = self . __params , ) await copyfileobj_and_verify ( result , f_r , part . end - part . start , request_id = result . request_id , ) part . part_crc = result . client_crc logger . debug ( \"down part success, add part info to record, part_number:\" \" %s , start: %s , end: %s \" , part . part_number , part . start , part . end , ) self . __finish_part ( part )","title":"ResumableDownloader"},{"location":"reference/aiooss2/resumable/#aiooss2.resumable.ResumableDownloader.download","text":"Resumable download object from oss server to a local file. Parameters: Name Type Description Default server_crc _type_ description . Defaults to None. None Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/resumable.py 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 async def download ( # pylint: disable=invalid-overridden-method self , server_crc = None ): \"\"\"Resumable download object from oss server to a local file. Args: server_crc (_type_, optional): _description_. Defaults to None. \"\"\" self . __load_record () parts_to_download = self . __get_parts_to_download () logger . debug ( \"Parts need to download: %s \" , parts_to_download ) with open ( self . __tmp_file , \"ab\" ): pass sem = asyncio . Semaphore ( self . __num_threads ) tasks = [ asyncio . ensure_future ( self . _download_task ( sem , part_to_download )) for part_to_download in parts_to_download ] await asyncio . gather ( * tasks ) if self . bucket . enable_crc : parts = sorted ( self . __finished_parts , key = lambda p : p . part_number ) object_crc = calc_obj_crc_from_parts ( parts ) check_crc ( \"resume download\" , object_crc , server_crc , None ) force_rename ( self . __tmp_file , self . filename ) self . _report_progress ( self . size ) self . _del_record ()","title":"download()"},{"location":"reference/aiooss2/resumable/#aiooss2.resumable.ResumableUploader","text":"Bases: _ResumableUploader Resumable Uploader Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/resumable.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 class ResumableUploader ( _ResumableUploader ): \"\"\"Resumable Uploader\"\"\" bucket : \"AioBucket\" async def upload ( # pylint: disable=invalid-overridden-method self , ) -> \"PutObjectResult\" : \"\"\"resumable upload file to oss storage Returns: PutObjectResult: \"\"\" await self . _load_record () parts_to_upload : Collection [ \"_PartToProcess\" ] = self . __get_parts_to_upload ( self . __finished_parts ) parts_to_upload = sorted ( parts_to_upload , key = lambda p : p . part_number ) logger . debug ( \"Parts need to upload: %s \" , parts_to_upload ) sem = asyncio . Semaphore ( self . __num_threads ) tasks = [ asyncio . ensure_future ( self . _upload_task ( sem , part_to_upload )) for part_to_upload in parts_to_upload ] await asyncio . gather ( * tasks ) self . _report_progress ( self . size ) headers = _populate_valid_headers ( self . __headers , [ OSS_REQUEST_PAYER , OSS_OBJECT_ACL ] ) result = await self . bucket . complete_multipart_upload ( self . key , self . __upload_id , self . __finished_parts , headers = headers ) self . _del_record () return result async def _upload_task ( self , sem : \"asyncio.Semaphore\" , part_to_upload : _PartToProcess ): async with sem : return await self . _upload_part ( part_to_upload ) async def _upload_part ( self , part : _PartToProcess ): with open ( to_unicode ( self . filename ), \"rb\" ) as f_r : self . _report_progress ( self . __finished_size ) f_r . seek ( part . start , os . SEEK_SET ) headers = _populate_valid_headers ( self . __headers , [ OSS_REQUEST_PAYER , OSS_TRAFFIC_LIMIT ] ) if self . __encryption : result = await self . bucket . upload_part ( self . key , self . __upload_id , part . part_number , FilelikeObjectAdapter ( f_r , size = part . size ), headers = headers , upload_context = self . __upload_context , ) else : result = await self . bucket . upload_part ( self . key , self . __upload_id , part . part_number , FilelikeObjectAdapter ( f_r , size = part . size ), headers = headers , ) logger . debug ( \"Upload part success, add part info to record, part_number: \" \" %s , etag: %s , size: %s \" , part . part_number , result . etag , part . size , ) self . __finish_part ( PartInfo ( part . part_number , result . etag , size = part . size , part_crc = result . crc , ) ) def _verify_record ( self , record : Optional [ Dict ]): if record and not self . __is_record_sane ( record ): logger . warning ( \"The content of record is invalid, delete the record\" ) self . _del_record () return None if record and self . __file_changed ( record ): logger . warning ( \"File: %s has been changed, delete the record\" , self . filename ) self . _del_record () return None if record and not self . __upload_exists ( record [ \"upload_id\" ]): logger . warning ( \"Multipart upload: %s does not exist, delete the record\" , record [ \"upload_id\" ], ) self . _del_record () return None return record async def init_record ( self ) -> Dict : \"\"\"Initialization record for the file to upload.\"\"\" params = _populate_valid_params ( self . __params , [ Bucket . SEQUENTIAL ]) part_size = determine_part_size ( self . size , self . __part_size ) logger . debug ( \"Upload File size: %d , User-specify part_size: %d , \" \"Calculated part_size: %d \" , self . size , self . __part_size , part_size , ) if self . __encryption : upload_context = MultipartUploadCryptoContext ( self . size , part_size ) init_result = await self . bucket . init_multipart_upload ( self . key , self . __headers , params , upload_context ) upload_id = init_result . upload_id if self . __record_upload_context : material = upload_context . content_crypto_material material_record = { \"wrap_alg\" : material . wrap_alg , \"cek_alg\" : material . cek_alg , \"encrypted_key\" : b64encode_as_string ( material . encrypted_key ), \"encrypted_iv\" : b64encode_as_string ( material . encrypted_iv ), \"mat_desc\" : material . mat_desc , } else : init_result = await self . bucket . init_multipart_upload ( self . key , self . __headers , params ) upload_id = init_result . upload_id record = { \"op_type\" : self . __op , \"upload_id\" : upload_id , \"file_path\" : self . _abspath , \"size\" : self . size , \"mtime\" : self . __mtime , \"bucket\" : self . bucket . bucket_name , \"key\" : self . key , \"part_size\" : part_size , } if self . __record_upload_context : record [ \"content_crypto_material\" ] = material_record logger . debug ( \"Add new record, bucket: %s , key: %s , upload_id: %s , part_size: %d \" , self . bucket . bucket_name , self . key , upload_id , part_size , ) self . _put_record ( record ) return record async def _get_finished_parts ( self ): parts = [] valid_headers = _filter_invalid_headers ( self . __headers , [ OSS_SERVER_SIDE_ENCRYPTION , OSS_SERVER_SIDE_DATA_ENCRYPTION ], ) async for part in AioPartIterator ( self . bucket , self . key , self . __upload_id , headers = valid_headers ): parts . append ( part ) return parts async def _load_record ( self ): record : Optional [ Dict ] = self . _get_record () logger . debug ( \"Load record return %s \" , record ) record : Optional [ Dict ] = self . _verify_record ( record ) record : Dict = record or await self . init_record () self . __record : Dict = record self . __part_size : int = self . __record [ \"part_size\" ] self . __upload_id = self . __record [ \"upload_id\" ] if self . __record_upload_context : if \"content_crypto_material\" in self . __record : material_record = self . __record [ \"content_crypto_material\" ] wrap_alg = material_record [ \"wrap_alg\" ] cek_alg = material_record [ \"cek_alg\" ] if ( cek_alg != self . bucket . crypto_provider . cipher . alg or wrap_alg != self . bucket . crypto_provider . wrap_alg ): err_msg = ( \"Envelope or data encryption/decryption \" \"algorithm is inconsistent\" ) raise InconsistentError ( err_msg , self ) content_crypto_material = ContentCryptoMaterial ( self . bucket . crypto_provider . cipher , material_record [ \"wrap_alg\" ], b64decode_from_string ( material_record [ \"encrypted_key\" ]), b64decode_from_string ( material_record [ \"encrypted_iv\" ]), material_record [ \"mat_desc\" ], ) self . __upload_context = MultipartUploadCryptoContext ( self . size , self . __part_size , content_crypto_material ) else : err_msg = ( \"If record_upload_context flag is true, \" \"content_crypto_material must in the the record\" ) raise InconsistentError ( err_msg , self ) else : if \"content_crypto_material\" in self . __record : err_msg = ( \"content_crypto_material must in the the record, \" \"but record_upload_context flat is false\" ) raise InvalidEncryptionRequest ( err_msg , self ) self . __finished_parts = await self . _get_finished_parts () self . __finished_size = sum ( p . size for p in self . __finished_parts )","title":"ResumableUploader"},{"location":"reference/aiooss2/resumable/#aiooss2.resumable.ResumableUploader.init_record","text":"Initialization record for the file to upload. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/resumable.py 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 async def init_record ( self ) -> Dict : \"\"\"Initialization record for the file to upload.\"\"\" params = _populate_valid_params ( self . __params , [ Bucket . SEQUENTIAL ]) part_size = determine_part_size ( self . size , self . __part_size ) logger . debug ( \"Upload File size: %d , User-specify part_size: %d , \" \"Calculated part_size: %d \" , self . size , self . __part_size , part_size , ) if self . __encryption : upload_context = MultipartUploadCryptoContext ( self . size , part_size ) init_result = await self . bucket . init_multipart_upload ( self . key , self . __headers , params , upload_context ) upload_id = init_result . upload_id if self . __record_upload_context : material = upload_context . content_crypto_material material_record = { \"wrap_alg\" : material . wrap_alg , \"cek_alg\" : material . cek_alg , \"encrypted_key\" : b64encode_as_string ( material . encrypted_key ), \"encrypted_iv\" : b64encode_as_string ( material . encrypted_iv ), \"mat_desc\" : material . mat_desc , } else : init_result = await self . bucket . init_multipart_upload ( self . key , self . __headers , params ) upload_id = init_result . upload_id record = { \"op_type\" : self . __op , \"upload_id\" : upload_id , \"file_path\" : self . _abspath , \"size\" : self . size , \"mtime\" : self . __mtime , \"bucket\" : self . bucket . bucket_name , \"key\" : self . key , \"part_size\" : part_size , } if self . __record_upload_context : record [ \"content_crypto_material\" ] = material_record logger . debug ( \"Add new record, bucket: %s , key: %s , upload_id: %s , part_size: %d \" , self . bucket . bucket_name , self . key , upload_id , part_size , ) self . _put_record ( record ) return record","title":"init_record()"},{"location":"reference/aiooss2/resumable/#aiooss2.resumable.ResumableUploader.upload","text":"resumable upload file to oss storage Returns: Name Type Description PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/resumable.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 async def upload ( # pylint: disable=invalid-overridden-method self , ) -> \"PutObjectResult\" : \"\"\"resumable upload file to oss storage Returns: PutObjectResult: \"\"\" await self . _load_record () parts_to_upload : Collection [ \"_PartToProcess\" ] = self . __get_parts_to_upload ( self . __finished_parts ) parts_to_upload = sorted ( parts_to_upload , key = lambda p : p . part_number ) logger . debug ( \"Parts need to upload: %s \" , parts_to_upload ) sem = asyncio . Semaphore ( self . __num_threads ) tasks = [ asyncio . ensure_future ( self . _upload_task ( sem , part_to_upload )) for part_to_upload in parts_to_upload ] await asyncio . gather ( * tasks ) self . _report_progress ( self . size ) headers = _populate_valid_headers ( self . __headers , [ OSS_REQUEST_PAYER , OSS_OBJECT_ACL ] ) result = await self . bucket . complete_multipart_upload ( self . key , self . __upload_id , self . __finished_parts , headers = headers ) self . _del_record () return result","title":"upload()"},{"location":"reference/aiooss2/resumable/#aiooss2.resumable.resumable_download","text":"Resumable download object to local file, implementation method is to create a list temporary files whose name is formed by the original filename with some random surfix. If the downloading was interrupted by some reasons, only those remaied parts need to be downloaded.","title":"resumable_download()"},{"location":"reference/aiooss2/resumable/#aiooss2.resumable.resumable_download--using-cryptobucket-will-make-the-download-fallback-to-the-normal-one","text":"","title":"Using CryptoBucket will make the download fallback to the normal one."},{"location":"reference/aiooss2/resumable/#aiooss2.resumable.resumable_download--avoid-using-multi-threadingprocessing-as-the-temp-downloaded-file-might","text":"","title":"Avoid using multi-threading/processing as the temp downloaded file might"},{"location":"reference/aiooss2/resumable/#aiooss2.resumable.resumable_download--be-covered","text":"Parameters: Name Type Description Default bucket Union [ AioBucket , CryptoBucket ] bucket object to download required key Union [ str , bytes ] object key to store the file required filename Union [ str , bytes ] filename to download required store Optional [ ResumableStore ] ResumableStore object to keep the downloading info in the previous operation. None headers Optional [ Mapping ] HTTP headers to send. Defaults to None.","title":"be covered."},{"location":"reference/aiooss2/resumable/#aiooss2.resumable.resumable_download--download_part-only-accept-oss_request_payer","text":"","title":"download_part only accept OSS_REQUEST_PAYER"},{"location":"reference/aiooss2/resumable/#aiooss2.resumable.resumable_download--get_object-and-get_object_to_file-only-accept-oss_request_payer","text":"OSS_TRAFFIC_LIMIT None multipget_threshold Optional [ int ] threshold to use multipart download instead of a normal one. required part_size Optional [ int ] partition size of the multipart. None progress_callback Optional [ Callable ] callback function for progress bar. None num_threads Optional [ int ] concurrency number during the uploadinging None params Optional [ Mapping ] None Return None: Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/resumable.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 async def resumable_download ( # pylint: disable=too-many-arguments bucket : Union [ \"AioBucket\" , \"CryptoBucket\" ], key : Union [ str , bytes ], filename : Union [ str , bytes ], store : Optional [ \"ResumableStore\" ] = None , headers : Optional [ Mapping ] = None , part_size : Optional [ int ] = None , progress_callback : Optional [ Callable ] = None , num_threads : Optional [ int ] = None , params : Optional [ Mapping ] = None , multiget_threshold : Optional [ int ] = None , ) -> None : \"\"\"Resumable download object to local file, implementation method is to create a list temporary files whose name is formed by the original filename with some random surfix. If the downloading was interrupted by some reasons, only those remaied parts need to be downloaded. # Using `CryptoBucket` will make the download fallback to the normal one. # Avoid using multi-threading/processing as the temp downloaded file might # be covered. Args: bucket (Union[AioBucket, CryptoBucket]): bucket object to download key (Union[str, bytes]): object key to store the file filename (Union[str, bytes]): filename to download store (Optional[\"ResumableStore\"]): ResumableStore object to keep the downloading info in the previous operation. headers (Optional[Mapping]): HTTP headers to send. Defaults to None. # download_part only accept OSS_REQUEST_PAYER # get_object and get_object_to_file only accept OSS_REQUEST_PAYER, OSS_TRAFFIC_LIMIT multipget_threshold (Optional[int]): threshold to use multipart download instead of a normal one. part_size (Optional[int]): partition size of the multipart. progress_callback (Optional[Callable]): callback function for progress bar. num_threads (Optional[int]): concurrency number during the uploadinging params (Optional[Mapping]): Return: None: \"\"\" key_str : str = to_string ( key ) filename_str : str = to_unicode ( filename ) logger . debug ( \"Start to resumable download, bucket: %s , key: %s , filename: %s , \" \"multiget_threshold: %s , part_size: %s , num_threads: %s \" , bucket . bucket_name , key_str , filename_str , multiget_threshold , part_size , num_threads , ) multiget_threshold = multiget_threshold or MULTIGET_THRESHOLD valid_headers = _populate_valid_headers ( headers , [ OSS_REQUEST_PAYER , OSS_TRAFFIC_LIMIT ] ) result = await bucket . head_object ( key_str , params = params , headers = valid_headers ) logger . debug ( \"The size of object to download is: %s \" , result . content_length ) if result . content_length >= multiget_threshold : downloader = ResumableDownloader ( bucket , key , filename , _ObjectInfo . make ( result ), part_size = part_size , progress_callback = progress_callback , num_threads = num_threads , store = store , params = params , headers = valid_headers , ) await downloader . download ( result . server_crc ) else : await bucket . get_object_to_file ( key_str , filename_str , progress_callback = progress_callback , params = params , headers = valid_headers , )","title":"get_object and get_object_to_file only accept OSS_REQUEST_PAYER,"},{"location":"reference/aiooss2/resumable/#aiooss2.resumable.resumable_upload","text":"Resumable upload local file , The implementation is splitting local files to multipart, storing uploading information in local files. If the uploading was interrupted by some reasons, only those remaied parts need to be uploaded.","title":"resumable_upload()"},{"location":"reference/aiooss2/resumable/#aiooss2.resumable.resumable_upload--using-cryptobucket-will-make-the-upload-fallback-to-the-normal-one","text":"Parameters: Name Type Description Default bucket Union [ AioBucket , CryptoBucket ] bucket object to upload required key Union [ str , bytes ] object key to store the file required filename Union [ str , bytes ] filename to upload required store Optional [ ResumableStore ] ResumableStore object to keep the uploading info in the previous operation. Defaults to None. None headers Optional [ Mapping ] HTTP headers to send. Defaults to None.","title":"Using CryptoBucket will make the upload fallback to the normal one."},{"location":"reference/aiooss2/resumable/#aiooss2.resumable.resumable_upload--put_object-or-init_multipart_upload-can-make-use-of-the-whole","text":"headers","title":"put_object or init_multipart_upload can make use of the whole"},{"location":"reference/aiooss2/resumable/#aiooss2.resumable.resumable_upload--uplpad_part-only-accept-oss_request_payer-oss_traffic_limit","text":"","title":"uplpad_part only accept OSS_REQUEST_PAYER, OSS_TRAFFIC_LIMIT"},{"location":"reference/aiooss2/resumable/#aiooss2.resumable.resumable_upload--complete_multipart_upload-only-accept-oss_request_payer","text":"OSS_OBJECT_ACL None multipart_threshold Optional [ int ] threshold to use multipart upload instead of a normal one. Defaults to None. None part_size Optional [ int ] partition size of the multipart. Defaults to None. None progress_callback Optional [ Callable ] callback function for progress bar. Defaults to None. None num_threads Optional [ int ] concurrency number during the uploading Defaults to None. None params Optional [ Mapping ] Defaults to None. None Returns: Name Type Description PutObjectResult PutObjectResult Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/resumable.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 async def resumable_upload ( # pylint: disable=too-many-arguments bucket : Union [ \"AioBucket\" , \"CryptoBucket\" ], key : Union [ str , bytes ], filename : Union [ str , bytes ], store : Optional [ \"ResumableStore\" ] = None , headers : Optional [ Mapping ] = None , multipart_threshold : Optional [ int ] = None , part_size : Optional [ int ] = None , progress_callback : Optional [ Callable ] = None , num_threads : Optional [ int ] = None , params : Optional [ Mapping ] = None , ) -> \"PutObjectResult\" : \"\"\"Resumable upload local file , The implementation is splitting local files to multipart, storing uploading information in local files. If the uploading was interrupted by some reasons, only those remaied parts need to be uploaded. # Using `CryptoBucket` will make the upload fallback to the normal one. Args: bucket (Union[AioBucket, CryptoBucket]): bucket object to upload key (Union[str, bytes]): object key to store the file filename (Union[str, bytes]): filename to upload store (Optional[\"ResumableStore\"]): ResumableStore object to keep the uploading info in the previous operation. Defaults to None. headers (Optional[Mapping]): HTTP headers to send. Defaults to None. # put_object or init_multipart_upload can make use of the whole headers # uplpad_part only accept OSS_REQUEST_PAYER, OSS_TRAFFIC_LIMIT # complete_multipart_upload only accept OSS_REQUEST_PAYER, OSS_OBJECT_ACL multipart_threshold (Optional[int]): threshold to use multipart upload instead of a normal one. Defaults to None. part_size (Optional[int]): partition size of the multipart. Defaults to None. progress_callback (Optional[Callable]): callback function for progress bar. Defaults to None. num_threads (Optional[int]): concurrency number during the uploading Defaults to None. params (Optional[Mapping]): Defaults to None. Returns: PutObjectResult: \"\"\" key_str = to_string ( key ) filename_str = to_unicode ( filename ) size = os . path . getsize ( filename_str ) logger . debug ( \"Start to resumable upload, bucket: %s , key: %s , filename: %s , \" \"headers: %s , multipart_threshold: %s , part_size: %s , \" \"num_threads: %s , size of file to upload is %s \" , bucket . bucket_name , key_str , filename_str , headers , multipart_threshold , part_size , num_threads , size , ) multipart_threshold = multipart_threshold or MULTIPART_THRESHOLD num_threads = num_threads or MULTIPART_NUM_THREADS part_size = part_size or PART_SIZE if size >= multipart_threshold and not isinstance ( bucket , CryptoBucket ): store = store or ResumableStore () uploader = ResumableUploader ( bucket , key_str , filename_str , size , store , part_size = part_size , headers = headers , progress_callback = progress_callback , num_threads = num_threads , params = params , ) result = await uploader . upload () else : result = await bucket . put_object_from_file ( key_str , filename_str , headers = headers , progress_callback = progress_callback , ) return result","title":"complete_multipart_upload only accept OSS_REQUEST_PAYER,"},{"location":"reference/aiooss2/utils/","text":"Utils used in project. copyfileobj ( fsrc , fdst , length = 0 ) async copy data from file-like object fsrc to file-like object fdst Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/utils.py 103 104 105 106 107 108 109 110 111 112 113 async def copyfileobj ( fsrc , fdst , length = 0 ): \"\"\"copy data from file-like object fsrc to file-like object fdst\"\"\" if not length : length = COPY_BUFSIZE fsrc_read = fsrc . read fdst_write = fdst . write while True : buf = await fsrc_read ( length ) if not buf : break fdst_write ( buf ) copyfileobj_and_verify ( fsrc , fdst , expected_len , chunk_size = 16 * 1024 , request_id = '' ) async copy data from file-like object fsrc to file-like object fdst, and verify length Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/utils.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 async def copyfileobj_and_verify ( fsrc , fdst , expected_len , chunk_size = 16 * 1024 , request_id = \"\" ): \"\"\"copy data from file-like object fsrc to file-like object fdst, and verify length \"\"\" num_read = 0 while 1 : buf = await fsrc . read ( chunk_size ) if not buf : break num_read += len ( buf ) fdst . write ( buf ) if num_read != expected_len : raise InconsistentError ( \"IncompleteRead from source\" , request_id ) make_adapter ( stream , progress_callback = None , enable_crc = False , size = None , init_crc = 0 , discard = 0 ) Add crc calculation or progress bar callback to the data object. Parameters: Name Type Description Default stream _type_ bytes, file object or iterable required progress_callback Optional [ Callable ] progress bar callback function None size Optional [ int ] size of the data None enable_crc bool enable crc check or not False init_crc int init value of the crc check 0 discard int 0 Raises: Type Description ClientError description Returns: Name Type Description _type_ StreamAdapter description Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/utils.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def make_adapter ( # pylint: disable=too-many-arguments stream , progress_callback : Optional [ Callable ] = None , enable_crc : bool = False , size : Optional [ int ] = None , init_crc : int = 0 , discard : int = 0 , ) -> StreamAdapter : \"\"\"Add crc calculation or progress bar callback to the data object. Args: stream (_type_): bytes, file object or iterable progress_callback (Optional[Callable], optional): progress bar callback function size (Optional[int], optional): size of the data enable_crc (bool, optional): enable crc check or not init_crc (int, optional): init value of the crc check discard (int, optional): Raises: ClientError: _description_ Returns: _type_: _description_ \"\"\" stream = to_bytes ( stream ) if not enable_crc and not progress_callback : return stream crc_callback = Crc64 ( init_crc ) if enable_crc else None params = { \"stream\" : stream , \"progress_callback\" : progress_callback , \"crc_callback\" : crc_callback , \"discard\" : discard , \"size\" : size , } if isinstance ( stream , ( bytes , bytearray , memoryview )): return SliceableAdapter ( ** params ) if hasattr ( stream , \"read\" ): return FilelikeObjectAdapter ( ** params ) if ( hasattr ( stream , \"__aiter__\" ) or hasattr ( stream , \"__iter__\" ) or hasattr ( stream , \"__next__\" ) or hasattr ( stream , \"__anext__\" ) ): return IterableAdapter ( ** params ) raise ClientError ( f \" { stream . __class__ . __name__ } is neither a string nor bytes \" \"nor a file object nor an iterator\" )","title":"Utils"},{"location":"reference/aiooss2/utils/#aiooss2.utils.copyfileobj","text":"copy data from file-like object fsrc to file-like object fdst Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/utils.py 103 104 105 106 107 108 109 110 111 112 113 async def copyfileobj ( fsrc , fdst , length = 0 ): \"\"\"copy data from file-like object fsrc to file-like object fdst\"\"\" if not length : length = COPY_BUFSIZE fsrc_read = fsrc . read fdst_write = fdst . write while True : buf = await fsrc_read ( length ) if not buf : break fdst_write ( buf )","title":"copyfileobj()"},{"location":"reference/aiooss2/utils/#aiooss2.utils.copyfileobj_and_verify","text":"copy data from file-like object fsrc to file-like object fdst, and verify length Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/utils.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 async def copyfileobj_and_verify ( fsrc , fdst , expected_len , chunk_size = 16 * 1024 , request_id = \"\" ): \"\"\"copy data from file-like object fsrc to file-like object fdst, and verify length \"\"\" num_read = 0 while 1 : buf = await fsrc . read ( chunk_size ) if not buf : break num_read += len ( buf ) fdst . write ( buf ) if num_read != expected_len : raise InconsistentError ( \"IncompleteRead from source\" , request_id )","title":"copyfileobj_and_verify()"},{"location":"reference/aiooss2/utils/#aiooss2.utils.make_adapter","text":"Add crc calculation or progress bar callback to the data object. Parameters: Name Type Description Default stream _type_ bytes, file object or iterable required progress_callback Optional [ Callable ] progress bar callback function None size Optional [ int ] size of the data None enable_crc bool enable crc check or not False init_crc int init value of the crc check 0 discard int 0 Raises: Type Description ClientError description Returns: Name Type Description _type_ StreamAdapter description Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/aiooss2/utils.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def make_adapter ( # pylint: disable=too-many-arguments stream , progress_callback : Optional [ Callable ] = None , enable_crc : bool = False , size : Optional [ int ] = None , init_crc : int = 0 , discard : int = 0 , ) -> StreamAdapter : \"\"\"Add crc calculation or progress bar callback to the data object. Args: stream (_type_): bytes, file object or iterable progress_callback (Optional[Callable], optional): progress bar callback function size (Optional[int], optional): size of the data enable_crc (bool, optional): enable crc check or not init_crc (int, optional): init value of the crc check discard (int, optional): Raises: ClientError: _description_ Returns: _type_: _description_ \"\"\" stream = to_bytes ( stream ) if not enable_crc and not progress_callback : return stream crc_callback = Crc64 ( init_crc ) if enable_crc else None params = { \"stream\" : stream , \"progress_callback\" : progress_callback , \"crc_callback\" : crc_callback , \"discard\" : discard , \"size\" : size , } if isinstance ( stream , ( bytes , bytearray , memoryview )): return SliceableAdapter ( ** params ) if hasattr ( stream , \"read\" ): return FilelikeObjectAdapter ( ** params ) if ( hasattr ( stream , \"__aiter__\" ) or hasattr ( stream , \"__iter__\" ) or hasattr ( stream , \"__next__\" ) or hasattr ( stream , \"__anext__\" ) ): return IterableAdapter ( ** params ) raise ClientError ( f \" { stream . __class__ . __name__ } is neither a string nor bytes \" \"nor a file object nor an iterator\" )","title":"make_adapter()"}]}